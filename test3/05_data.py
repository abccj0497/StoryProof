# 05_data_analyzer_plus.py

# parent/child ë¹„ìœ¨, parent_id ëˆ„ë½ë¥ (PC í’ˆì§ˆ)
# embedding ì°¨ì›, ëˆ„ë½, NaN/Inf, L2 norm í†µê³„(ì •ìƒì„±)
# ì¤‘ë³µ ì²­í¬(ë™ì¼ content) ë¹„ìœ¨
# ê¸¸ì´ ë¶„í¬(p50/p90), ë„ˆë¬´ ì§§ìŒ/ë„ˆë¬´ ê¹€ ë¹„ìœ¨
# ë©”íƒ€ë°ì´í„° íƒœê·¸ ë¶€ì°©ë¥ , ìºë¦­í„°/ì•„ì´í…œ TOP

import json, os, random, math
import numpy as np
from collections import Counter

FILES = [
    "01_entity_pc_data.json",
    "02_recursive_pc_data.json",
    "03_sliding_pc_data.json",
    "00_full_data.json",
]

def pct(xs, p):
    if not xs:
        return 0
    xs = sorted(xs)
    k = int((len(xs) - 1) * p)
    return xs[k]

def analyze_file(filename):
    if not os.path.exists(filename):
        print(f"âŒ {filename} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ê±´ë„ˆëœ€)")
        return

    with open(filename, "r", encoding="utf-8") as f:
        data = json.load(f)

    total = len(data)
    if total == 0:
        return

    # íƒ€ì… ë¶„í¬
    type_counts = Counter([d.get("type", "unknown") for d in data])
    parent_cnt = type_counts.get("parent", 0)
    child_cnt = type_counts.get("child", 0)

    # parent_id ìƒíƒœ
    child_with_parent = sum(1 for d in data if d.get("type") == "child" and d.get("parent_id"))
    child_missing_parent = sum(1 for d in data if d.get("type") == "child" and not d.get("parent_id"))

    # ê¸¸ì´ ë¶„í¬
    lens = [len(d.get("content", "")) for d in data]
    avg_len = float(np.mean(lens))
    short_cnt = sum(1 for l in lens if l < 100)
    good_cnt  = sum(1 for l in lens if 100 <= l <= 800)
    long_cnt  = sum(1 for l in lens if l > 800)

    # ì„ë² ë”© ì²´í¬
    emb_missing = sum(1 for d in data if not d.get("embedding"))
    dims = [len(d["embedding"]) for d in data if d.get("embedding")]
    dim = dims[0] if dims else 0

    norms = []
    nan_inf = 0
    for d in data:
        e = d.get("embedding")
        if not e:
            continue
        arr = np.array(e, dtype=np.float32)
        if np.any(np.isnan(arr)) or np.any(np.isinf(arr)):
            nan_inf += 1
        norms.append(float(np.linalg.norm(arr)))

    # ì¤‘ë³µ ì²­í¬(ë‚´ìš©)
    contents = [d.get("content", "").strip() for d in data]
    dup_ratio = 0.0
    if contents:
        uniq = len(set(contents))
        dup_ratio = 1.0 - (uniq / len(contents))

    # íƒœê·¸ ë¶„ì„
    all_chars, all_items = [], []
    tag_attached = 0
    for d in data:
        md = d.get("metadata", {}) or {}
        chars = md.get("characters", []) or []
        items = md.get("items", []) or []
        if chars or items:
            tag_attached += 1
        all_chars.extend(chars)
        all_items.extend(items)

    char_counts = Counter(all_chars)
    item_counts = Counter(all_items)

    # ì¶œë ¥
    print("\n" + "=" * 70)
    print(f"ğŸ“Š [05 ë°ì´í„° ê±´ê°•ê²€ì§„+] íŒŒì¼ëª…: {filename}")
    print("=" * 70)

    print("1ï¸âƒ£  íƒ€ì…/êµ¬ì¡° ìƒíƒœ")
    print(f"   - ì´ ë ˆì½”ë“œ ìˆ˜     : {total}")
    print(f"   - íƒ€ì… ë¶„í¬        : {dict(type_counts)}")
    if child_cnt > 0:
        print(f"   - Child parent_id ë¶€ì°©ë¥ : {child_with_parent}/{child_cnt} ({child_with_parent/child_cnt*100:.1f}%)")
        if child_missing_parent > 0:
            print(f"   - âš ï¸ parent_id ëˆ„ë½ child: {child_missing_parent}")

    print("-" * 70)
    print("2ï¸âƒ£  ì²­í‚¹(Chunking) ìƒíƒœ")
    print(f"   - í‰ê·  ê¸¸ì´        : {avg_len:.1f}ì")
    print(f"   - p50/p90          : {pct(lens, 0.5)} / {pct(lens, 0.9)}")
    print(f"   - ìµœì†Œ/ìµœëŒ€        : {min(lens)} / {max(lens)}")
    print(f"   - ğŸŸ¥ <100ì        : {short_cnt} ({short_cnt/total*100:.1f}%)")
    print(f"   - ğŸŸ© 100~800ì      : {good_cnt} ({good_cnt/total*100:.1f}%)")
    print(f"   - ğŸŸ§ >800ì        : {long_cnt} ({long_cnt/total*100:.1f}%)")
    print(f"   - ì¤‘ë³µ content ë¹„ìœ¨: {dup_ratio*100:.1f}%")

    print("-" * 70)
    print("3ï¸âƒ£  ë²¡í„°(Vector) ìƒíƒœ")
    print(f"   - embedding ëˆ„ë½   : {emb_missing}ê°œ")
    print(f"   - ì°¨ì› ìˆ˜          : {dim} (ë³´í†µ 768ì´ë©´ ì •ìƒ)")
    if norms:
        print(f"   - L2 norm(min/mean/max): {min(norms):.3f} / {np.mean(norms):.3f} / {max(norms):.3f}")
    if nan_inf > 0:
        print(f"   - âš ï¸ NaN/Inf í¬í•¨ ë²¡í„°: {nan_inf}ê°œ")

    print("-" * 70)
    print("4ï¸âƒ£  ë©”íƒ€ë°ì´í„°(Tag) ìƒíƒœ")
    print(f"   - íƒœê·¸ ë¶€ì°©ë¥       : {tag_attached}/{total} ({tag_attached/total*100:.1f}%)")
    print(f"   - ğŸ‘¤ ì¸ë¬¼ TOP 5     : {char_counts.most_common(5)}")
    print(f"   - ğŸ—ï¸ ì•„ì´í…œ TOP 5   : {item_counts.most_common(5)}")

    print("-" * 70)
    print("5ï¸âƒ£  ë¬´ì‘ìœ„ ìƒ˜í”Œ(ì²­ì†Œ/ë¬¸ë§¥ í™•ì¸)")
    sample = random.choice(data)
    preview = sample.get("content", "")[:140].replace("\n", " ")
    print(f"   >> \"{preview}...\"")
    print("=" * 70)

if __name__ == "__main__":
    print("\nğŸ” JSON ë°ì´í„° ì •ë°€ ë¶„ì„(í™•ì¥íŒ)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    for f in FILES:
        analyze_file(f)
