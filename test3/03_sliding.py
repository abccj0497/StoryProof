# 03_gen_sliding_pc.py


# parent = â€œë¬¸ì¥ ë³´ì¡´ ìŠ¬ë¼ì´ë”©(í† í° ê¸°ì¤€)â€ 1000/200
# child = parent ë‚´ë¶€ë¥¼ ë” ì‘ì€ ë¬¸ì¥ ë³´ì¡´ ìŠ¬ë¼ì´ë”©(400/80)
# tokenizerëŠ” sentence-transformers ëª¨ë¸ tokenizerë¥¼ ì‚¬ìš©

import json, uuid, re, os, time
import fitz
from sentence_transformers import SentenceTransformer

SOURCE_FILE = "alice_utf8.txt"
OUTPUT_FILE = "03_sliding_pc_data.json"
MODEL_NAME = "Alibaba-NLP/gte-multilingual-base"

def clean_text(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"ê·¸ë¦¼ì„¤ëª…\s*:.*", "", text)
    text = re.sub(r"[-=]{3,}", "", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def load_any(path: str) -> str:
    if path.lower().endswith(".txt"):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    if path.lower().endswith(".pdf"):
        doc = fitz.open(path)
        pages = [doc.load_page(i).get_text("text") for i in range(len(doc))]
        return "\n".join(pages)
    raise ValueError("ì§€ì› í™•ì¥ì: .txt, .pdf")

def sentence_split(text: str):
    # ì•„ì£¼ ê°€ë²¼ìš´ ë¬¸ì¥ ë¶„í• (í•œêµ­ì–´ ì™„ë²½Xì§€ë§Œ ë¬¸ì¥ ë³´ì¡´ ëª©ì )
    text = re.sub(r"\n+", " ", text).strip()
    if not text:
        return []
    sents = re.split(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+", text)
    sents = [s.strip() for s in sents if s.strip()]
    return sents

def sliding_sentence_preserving(sents, tokenizer, chunk_tokens: int, overlap_tokens: int):
    def tok_len(s: str) -> int:
        return len(tokenizer.encode(s, add_special_tokens=False))

    chunks = []
    cur = []
    cur_tok = 0
    i = 0

    while i < len(sents):
        s = sents[i]
        t = tok_len(s)

        # sentence ìì²´ê°€ ë„ˆë¬´ ê¸¸ë©´ ê°•ì œ ë¶„í• (ìµœí›„ ìˆ˜ë‹¨)
        if t > chunk_tokens:
            if cur:
                chunks.append(" ".join(cur).strip())
                cur, cur_tok = [], 0
            step = max(200, int(len(s) * (chunk_tokens / max(t, 1))))
            for a in range(0, len(s), step):
                chunks.append(s[a:a+step].strip())
            i += 1
            continue

        if cur_tok + t <= chunk_tokens:
            cur.append(s)
            cur_tok += t
            i += 1
        else:
            chunks.append(" ".join(cur).strip())

            # overlap ìœ ì§€: ë’¤ì—ì„œ overlap_tokens ë§Œí¼ ë¬¸ì¥ ìœ ì§€
            keep = []
            keep_tok = 0
            for ss in reversed(cur):
                tt = tok_len(ss)
                if keep_tok + tt > overlap_tokens:
                    break
                keep.append(ss)
                keep_tok += tt
            keep = list(reversed(keep))
            cur = keep
            cur_tok = keep_tok

    if cur:
        chunks.append(" ".join(cur).strip())

    return [c for c in chunks if c]

def run(source_file: str = SOURCE_FILE, output_file: str = OUTPUT_FILE):
    print(f">>> [03ë²ˆ ì „ëµ: Sliding(1000_200) + Parent-Child] {source_file} ì²˜ë¦¬ ì‹œì‘...")
    if not os.path.exists(source_file):
        print("âŒ íŒŒì¼ ì—†ìŒ")
        return

    raw = load_any(source_file)
    text = clean_text(raw)

    print("   ...ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = SentenceTransformer(MODEL_NAME, trust_remote_code=True)
    tokenizer = model.tokenizer

    sents = sentence_split(text)

    # parent: 1000/200
    parents = sliding_sentence_preserving(sents, tokenizer, chunk_tokens=1000, overlap_tokens=200)

    # child: parent ë‚´ë¶€ë¥¼ 400/80ìœ¼ë¡œ í•œ ë²ˆ ë”
    all_children = []
    child_parent_link = []
    for p in parents:
        ps = sentence_split(p)
        children = sliding_sentence_preserving(ps, tokenizer, chunk_tokens=400, overlap_tokens=80)
        for c in children:
            all_children.append(c)
            child_parent_link.append(None)  # ë‚˜ì¤‘ì— parent_id ì±„ì›€ (ê°™ì€ parent loop)
        # ìœ„ ë°©ì‹ì€ parent loopì—ì„œ ê°™ì´ ë„£ì–´ì•¼ í•˜ëŠ”ë°, ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì•„ë˜ì—ì„œ ì¬êµ¬ì„±

    # parent_idë¥¼ ì •í™•íˆ ë§¤í•‘í•˜ê¸° ìœ„í•´ ë‹¤ì‹œ êµ¬ì„±
    all_children = []
    child_parent_link = []
    for p in parents:
        pid = str(uuid.uuid4())
        # ì¼ë‹¨ parent id ë”°ë¡œ ì €ì¥í•´ë‘ê³ , data ë§Œë“¤ ë•Œ ì‚¬ìš©
        child_sents = sentence_split(p)
        kids = sliding_sentence_preserving(child_sents, tokenizer, chunk_tokens=400, overlap_tokens=80)
        for k in kids:
            all_children.append(k)
            child_parent_link.append(pid)

    # ìœ„ì—ì„œ parent idë¥¼ ìƒˆë¡œ ë§Œë“¤ì—ˆìœ¼ë‹ˆ, parentë„ ê°™ì€ ìˆœì„œë¡œ ë‹¤ì‹œ ë§Œë“¤ê¸°
    parent_ids = []
    tmp_parents = []
    for p in parents:
        pid = str(uuid.uuid4())
        parent_ids.append(pid)
        tmp_parents.append(p)

    # ë‹¤ì‹œ child-parent ì—°ê²°ì„ parent_idsë¡œ ë§ì¶¤
    all_children = []
    child_parent_link = []
    for pid, p in zip(parent_ids, tmp_parents):
        kids = sliding_sentence_preserving(sentence_split(p), tokenizer, chunk_tokens=400, overlap_tokens=80)
        for k in kids:
            all_children.append(k)
            child_parent_link.append(pid)

    data = []
    start_time = time.time()

    print(f"   ...Parent ì„ë² ë”© (ì´ {len(tmp_parents)}ê°œ)")
    parent_emb = model.encode(tmp_parents, show_progress_bar=True)

    for i, p in enumerate(tmp_parents):
        data.append({
            "id": parent_ids[i],
            "type": "parent",
            "parent_id": None,
            "content": p,
            "metadata": {"strategy": "sliding_parent", "token_chunk": 1000, "token_overlap": 200, "len": len(p)},
            "embedding": parent_emb[i].tolist()
        })

    print(f"   ...Child ì„ë² ë”© (ì´ {len(all_children)}ê°œ)")
    child_emb = model.encode(all_children, show_progress_bar=True)

    for i, c in enumerate(all_children):
        data.append({
            "id": str(uuid.uuid4()),
            "type": "child",
            "parent_id": child_parent_link[i],
            "content": c,
            "metadata": {"strategy": "sliding_child", "token_chunk": 400, "token_overlap": 80, "len": len(c)},
            "embedding": child_emb[i].tolist()
        })

    duration = time.time() - start_time

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    parent_cnt = sum(1 for d in data if d["type"] == "parent")
    child_cnt = sum(1 for d in data if d["type"] == "child")
    vec_ok = all("embedding" in d and len(d["embedding"]) > 0 for d in data)

    print("\n" + "=" * 48)
    print("ğŸ“Š [03ë²ˆ Sliding+PC ê²°ê³¼ ë¦¬í¬íŠ¸]")
    print(f"âœ… ì €ì¥ ì™„ë£Œ          : {output_file}")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„          : {duration:.2f} ì´ˆ")
    print(f"ğŸ“¦ Parent ì²­í¬ ê°œìˆ˜   : {parent_cnt} ê°œ")
    print(f"ğŸ“¦ Child ì²­í¬ ê°œìˆ˜    : {child_cnt} ê°œ")
    print(f"ğŸ”¢ ë²¡í„°í™” ì •ìƒ ì—¬ë¶€   : {'OK' if vec_ok else 'WARN'}")
    print("=" * 48)

if __name__ == "__main__":
    run()
