import os
import gc
import json
import random
import time
import pandas as pd
import torch
from datetime import datetime
from chromadb import PersistentClient
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer

# [0. í™˜ê²½ ì„¤ì • ë° ì¸ì¦]
# Hugging Face ê²½ê³ ë¥¼ ì—†ì• ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì— í† í°ì„ ë„£ìœ¼ì„¸ìš”.
# os.environ["HF_TOKEN"] = "your_token_here"
DB_PATH = "./storyproof_db"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# [1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ í•´ê²°ì„ ìœ„í•œ ì–‘ìí™” ì„¤ì •]
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    # í•µì‹¬ ìˆ˜ì • ì‚¬í•­: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU ì˜¤í”„ë¡œë“œ í—ˆìš©
    llm_int8_enable_fp32_cpu_offload=True 
)

model_id = "zai-org/GLM-4.7-Flash"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

# ëª¨ë¸ ë¡œë“œ (ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ ìµœì í™” ì˜µì…˜ ì¶”ê°€)
llm_model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    quantization_config=bnb_config, 
    device_map="auto", 
    trust_remote_code=True,
    low_cpu_mem_usage=True
).eval()

embed_model = SentenceTransformer('BAAI/bge-m3', device=DEVICE)

# [2. í†µí•© ì—”ì§„ í´ë˜ìŠ¤]
class StoryProofEvolution:
    def __init__(self):
        self.client = PersistentClient(path=DB_PATH)
        self.collection = self.client.get_or_create_collection(name="story_bible")
        self.strategy_guide = "ì •í™•í•œ ê³ ìœ ëª…ì‚¬ì™€ ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ ê· í˜• ìˆê²Œ ê²€ìƒ‰í•˜ì„¸ìš”."

    def _clean_memory(self):
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def _generate(self, prompt):
        inputs = tokenizer.apply_chat_template([{"role": "user", "content": prompt}], 
                                               add_generation_prompt=True, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            outputs = llm_model.generate(inputs, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id)
        res = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        self._clean_memory()
        return res

    def ingest_novel(self, file_path):
        if not os.path.exists(file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        scenes = [s.strip() for s in text.split("\n\n") if len(s.strip()) > 100]
        print(f"ğŸ“– ì´ {len(scenes)}ê°œì˜ ì¥ë©´ ì¸ë±ì‹± ì‹œì‘...")

        for i, scene in enumerate(scenes[:20]): # ì´ˆê¸° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 20ê°œë§Œ ì§„í–‰
            extract_prompt = f"ë‹¤ìŒ ì†Œì„¤ ì¥ë©´ì—ì„œ ì£¼ìš” ì¸ë¬¼, ì•„ì´í…œ, ì‚¬ê±´ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜:\n\n{scene[:500]}"
            bible_json = self._generate(extract_prompt)
            
            vector = embed_model.encode(scene).tolist()
            self.collection.add(
                ids=[f"scene_{i}"],
                embeddings=[vector],
                documents=[scene],
                metadatas=[{"bible": bible_json, "index": i}]
            )
            print(f"âœ… [{i+1}/{len(scenes)}] ì¥ë©´ ì²˜ë¦¬ ì™„ë£Œ")
        
        print("ğŸ¯ ì¸ë±ì‹± ë° ë°”ì´ë¸” ì¶”ì¶œ ì™„ë£Œ!")

    def generate_eval_set(self, count=5):
        all_docs = self.collection.get()
        if not all_docs['ids']: return []
        
        samples = random.sample(range(len(all_docs['ids'])), min(count, len(all_docs['ids'])))
        eval_set = []
        
        for idx in samples:
            target_text = all_docs['documents'][idx]
            target_id = all_docs['ids'][idx]
            q_prompt = f"ë‹¤ìŒ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì§§ì€ ì§ˆë¬¸ í•˜ë‚˜ë§Œ ë§Œë“¤ì–´ì¤˜:\n\n{target_text[:300]}"
            question = self._generate(q_prompt)
            eval_set.append({"query": question, "ground_truth": target_id})
        return eval_set

    def evaluate_and_improve(self, eval_set):
        if not eval_set: return 0
        hits = 0
        for item in eval_set:
            q_vec = embed_model.encode(item['query']).tolist()
            results = self.collection.query(query_embeddings=[q_vec], n_results=3)
            if item['ground_truth'] in results['ids'][0]:
                hits += 1
        return hits / len(eval_set)

# [3. ì‹¤í–‰ ë£¨í”„]
if __name__ == "__main__":
    engine = StoryProofEvolution()

    # 1. ì•¨ë¦¬ìŠ¤ í…ìŠ¤íŠ¸ ë°ì´í„° êµ¬ì¶•
    print(f"ğŸš€ [{datetime.now()}] ë°ì´í„° êµ¬ì¶• ì‹œì‘...")
    engine.ingest_novel("alice_utf8.txt")

    # 2. ìê°€ ì§„í™” ë£¨í”„ (ë¬´í•œ ë°˜ë³µ)
    print(f"ğŸ”„ [{datetime.now()}] ìê°€ ì§„í™” ë£¨í”„ ì‹œì‘...")
    while True:
        test_data = engine.generate_eval_set(count=3)
        accuracy = engine.evaluate_and_improve(test_data)
        
        print(f"ğŸ“Š í˜„ì¬ ê²€ìƒ‰ ì •í™•ë„: {accuracy:.2%} | ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
        
        time.sleep(600) # 10ë¶„ ëŒ€ê¸° í›„ ë‹¤ìŒ ì‚¬ì´í´