import os
import gc
import json
import random
import time
import pandas as pd
import torch
from datetime import datetime
from chromadb import PersistentClient
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer

# =========================================================
# [1. ÌôòÍ≤Ω ÏÑ§Ï†ï]
# =========================================================
DB_PATH = "./storyproof_db"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

assert DEVICE == "cuda", "‚ùå CUDA Ïù∏Ïãù ÏïàÎê® ‚Äì PyTorch CUDA Î≤ÑÏ†Ñ ÌôïÏù∏ ÌïÑÏöî"

# =========================================================
# [2. LLM Î°úÎìú (RTX 4060 ÏïàÏ†ï ÏÑ∏ÌåÖ)]
# =========================================================
model_id = "zai-org/GLM-4.7-Flash"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16  # ‚≠ê 4060 ÏïàÏ†ï
)

tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    trust_remote_code=True
)

llm_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="cuda",          # ‚≠ê auto ‚ùå
    trust_remote_code=True
).eval()

print("‚úÖ LLM loaded on:", next(llm_model.parameters()).device)

# =========================================================
# [3. Embedding Î™®Îç∏]
# =========================================================
embed_model = SentenceTransformer(
    "BAAI/bge-m3",
    device=DEVICE
)

# =========================================================
# [4. ÏóîÏßÑ ÌÅ¥ÎûòÏä§]
# =========================================================
class StoryProofEvolution:
    def __init__(self):
        self.client = PersistentClient(path=DB_PATH)
        self.collection = self.client.get_or_create_collection(
            name="story_bible"
        )
        self.best_alpha = 0.5
        self.strategy_guide = "Ï†ïÌôïÌïú Í≥†Ïú†Î™ÖÏÇ¨ÏôÄ Î¨∏Îß•ÏùÑ Ìï®Íªò Í≥†Î†§ÌïòÏÑ∏Ïöî."

    def _clean_memory(self):
        torch.cuda.empty_cache()
        gc.collect()

    def _generate(self, prompt: str) -> str:
        model_device = next(llm_model.parameters()).device

        inputs = tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model_device)

        with torch.no_grad():
            outputs = llm_model.generate(
                inputs,
                max_new_tokens=300,
                pad_token_id=tokenizer.eos_token_id
            )

        result = tokenizer.decode(
            outputs[0][len(inputs[0]):],
            skip_special_tokens=True
        )

        self._clean_memory()
        return result.strip()

    # -----------------------------------------------------
    # Step 1. ÏÜåÏÑ§ Ïù∏Îç±Ïã±
    # -----------------------------------------------------
    def ingest_novel(self, text: str):
        scenes = text.split("\n\n\n")

        for i, scene in enumerate(scenes):
            if len(scene.strip()) < 50:
                continue

            extract_prompt = (
                "Îã§Ïùå ÏÜåÏÑ§ Ïû•Î©¥ÏóêÏÑú Ïù∏Î¨º, ÏïÑÏù¥ÌÖú, ÏÇ¨Í±¥ÏùÑ JSONÏúºÎ°ú Ï†ïÎ¶¨Ìï¥Ï§ò:\n\n"
                + scene[:800]
            )

            bible_json = self._generate(extract_prompt)

            vector = embed_model.encode(scene).tolist()

            self.collection.add(
                ids=[f"scene_{i}"],
                embeddings=[vector],
                documents=[scene],
                metadatas=[{
                    "bible": bible_json,
                    "index": i
                }]
            )

        print(f"‚úÖ Ïù∏Îç±Ïã± ÏôÑÎ£å: {len(scenes)} scenes")

    # -----------------------------------------------------
    # Step 2. ÌèâÍ∞Ä ÏßàÎ¨∏ ÏûêÎèô ÏÉùÏÑ±
    # -----------------------------------------------------
    def generate_eval_set(self, count=10):
        docs = self.collection.get()
        indices = random.sample(
            range(len(docs["ids"])),
            min(count, len(docs["ids"]))
        )

        eval_set = []
        for idx in indices:
            text = docs["documents"][idx]
            q_prompt = (
                "Îã§Ïùå Î≥∏Î¨∏ÏóêÏÑú Ï†ïÎãµÏù¥ Î™ÖÌôïÌïú ÏßàÎ¨∏ ÌïòÎÇòÎßå ÎßåÎì§Ïñ¥Ï§ò:\n\n"
                + text[:400]
            )
            question = self._generate(q_prompt)
            eval_set.append({
                "query": question,
                "ground_truth": docs["ids"][idx]
            })

        return eval_set

# =========================================================
# [5. Ïã§Ìñâ]
# =========================================================
engine = StoryProofEvolution()
print(f"[{datetime.now()}] üöÄ StoryProof Evolution Ready")
