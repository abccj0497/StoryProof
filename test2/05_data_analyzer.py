import json
import os
import numpy as np
import random
from collections import Counter

# ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
FILES = ["01_entity_data.json", "02_recursive_data.json", "03_sliding_data.json"]

def analyze_file(filename):
    if not os.path.exists(filename):
        print(f"âŒ {filename} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ê±´ë„ˆëœ€)")
        return

    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)

    total_chunks = len(data)
    if total_chunks == 0:
        return

    # ----------------------------------------
    # 1. ë°ì´í„° ê³„ì‚° (Calculations)
    # ----------------------------------------
    # ê¸¸ì´ ë¶„ì„
    chunk_lengths = [len(d['content']) for d in data]
    avg_len = np.mean(chunk_lengths)
    
    # ê¸¸ì´ ë¶„í¬ êµ¬ê°„ ê³„ì‚°
    short_cnt = sum(1 for l in chunk_lengths if l < 100)
    good_cnt  = sum(1 for l in chunk_lengths if 100 <= l <= 800)
    long_cnt  = sum(1 for l in chunk_lengths if l > 800)

    # ë²¡í„° ë¶„ì„
    has_vector = sum(1 for d in data if "embedding" in d and d['embedding'])
    vec_dim = len(data[0]['embedding']) if has_vector > 0 else 0

    # ë©”íƒ€ë°ì´í„°(íƒœê·¸) ë¶„ì„
    all_chars = []
    all_items = []
    tag_attached_count = 0
    
    for d in data:
        meta = d.get('metadata', {})
        chars = meta.get('characters', [])
        items = meta.get('items', [])
        
        if chars or items:
            tag_attached_count += 1
            
        all_chars.extend(chars)
        all_items.extend(items)
        
    char_counts = Counter(all_chars)
    item_counts = Counter(all_items)

    # ----------------------------------------
    # 2. ë¦¬í¬íŠ¸ ì¶œë ¥ (Reporting)
    # ----------------------------------------
    print("\n" + "=" * 60)
    print(f"ğŸ“Š [ë°ì´í„° ê±´ê°•ê²€ì§„ ë¦¬í¬íŠ¸] íŒŒì¼ëª…: {filename}")
    print("=" * 60)

    # [ì„¹ì…˜ 1] ì²­í‚¹ ìƒíƒœ (ê°€ì¥ ì¤‘ìš”)
    print(f"1ï¸âƒ£  ì²­í‚¹(Chunking) ìƒíƒœ")
    print(f"   - ì´ ë©ì–´ë¦¬ ê°œìˆ˜ : {total_chunks}ê°œ")
    print(f"   - í‰ê·  ê¸€ì ìˆ˜   : {avg_len:.1f}ì")
    print(f"   - ìµœì†Œ/ìµœëŒ€ ê¸¸ì´ : {min(chunk_lengths)}ì / {max(chunk_lengths)}ì")
    print(f"   ------------------------------------")
    print(f"   [ê¸¸ì´ ë¶„í¬ ì§„ë‹¨]")
    print(f"   ğŸŸ¥ ë„ˆë¬´ ì§§ìŒ (<100ì) : {short_cnt}ê°œ ({short_cnt/total_chunks*100:.1f}%) -> ì •ë³´ ë¶€ì¡± ìœ„í—˜")
    print(f"   ğŸŸ© ì ì ˆí•¨ (100~800ì) : {good_cnt}ê°œ ({good_cnt/total_chunks*100:.1f}%) -> ë² ìŠ¤íŠ¸ ğŸ‘")
    print(f"   ğŸŸ§ ë„ˆë¬´ ê¹€ (>800ì)   : {long_cnt}ê°œ ({long_cnt/total_chunks*100:.1f}%) -> ì£¼ì œ í¬ì„ ìœ„í—˜")

    # [ì„¹ì…˜ 2] ë²¡í„° ìƒíƒœ
    print("-" * 60)
    print(f"2ï¸âƒ£  ë²¡í„°(Vector) ìƒíƒœ")
    if has_vector == total_chunks:
        print(f"   - âœ… ìƒíƒœ ì–‘í˜¸: ëª¨ë“  ì²­í¬({has_vector}ê°œ)ì— ë²¡í„° ìˆìŒ")
    else:
        print(f"   - âš ï¸ ê²½ê³ : {total_chunks - has_vector}ê°œ ì²­í¬ì— ë²¡í„°ê°€ ëˆ„ë½ë¨!")
    
    print(f"   - ì°¨ì› ìˆ˜: {vec_dim} ì°¨ì› (768ì´ë©´ ì •ìƒ)")

    # [ì„¹ì…˜ 3] ë©”íƒ€ë°ì´í„° íƒœê·¸ (ì¸í…”ë¦¬ì „ìŠ¤)
    print("-" * 60)
    print(f"3ï¸âƒ£  ë©”íƒ€ë°ì´í„°(Tag) ë¶„ì„")
    print(f"   - íƒœê·¸ ë¶€ì°©ë¥ : {tag_attached_count}ê°œ ({tag_attached_count/total_chunks*100:.1f}%)")
    
    if char_counts:
        print(f"   - ğŸ‘¤ ì£¼ìš” ì¸ë¬¼ TOP 3: {char_counts.most_common(3)}")
    else:
        print("   - ğŸ‘¤ ì¸ë¬¼ íƒœê·¸: ì—†ìŒ (ì „ëµì— ë”°ë¼ ë‹¤ë¦„)")
        
    if item_counts:
        print(f"   - ğŸ—ï¸  ì£¼ìš” ì•„ì´í…œ TOP 3: {item_counts.most_common(3)}")

    # [ì„¹ì…˜ 4] ë¶ˆëŸ‰ ê²€ì¶œ (ìƒ˜í”Œë§)
    print("-" * 60)
    print(f"4ï¸âƒ£  ë¬´ì‘ìœ„ ìƒ˜í”Œ (ì²­ì†Œ ìƒíƒœ í™•ì¸ìš©)")
    sample = random.choice(data)
    preview = sample['content'][:100].replace("\n", " ")
    print(f"   >> \"{preview}...\"")
    print("=" * 60)

if __name__ == "__main__":
    print("\nğŸ” JSON ë°ì´í„° ì •ë°€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    for f in FILES:
        analyze_file(f)