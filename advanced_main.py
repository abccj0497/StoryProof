import os
import glob
from typing import List
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.docstore.document import Document
from transformers import AutoTokenizer

# ==========================================
# 1. ì„¤ì • (Configuration)
# ==========================================
MODEL_NAME = "Alibaba-NLP/gte-multilingual-base"
DB_PATH = "./chroma_advanced_db"

# ì£¼ìš” ë“±ì¥ì¸ë¬¼ (Entity íƒœê¹…ìš© í‚¤ì›Œë“œ ì‚¬ì „)
ENTITIES = ["ì•¨ë¦¬ìŠ¤", "í† ë¼", "ì—¬ì™•", "ëª¨ìì¥ìˆ˜", "ê³ ì–‘ì´", "ë„ë„ìƒˆ", "ì• ë²Œë ˆ"]

class AdvancedChunker:
    def __init__(self):
        print(f"âš™ï¸ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=MODEL_NAME,
            model_kwargs={'device': 'cpu', 'trust_remote_code': True},
            encode_kwargs={'normalize_embeddings': True}
        )
        # í† í° ê³„ì‚°ì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ë¡œë“œ (ì „ëµ 3ë²ˆìš©)
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

    def load_file(self):
        # í´ë” ë‚´ì˜ 'ì•¨ë¦¬ìŠ¤' í…ìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°
        files = glob.glob("*.txt")
        alice_file = next((f for f in files if "ì•¨ë¦¬ìŠ¤" in f), None)
        
        if not alice_file:
            print("âŒ 'ì•¨ë¦¬ìŠ¤' í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"ğŸ“‚ íŒŒì¼ ì½ê¸°: {alice_file}")
        try:
            with open(alice_file, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            with open(alice_file, 'r', encoding='cp949', errors='ignore') as f:
                return f.read()

    # ==========================================
    # ì „ëµ 1: ê°œì²´(Entity) ì¤‘ì‹¬ ë©”íƒ€ë°ì´í„° íƒœê¹…
    # ==========================================
    def strategy_entity_tagging(self, text):
        print("\n[ì „ëµ 1] ê°œì²´(Entity) íƒœê¹… ì²­í‚¹ ì‹¤í–‰ ì¤‘...")
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ë¬¸ë§¥ ë‹¨ìœ„ë¡œ ìë¥´ë˜, íƒœê·¸ë¥¼ ì…í˜
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        chunks = splitter.split_text(text)
        
        docs = []
        for chunk in chunks:
            # ë“±ì¥ì¸ë¬¼ ì°¾ê¸°
            found_entities = [e for e in ENTITIES if e in chunk]
            
            # ë©”íƒ€ë°ì´í„°ì— íƒœê·¸ ì¶”ê°€
            metadata = {"strategy": "entity_tag", "entities": found_entities}
            docs.append(Document(page_content=chunk, metadata=metadata))
            
        self._save_to_db(docs, "collection_entity")
        return docs

    # ==========================================
    # ì „ëµ 2: ì¬ê·€ì  ë¬¸ë‹¨ ë¶„í•  (Recursive)
    # ==========================================
    def strategy_recursive(self, text):
        print("\n[ì „ëµ 2] ì¬ê·€ì  ë¶„í• (Recursive) ì‹¤í–‰ ì¤‘...")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", " ", ""] # ë¬¸ë‹¨ -> ë¬¸ì¥ -> ë‹¨ì–´ ìˆœ
        )
        docs = splitter.create_documents(texts=[text], metadatas=[{"strategy": "recursive"}])
        
        self._save_to_db(docs, "collection_recursive")
        return docs

    # ==========================================
    # ì „ëµ 3: ê³ ì • í† í° + ë¬¸ì¥ ë³´ì¡´ (Sliding Window)
    # ==========================================
    def strategy_token_sliding(self, text):
        print("\n[ì „ëµ 3] ê³ ì • í† í°(1000) + ì˜¤ë²„ë©(200) ì‹¤í–‰ ì¤‘...")
        
        # HuggingFace í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ 'í† í° ìˆ˜' ê¸°ì¤€ìœ¼ë¡œ ìë¦„
        splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
            tokenizer=self.tokenizer,
            chunk_size=1000,
            chunk_overlap=200,
        )
        docs = splitter.create_documents(texts=[text], metadatas=[{"strategy": "token_sliding"}])
        
        self._save_to_db(docs, "collection_token")
        return docs

    def _save_to_db(self, docs, collection_name):
        print(f"   ğŸ’¾ DB ì €ì¥ ì¤‘ ({collection_name})... {len(docs)}ê°œ ì¡°ê°")
        db = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory=DB_PATH
        )
        db.add_documents(docs)
        print("   âœ… ì €ì¥ ì™„ë£Œ!")

# ==========================================
# ì‹¤í–‰ë¶€
# ==========================================
if __name__ == "__main__":
    chunker = AdvancedChunker()
    text = chunker.load_file()
    
    if text:
        # 3ê°€ì§€ ì „ëµ ì‹¤í–‰
        docs_1 = chunker.strategy_entity_tagging(text)
        docs_2 = chunker.strategy_recursive(text)
        docs_3 = chunker.strategy_token_sliding(text)
        
        print("\n" + "="*50)
        print("ğŸ“Š [ê²°ê³¼ ë¹„êµ ë¦¬í¬íŠ¸]")
        print("="*50)
        
        # ì „ëµ 1 ê²°ê³¼ ìƒ˜í”Œ
        print(f"\n1ï¸âƒ£ [Entity íƒœê¹…] ì´ ì¡°ê° ìˆ˜: {len(docs_1)}")
        print(f"   ğŸ‘‰ ìƒ˜í”Œ ë©”íƒ€ë°ì´í„°: {docs_1[10].metadata}") 
        # ì˜ˆ: {'strategy': 'entity_tag', 'entities': ['ì•¨ë¦¬ìŠ¤', 'í† ë¼']}
        
        # ì „ëµ 2 ê²°ê³¼ ìƒ˜í”Œ
        print(f"\n2ï¸âƒ£ [Recursive] ì´ ì¡°ê° ìˆ˜: {len(docs_2)}")
        print(f"   ğŸ‘‰ ìƒ˜í”Œ ë‚´ìš© ê¸¸ì´: {len(docs_2[10].page_content)} ê¸€ì")
        
        # ì „ëµ 3 ê²°ê³¼ ìƒ˜í”Œ
        print(f"\n3ï¸âƒ£ [Token Sliding] ì´ ì¡°ê° ìˆ˜: {len(docs_3)}")
        # í† í° ìˆ˜ëŠ” ê¸€ì ìˆ˜ë³´ë‹¤ ì ê²Œ ë‚˜ì˜µë‹ˆë‹¤ (ë³´í†µ í•œê¸€ 1ê¸€ì = 1~2í† í°)
        print(f"   ğŸ‘‰ ìƒ˜í”Œ ë‚´ìš© ê¸¸ì´: {len(docs_3[10].page_content)} ê¸€ì (ì•½ 1000í† í°)") 
        
        print("\nâœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ./chroma_advanced_db ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")