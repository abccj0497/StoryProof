import os
import re
import json
import time
from typing import List, Dict, Any, Optional
from collections import defaultdict
from google import genai
from google.genai import types

class HybridSceneChunker:
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'ìˆ²', 'êµ´', 'ì •ì›', 'í™€', 'ë°”ë‹¤', 'ì§‘ì•ˆ', 'ë‚˜ë¬´', 'ì„±', 'ë§ˆì„', 'êµì‹¤', 'ë³µë„', 'ì°½ê°€', 'ë˜ì „', 'ì™•ê¶', 'ë²•ì •']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ê°‘ìê¸°', 'ë©°ì¹  ë’¤', 'ëª‡ ì‹œê°„ í›„', 'ìƒˆë²½', 'ì˜¤í›„', 'ê³„ì ˆì´', 'ì‹œê°„ì´', 'ì–´ëŠë§']
    CHAPTER_PATTERNS = [r"^\s*ì œ\s*[0-9]+\s*[ì¥í™”í¸]", r"^\s*Chapter\s*[0-9]+", r"^\s*\*\*\*"]

    def __init__(self, target_chars=3500, min_chars=1000, threshold=5, overlap_chars=200):
        self.target_chars = target_chars
        self.min_chars = min_chars
        self.threshold = threshold
        self.overlap_chars = overlap_chars 

    def _calculate_score(self, text_segment):
        score = 0
        if any(k in text_segment for k in self.LOCATION_KEYWORDS): score += 5
        if any(k in text_segment for k in self.TIME_TRANSITIONS): score += 4
        return score

    def split_content(self, text: str) -> List[str]:
        text = text.replace('\r\n', '\n')
        paragraphs = re.split(r'\n\s*\n', text)
        final_scenes, current_scene, current_len = [], [], 0
        
        for para in paragraphs:
            para = para.strip()
            if not para: continue
            
            is_chapter = any(re.match(p, para, re.IGNORECASE) for p in self.CHAPTER_PATTERNS)
            score = self._calculate_score(para[:50])

            if (is_chapter and current_len > 0) or \
               (score >= self.threshold and current_len >= self.min_chars) or \
               (current_len + len(para) > self.target_chars):
                
                full_scene_text = "\n\n".join(current_scene)
                final_scenes.append(full_scene_text)
                
                current_scene = [full_scene_text[-self.overlap_chars:]] if len(full_scene_text) > self.overlap_chars else []
                current_scene.append(para)
                current_len = len(para)
            else:
                current_scene.append(para)
                current_len += len(para)

        if current_scene: final_scenes.append("\n\n".join(current_scene))
        return final_scenes

class StoryAnalyzer:
    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash"):
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        ë‹¹ì‹ ì€ ì†Œì„¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
        
        [í•„ìˆ˜ ì§€ì¹¨]
        1. 'Character'ì—ëŠ” ì‘ê°€, ì±… ì œëª©ì„ ì ˆëŒ€ ë„£ì§€ ë§ˆì„¸ìš”.
        2. 'summary'ëŠ” ìœ¡í•˜ì›ì¹™ì— ë”°ë¼ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
        3. 'atmosphere'ëŠ” ë¶„ìœ„ê¸°ë¥¼ ë‹¨ì–´ í˜•íƒœë¡œ(ì˜ˆ: ê¸´ë°•í•œ, í‰í™”ë¡œìš´) ì¶”ì¶œí•˜ì„¸ìš”.

        [TEXT]
        {chunk['text'][:4500]}
        
        [OUTPUT JSON FORMAT]
        {{
          "book_info": {{ "title": "ì†Œì„¤ ì œëª©", "author": "ì‘ê°€ ì´ë¦„" }},
          "scene_title": "ì†Œì œëª©",
          "summary": "ìš”ì•½ë¬¸",
          "atmosphere": "ë¶„ìœ„ê¸°",
          "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
          "entities": [
            {{ "name": "ì´ë¦„", "type": "Character/Place/Item", "desc": "íŠ¹ì§•", "action": "í–‰ë™" }}
          ]
        }}
        """
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            result = json.loads(response.text)
            if isinstance(result, list): result = result[0]
            
            result['scene_id'] = chunk['id'] 
            
            if 'atmosphere' not in result: result['atmosphere'] = "N/A"
            if 'keywords' not in result: result['keywords'] = []
            return result
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨ ({chunk['id']}): {e}")
            return {"scene_id": chunk['id'], "error": str(e)}

class WikiGenerator:
    @staticmethod
    def generate_markdown(storyboard_list: List[Dict]) -> str:
        if not storyboard_list: return ""

        # 1. scene_id ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        storyboard_list.sort(key=lambda x: x.get('scene_id', ''))

        valid_scenes = [s for s in storyboard_list if 'book_info' in s]
        if not valid_scenes: return ""
        
        first_valid = valid_scenes[0]
        book_title = first_valid.get('book_info', {}).get('title', 'Unknown Title')
        author = first_valid.get('book_info', {}).get('author', 'Unknown Author')
        
        wiki = defaultdict(lambda: defaultdict(list))
        
        for scene in storyboard_list:
            s_id = scene.get('scene_id', 'unknown')
            if 'error' in scene: continue

            for ent in scene.get('entities', []):
                if ent['name'] in [book_title, author, "Project", "Book", "Unknown"]: continue
                wiki[ent['type']][ent['name']].append({
                    "scene": s_id, 
                    "desc": ent['desc'], 
                    "action": ent['action']
                })

        lines = []
        lines.append(f"# ğŸ“˜ {book_title} - ê³µì‹ ì„¤ì •ì§‘\n")
        lines.append(f"**Sorted & Organized by StoryProof AI**\n\n")
        
        lines.append("## ğŸ“‘ ëª©ì°¨\n")
        lines.append("1. [ìŠ¤í† ë¦¬ë¼ì¸ (Timeline)](#1-ìŠ¤í† ë¦¬ë¼ì¸-timeline)\n")
        lines.append("2. [ë“±ì¥ì¸ë¬¼ ìƒì„¸ (Characters)](#2-ë“±ì¥ì¸ë¬¼-ìƒì„¸-characters)\n")
        lines.append("3. [ì•„ì´í…œ & ì¥ì†Œ (Items & Places)](#3-ì•„ì´í…œ--ì¥ì†Œ-items--places)\n\n---\n\n")
        
        lines.append(f"## 1. ìŠ¤í† ë¦¬ë¼ì¸ (Timeline)\n")
        for scene in storyboard_list:
            if 'error' in scene: continue
            lines.append(f"### ğŸ¬ **[{scene.get('scene_id')}] {scene.get('scene_title','')}**\n")
            lines.append(f"> {scene.get('summary','')}\n\n")
        lines.append("---\n\n")

        lines.append(f"## 2. ë“±ì¥ì¸ë¬¼ ìƒì„¸ (Characters)\n")
        char_items = wiki.get("Character", {})
        
        if not char_items:
            lines.append("_ë°ì´í„° ì—†ìŒ_\n")
        else:
            sorted_chars = sorted(char_items.items())
            for name, details in sorted_chars:
                lines.append(f"### ğŸ‘¤ {name}\n")
                details.sort(key=lambda x: x['scene'])
                for d in details:
                    lines.append(f"- `{d['scene']}`\n")
                    lines.append(f"  - **íŠ¹ì§•:** {d['desc']}\n")
                    lines.append(f"  - **í–‰ë™:** {d['action']}\n")
                lines.append("\n")
        lines.append("---\n\n")

        lines.append(f"## 3. ì•„ì´í…œ & ì¥ì†Œ (Items & Places)\n")
        for key in ["Item", "Place"]:
            items = wiki.get(key, {})
            if not items: continue
            lines.append(f"### ğŸ”¹ {key}\n")
            sorted_items = sorted(items.items())
            
            for name, details in sorted_items:
                details.sort(key=lambda x: x['scene'])
                first_desc = details[0]['desc']
                scene_list = ", ".join([d['scene'].replace('scene_', '') for d in details])
                
                lines.append(f"- **{name}** (ë“±ì¥: {len(details)}íšŒ)\n")
                lines.append(f"  - ì„¤ëª…: {first_desc}\n")
                lines.append(f"  - ë“±ì¥: [{scene_list}]\n\n")

        return "".join(lines)

def run_scene_analysis(text: str, api_key: str, output_dir: str):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    scene_dir = os.path.join(output_dir, "scenes")
    if not os.path.exists(scene_dir):
        os.makedirs(scene_dir)

    # 1. Chunking
    chunker = HybridSceneChunker()
    chunks = chunker.split_content(text)
    scene_data = [{'id': f"scene_{i+1:03d}", 'text': txt} for i, txt in enumerate(chunks)]

    # 2. Save raw chunks
    for scene in scene_data:
        with open(os.path.join(scene_dir, f"{scene['id']}.txt"), "w", encoding="utf-8") as f:
            f.write(scene['text'])

    # 3. Analyze
    analyzer = StoryAnalyzer(api_key)
    results = []
    
    for chunk in scene_data:
        res = analyzer.analyze(chunk)
        if res: results.append(res)
        time.sleep(0.5) # Slight delay to avoid aggressive rate limiting

    if results:
        # Save JSON
        json_path = os.path.join(output_dir, "storyboard_analysis.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # Save Markdown
        wiki_gen = WikiGenerator()
        markdown_content = wiki_gen.generate_markdown(results)
        md_path = os.path.join(output_dir, "writer_bible.md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)
        
        return {
            "status": "completed",
            "json_path": json_path,
            "md_path": md_path,
            "scene_count": len(results)
        }
    else:
        return {"status": "failed", "message": "No results generated"}
