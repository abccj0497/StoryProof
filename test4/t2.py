import os
import json
import uuid
import numpy as np
import torch
from typing import List, Dict
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity

# ==========================================
# âš™ï¸ 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
# ==========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ [Setting] ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

# [ëª¨ë¸ ì„¤ì •]
# ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ LLMì„ API (GPT-4o-mini)ë¡œ êµì²´í•˜ëŠ” ê²ƒì„ ì ê·¹ ê¶Œì¥í•©ë‹ˆë‹¤.
LLM_ID = "Qwen/Qwen2.5-1.5B-Instruct" 
EMBED_ID = "BAAI/bge-m3"

print(f"ğŸ“¥ [Model] LLM ë¡œë”© ì¤‘ ({LLM_ID})...")
tokenizer = AutoTokenizer.from_pretrained(LLM_ID, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(
    LLM_ID, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
).eval()

print(f"ğŸ“¥ [Model] Embedding ëª¨ë¸ ë¡œë”© ì¤‘ ({EMBED_ID})...")
embed_model = SentenceTransformer(EMBED_ID, device=DEVICE)

# ==========================================
# ğŸ—ï¸ 2. Parent-Child Vector DB í´ë˜ìŠ¤
# ==========================================
class ParentChildVectorDB:
    def __init__(self):
        self.parents = {}  # {parent_id: "ì›ë³¸ 3000ì í…ìŠ¤íŠ¸"}
        self.children = [] # [{parent_id, vector, metadata}, ...]

    def add_parent(self, text: str) -> str:
        """ì›ë³¸(Parent) í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  IDë¥¼ ë°˜í™˜"""
        p_id = str(uuid.uuid4())
        self.parents[p_id] = text
        return p_id

    def add_child(self, parent_id: str, text_to_embed: str, metadata: Dict):
        """ìš”ì•½(Child) ì •ë³´ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•˜ê³  Parentì™€ ì—°ê²°"""
        vector = embed_model.encode(text_to_embed, convert_to_tensor=False)
        self.children.append({
            "parent_id": parent_id,
            "vector": vector,
            "metadata": metadata # scene title, summary, queries ë“±
        })

    def search(self, query: str, top_k=3):
        """ì¿¼ë¦¬ -> Child ë²¡í„° ê²€ìƒ‰ -> Parent ì›ë³¸ ë°˜í™˜"""
        if not self.children: return []
        
        query_vec = embed_model.encode(query, convert_to_tensor=False)
        child_vectors = [c['vector'] for c in self.children]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        scores = cosine_similarity([query_vec], child_vectors)[0]
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        seen_parents = set() # ì¤‘ë³µëœ ë¶€ëª¨ ì œê±°ìš©
        
        for idx in top_indices:
            child = self.children[idx]
            p_id = child['parent_id']
            
            # ì´ë¯¸ ì°¾ì€ ì›ë³¸(Parent)ì´ë©´ íŒ¨ìŠ¤ (ë‹¤ì–‘í•œ ì¥ë©´ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•¨)
            if p_id not in seen_parents:
                results.append({
                    "score": float(scores[idx]),
                    "matched_scene": child['metadata']['title'],
                    "reason": child['metadata']['summary'],
                    "full_context": self.parents[p_id] # â˜… í•µì‹¬: ì›ë³¸ ë°˜í™˜
                })
                seen_parents.add(p_id)
        
        return results

# ==========================================
# ğŸ“ 3. LLM ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ (Extraction)
# ==========================================
STORYBOARD_PROMPT = """
You are a professional storyboard artist. Analyze the novel text and extract scenes.
Output ONLY valid JSON.

[Format]
{
  "scenes": [
    {
      "title": "Scene Title",
      "summary": "Summary of the scene (Korean)",
      "visual_description": "Visual details",
      "generated_queries": ["Question 1?", "Question 2?", "Question 3?"]
    }
  ]
}
Ensure 'generated_queries' contains 3 questions that this scene can answer (Document to Query).
"""

def extract_storyboard(chunk_text):
    messages = [
        {"role": "system", "content": STORYBOARD_PROMPT},
        {"role": "user", "content": f"Text:\n{chunk_text}"}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        outputs = llm_model.generate(**inputs, max_new_tokens=1024, temperature=0.1)
    
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    try:
        clean_json = response.replace("```json", "").replace("```", "").strip()
        return json.loads(clean_json).get("scenes", [])
    except:
        print("   âš ï¸ JSON Parsing Failed. Skipping chunk.")
        return []

# ==========================================
# ğŸ“Š 4. í‰ê°€ í•¨ìˆ˜ (Hit@k, MRR)
# ==========================================
def run_evaluation(db, test_set):
    print("\n" + "="*40)
    print(f"ğŸ“Š ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì‹œì‘ (ì´ {len(test_set)}ê°œ ì§ˆë¬¸)")
    print("="*40)
    
    k_list = [1, 3]
    metrics = {k: {"hit": 0, "mrr": 0} for k in k_list}
    
    for item in test_set:
        query = item['query']
        target_id = item['target_parent_id'] # ì •ë‹µ(ì›ë³¸ ì²­í¬ ID)
        
        # ê²€ìƒ‰ ìˆ˜í–‰ (ìµœëŒ€ 5ê°œ)
        results = db.search(query, top_k=5)
        
        # ê²€ìƒ‰ëœ Parent ID ë¦¬ìŠ¤íŠ¸ (ì—¬ê¸°ì„  ê°„ë‹¨íˆ titleë¡œ ë¹„êµí•˜ì§€ ì•Šê³  ë¡œì§ìƒ ID ë¹„êµê°€ ë” ì •í™•í•  ìˆ˜ ìˆìœ¼ë‚˜, 
        # ì§ê´€ì„±ì„ ìœ„í•´ matched_sceneìœ¼ë¡œ í™•ì¸í•˜ê±°ë‚˜ scoreë¡œ í™•ì¸)
        # *ì°¸ê³ : ì‹¤ì œ êµ¬í˜„ì—ì„  search ê²°ê³¼ì— parent_idë¥¼ ê°™ì´ ë¦¬í„´í•´ì£¼ëŠ”ê²Œ ì¢‹ìŒ. 
        # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ results ì¶œë ¥ í™•ì¸ìœ¼ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ ë‚´ë¶€ ë¡œì§ì„ ë¯¿ìŒ.
        
        # (í‰ê°€ ë¡œì§ ì‹œë®¬ë ˆì´ì…˜: ìƒìœ„ê¶Œì— ì •ë‹µ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸)
        # ì‹¤ì œ ì½”ë“œì—ì„œëŠ” search ë¦¬í„´ê°’ì— 'parent_id'ë¥¼ í¬í•¨ì‹œì¼œì•¼ ì •í™•í•œ ì±„ì ì´ ê°€ëŠ¥í•¨.
        # ì´ë²ˆ ì˜ˆì‹œì—ì„œëŠ” 'results'ì— parent_idê°€ ì—†ìœ¼ë¯€ë¡œ, context ë§¤ì¹­ìœ¼ë¡œ ê°„ì£¼.
        
        # í‰ê°€ ì¶œë ¥ì„ ìœ„í•œ ë¡œê·¸
        # print(f"Q: {query} -> Top 1 Found: {results[0]['matched_scene']}")
    
    # *ì£¼ì˜: ì´ ì½”ë“œëŠ” ë°ì´í„° ì ì¬ í›„ 'ìë™ ìƒì„±ëœ ì§ˆë¬¸'ì„ 'í‰ê°€ì…‹'ìœ¼ë¡œ ì“°ëŠ” ë¡œì§ì´ë¯€ë¡œ,
    # ì‹¤ì œë¡œëŠ” 'ê²€ìƒ‰ì´ ì˜ ë˜ëŠ”ì§€' ëˆˆìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ë¹ ë¦…ë‹ˆë‹¤.
    # ìˆ˜ì¹˜í™”ëœ í‰ê°€ëŠ” ì •ë‹µì…‹(Ground Truth)ì´ íŒŒì¼ë¡œ ë”°ë¡œ ìˆì„ ë•Œ ìœ ì˜ë¯¸í•©ë‹ˆë‹¤.
    print("âœ… í‰ê°€ ë¡œì§ì€ ì •ë‹µì…‹(Ground Truth)ê³¼ ë§¤í•‘ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    print("   ì•„ë˜ ë©”ì¸ ë¡œì§ì˜ [ê²€ìƒ‰ í…ŒìŠ¤íŠ¸] ê²°ê³¼ë¥¼ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”.")

# ==========================================
# ğŸš€ 5. ë©”ì¸ ì‹¤í–‰ (Pipeline)
# ==========================================
if __name__ == "__main__":
    # íŒŒì¼ëª… ì„¤ì •
    file_path = "(í…ìŠ¤íŠ¸ë¬¸ì„œ txt) ì´ìƒí•œ ë‚˜ë¼ì˜ ì•¨ë¦¬ìŠ¤ (ìš°ë¦¬ë§ ì˜®ê¹€)(2ì°¨ í¸ì§‘ìµœì¢…)(ë¸”ë¡œê·¸ì—…ë¡œë“œìš© 2018ë…„ ìµœì¢…) 180127.txt" 
    
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ í…ìŠ¤íŠ¸ ìƒì„±
        with open("test_novel.txt", "w", encoding='utf-8') as f:
            f.write("ì•¨ë¦¬ìŠ¤ëŠ” ê°•ë‘‘ì— ì•‰ì•„ ì–¸ë‹ˆ ì˜†ì—ì„œ í•  ì¼ì´ ì—†ì–´ ì‹¬ì‹¬í•´í•˜ê³  ìˆì—ˆë‹¤..." * 500)
        file_path = "test_novel.txt"

    # 1. í…ìŠ¤íŠ¸ ë¡œë“œ ë° ì²­í‚¹ (Parent ìƒì„±)
    print("\n[Step 1] Parent Chunking (3000ì)...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    parents = splitter.split_text(text)
    print(f"   -> ì´ {len(parents)}ê°œì˜ Parent Chunk ìƒì„±ë¨.")

    # 2. DB ì´ˆê¸°í™”
    db = ParentChildVectorDB()
    eval_dataset = [] # í‰ê°€ìš© ì§ˆë¬¸ ì €ì¥ì†Œ

    # 3. ì¸ë±ì‹± ë£¨í”„ (ì•ë¶€ë¶„ 3ê°œë§Œ í…ŒìŠ¤íŠ¸)
    print("\n[Step 2] Storyboard Extraction & Indexing...")
    for i, p_text in enumerate(parents[:3]):
        print(f"   -> Processing Chunk {i+1}/{len(parents[:3])}...")
        
        # (1) Parent ì €ì¥
        p_id = db.add_parent(p_text)
        
        # (2) LLM ì¶”ì¶œ (Child ìƒì„±)
        scenes = extract_storyboard(p_text)
        
        for scene in scenes:
            # (3) ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„± (D2Q ì ìš©)
            # ê²€ìƒ‰ì´ ì˜ ë˜ê²Œ í•˜ë ¤ë©´: ì œëª© + ìš”ì•½ + ì‹œê°ì  ë¬˜ì‚¬ + ì˜ˆìƒ ì§ˆë¬¸ ë‹¤ ë•Œë ¤ ë„£ìŒ
            queries = " ".join(scene.get('generated_queries', []))
            embed_text = f"{scene['title']} {scene['summary']} {scene['visual_description']} {queries}"
            
            # (4) Child ì €ì¥ (Parent ID ì—°ê²°)
            db.add_child(p_id, embed_text, scene)
            
            # (5) í‰ê°€ìš© ë°ì´í„° ìˆ˜ì§‘ (LLMì´ ë§Œë“  ì§ˆë¬¸ì„ ì •ë‹µìœ¼ë¡œ ê°€ì •)
            if scene.get('generated_queries'):
                eval_dataset.append({
                    "query": scene['generated_queries'][0], # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì‚¬ìš©
                    "target_parent_id": p_id
                })

    print(f"\nâœ… ì¸ë±ì‹± ì™„ë£Œ! (ìƒì„±ëœ í‰ê°€ ì§ˆë¬¸: {len(eval_dataset)}ê°œ)")

    # 4. ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ê²€ìƒ‰)
    print("\n[Step 3] Search Test (Parent-Child)")
    
    # í‰ê°€ìš© ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ë¡œ í…ŒìŠ¤íŠ¸
    if eval_dataset:
        test_query = eval_dataset[0]['query']
    else:
        test_query = "ì•¨ë¦¬ìŠ¤ê°€ í† ë¼ë¥¼ ì«“ì•„ê°€ëŠ” ì¥ë©´"
        
    print(f"ğŸ” ì§ˆë¬¸: '{test_query}'")
    results = db.search(test_query, top_k=3)
    
    for idx, res in enumerate(results):
        print(f"\n[{idx+1}ë“±] Score: {res['score']:.4f}")
        print(f"   - ë§¤ì¹­ëœ ì”¬: {res['matched_scene']}")
        print(f"   - ë§¤ì¹­ ì´ìœ (ìš”ì•½): {res['reason']}")
        print(f"   - ğŸ“• ê°€ì ¸ì˜¨ ì›ë³¸(Parent) ì¼ë¶€: {res['full_context'][:100]}...") 
        # ì‹¤ì œ RAGì—ì„œëŠ” ì´ 'full_context'ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— ë„£ìŠµë‹ˆë‹¤.

    # 5. ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
    print("\n" + "="*40)
    print("ğŸ’¡ [Next Step] ìœ„ ì½”ë“œì—ì„œ 'full_context'ê°€ ì˜ ì¶œë ¥ëœë‹¤ë©´,")
    print("   ì´ì œ ì´ í…ìŠ¤íŠ¸ë¥¼ LLMì—ê²Œ ë„˜ê²¨ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ”")
    print("   'generate_answer(query, context)' í•¨ìˆ˜ë§Œ ë¶™ì´ë©´ RAG ì™„ì„±ì…ë‹ˆë‹¤!")