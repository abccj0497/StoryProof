import os
import json
import uuid
import numpy as np
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity

# ==========================================
# âš™ï¸ 0. ì„¤ì •
# ==========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[*] Device: {DEVICE}")

# ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ gpt-4o-mini API ì‚¬ìš© ê¶Œì¥)
LLM_ID = "Qwen/Qwen2.5-1.5B-Instruct" 
EMBED_ID = "BAAI/bge-m3"

tokenizer = AutoTokenizer.from_pretrained(LLM_ID, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(
    LLM_ID, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
).eval()
embed_model = SentenceTransformer(EMBED_ID, device=DEVICE)

# ==========================================
# ğŸ§© 1. ë°ì´í„° í´ë˜ìŠ¤ (Parent-Child êµ¬ì¡°)
# ==========================================
class DocumentStore:
    def __init__(self):
        self.parents = {}  # {parent_id: ì›ë³¸_3000ì_í…ìŠ¤íŠ¸}
        self.children = [] # [{parent_id, vector, metadata(ìš”ì•½,ì§ˆë¬¸)}, ...]

    def add_parent(self, text):
        p_id = str(uuid.uuid4())
        self.parents[p_id] = text
        return p_id

    def add_child(self, parent_id, vector, metadata):
        self.children.append({
            "parent_id": parent_id,
            "vector": vector,
            "metadata": metadata
        })

    def search(self, query, top_k=3):
        # 1. ì¿¼ë¦¬ ë²¡í„°í™”
        query_vec = embed_model.encode(query, convert_to_tensor=False)
        
        # 2. Child ë²¡í„°ë“¤ê³¼ ìœ ì‚¬ë„ ê²€ìƒ‰
        child_vectors = [c['vector'] for c in self.children]
        if not child_vectors: return []
        
        scores = cosine_similarity([query_vec], child_vectors)[0]
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        # 3. Childë¥¼ í†µí•´ Parent(ì›ë³¸) ì°¾ê¸° (Lift)
        results = []
        seen_parents = set()
        
        for idx in top_indices:
            child = self.children[idx]
            p_id = child['parent_id']
            
            # ì¤‘ë³µëœ ë¶€ëª¨ëŠ” ì œê±° (ê°™ì€ 3000ì ì•ˆì—ì„œ ì—¬ëŸ¬ ì”¬ì´ ê²€ìƒ‰ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
            if p_id not in seen_parents:
                results.append({
                    "score": float(scores[idx]),
                    "summary_found": child['metadata']['summary'], # ê²€ìƒ‰ëœ ì´ìœ (ìš”ì•½)
                    "original_context": self.parents[p_id] # â˜… ì§„ì§œ í•„ìš”í•œ ì›ë³¸
                })
                seen_parents.add(p_id)
                
        return results

# ==========================================
# ğŸ› ï¸ 2. í•µì‹¬ ë¡œì§
# ==========================================

# (1) 3000ì ì²­í‚¹ (Parent ìƒì„±)
def create_parent_chunks(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=3000,
        chunk_overlap=200, # ë¬¸ë§¥ ëŠê¹€ ë°©ì§€ìš© ì•½ê°„ì˜ ì˜¤ë²„ë©
    )
    return splitter.split_text(text)

# (2) LLM ì¶”ì¶œ (Child ë°ì´í„° ìƒì„±)
def extract_storyboard_nodes(parent_text):
    system_prompt = """
    ì†Œì„¤ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  'ì¥ë©´(Scene)' ë‹¨ìœ„ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì‹œì˜¤.
    ê° ì¥ë©´ë³„ë¡œ 'summary'(ìš”ì•½), 'generated_queries'(ì˜ˆìƒ ì§ˆë¬¸ 3ê°œ)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì‹œì˜¤.
    """
    
    user_prompt = f"Text:\n{parent_text}\n\nOutput JSON format:\n{{ 'scenes': [ {{ 'title': '...', 'summary': '...', 'generated_queries': ['Q1', 'Q2'] }} ] }}"
    
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        outputs = llm_model.generate(**inputs, max_new_tokens=1024, temperature=0.1)
    
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    try:
        clean_json = response.replace("```json", "").replace("```", "").strip()
        return json.loads(clean_json).get("scenes", [])
    except:
        return []

# ==========================================
# ğŸš€ 3. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ==========================================
if __name__ == "__main__":
    db = DocumentStore()
    file_name = "(í…ìŠ¤íŠ¸ë¬¸ì„œ txt) ì´ìƒí•œ ë‚˜ë¼ì˜ ì•¨ë¦¬ìŠ¤ (ìš°ë¦¬ë§ ì˜®ê¹€)(2ì°¨ í¸ì§‘ìµœì¢…)(ë¸”ë¡œê·¸ì—…ë¡œë“œìš© 2018ë…„ ìµœì¢…) 180127.txt"

    # 1. Parent Chunking
    if os.path.exists(file_name):
        parents = create_parent_chunks(file_name)
        print(f"[*] 3000ì ë‹¨ìœ„ Parent ìƒì„± ì™„ë£Œ: {len(parents)}ê°œ")

        # 2. Process Loop
        for i, parent_text in enumerate(parents[:2]): # í…ŒìŠ¤íŠ¸ìš© 2ê°œë§Œ
            print(f"Processing Parent Chunk {i+1}...")
            
            # DBì— Parent(ì›ë³¸) ì €ì¥
            p_id = db.add_parent(parent_text)
            
            # LLMìœ¼ë¡œ Child(ìŠ¤í† ë¦¬ë³´ë“œ) ì¶”ì¶œ
            scenes = extract_storyboard_nodes(parent_text)
            
            for scene in scenes:
                # 3. Child Embedding (ìš”ì•½ + ì§ˆë¬¸ + ì œëª©)
                # ì´ê²ƒì´ ê²€ìƒ‰ì˜ 'í‚¤'ê°€ ë¨ (D2Q ì ìš©)
                search_key = f"{scene['title']} {scene['summary']} {' '.join(scene.get('generated_queries', []))}"
                vector = embed_model.encode(search_key)
                
                # DBì— Child ì €ì¥ (Parent ID ì—°ê²°)
                db.add_child(p_id, vector, scene)
                
        print("[*] ì¸ë±ì‹± ì™„ë£Œ.")
        
        # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        query = "ì•¨ë¦¬ìŠ¤ê°€ êµ´ ì†ìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” ì¥ë©´"
        results = db.search(query)
        
        print("\n[ê²€ìƒ‰ ê²°ê³¼]")
        for res in results:
            print(f"Score: {res['score']:.4f}")
            print(f"Found via: {res['summary_found']}")
            print(f"Retrieved Parent Context: {res['original_context'][:100]}...") # ì›ë³¸ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
            print("-" * 30)