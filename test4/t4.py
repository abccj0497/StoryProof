import os
import json
import uuid
import numpy as np
import torch
from typing import List, Dict, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity

# ==========================================
# âš™ï¸ 0. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
# ==========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ [System] ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

# [ëª¨ë¸ ì„¤ì •]
LLM_ID = "Qwen/Qwen2.5-1.5B-Instruct" 
EMBED_ID = "BAAI/bge-m3"

print(f"ğŸ“¥ [Model] LLM ë¡œë”© ì¤‘ ({LLM_ID})...")
try:
    tokenizer = AutoTokenizer.from_pretrained(LLM_ID, trust_remote_code=True)
    llm_model = AutoModelForCausalLM.from_pretrained(
        LLM_ID, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
    ).eval()
except Exception as e:
    print(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

print(f"ğŸ“¥ [Model] Embedding ëª¨ë¸ ë¡œë”© ì¤‘ ({EMBED_ID})...")
embed_model = SentenceTransformer(EMBED_ID, device=DEVICE)

# ==========================================
# ğŸ—ï¸ 1. Parent-Child Vector DB í´ë˜ìŠ¤
# ==========================================
class ParentChildVectorDB:
    def __init__(self):
        # Parent: ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥ì†Œ (3000ì ì²­í¬)
        self.parents = {}  # {parent_id: "ì›ë³¸ í…ìŠ¤íŠ¸"}
        # Child: ê²€ìƒ‰ìš© ë²¡í„° ì €ì¥ì†Œ (ìŠ¤í† ë¦¬ë³´ë“œ ìš”ì•½ ì •ë³´)
        self.children = [] # [{parent_id, vector, metadata}, ...]

    def add_parent(self, text: str) -> str:
        """ì›ë³¸(Parent) í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  ê³ ìœ  ID ë°˜í™˜"""
        p_id = str(uuid.uuid4())
        self.parents[p_id] = text
        return p_id

    def add_child(self, parent_id: str, text_to_embed: str, metadata: Dict):
        """ìš”ì•½(Child) ì •ë³´ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•˜ê³  Parentì™€ ì—°ê²°"""
        vector = embed_model.encode(text_to_embed, convert_to_tensor=False)
        self.children.append({
            "parent_id": parent_id,
            "vector": vector,
            "metadata": metadata 
        })

    def search(self, query: str, top_k=3) -> List[Dict]:
        """ì¿¼ë¦¬ -> Child ë²¡í„° ê²€ìƒ‰ -> Parent ì›ë³¸ ë°˜í™˜"""
        if not self.children: return []
        
        query_vec = embed_model.encode(query, convert_to_tensor=False)
        child_vectors = [c['vector'] for c in self.children]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        scores = cosine_similarity([query_vec], child_vectors)[0]
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        seen_parents = set()
        
        for idx in top_indices:
            child = self.children[idx]
            p_id = child['parent_id']
            
            # ì¤‘ë³µëœ Parent ì œê±° (ë‹¤ì–‘í•œ ê²€ìƒ‰ ê²°ê³¼ ë³´ì¥)
            if p_id not in seen_parents:
                results.append({
                    "score": float(scores[idx]),
                    "parent_id": p_id,  # ğŸ‘ˆ í‰ê°€ë¥¼ ìœ„í•´ ë°˜ë“œì‹œ í•„ìš”
                    "matched_scene": child['metadata']['title'],
                    "summary": child['metadata']['summary'],
                    "full_context": self.parents[p_id] # â˜… ì›ë³¸ ë°˜í™˜
                })
                seen_parents.add(p_id)
        
        return results

# ==========================================
# ğŸ“ 2. ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ & í•¨ìˆ˜
# ==========================================
STORYBOARD_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì˜í™” ìŠ¤í† ë¦¬ë³´ë“œ ì‘ê°€ì…ë‹ˆë‹¤. ì†Œì„¤ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  'ì¥ë©´(Scene)' ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ì„ ì—„ê²©í•˜ê²Œ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤.

[JSON ì¶œë ¥ í¬ë§·]
{
  "scenes": [
    {
      "scene_id": "unique_id_1",
      "title": "ì¥ë©´ ì œëª©",
      "summary": "ì¥ë©´ì˜ í•µì‹¬ ì¤„ê±°ë¦¬ ìš”ì•½ (í•œê¸€)",
      "characters": ["ë“±ì¥ì¸ë¬¼1", "ë“±ì¥ì¸ë¬¼2"],
      "location": "ì¥ì†Œ",
      "time": "ì‹œê°„ì  ë°°ê²½",
      "visual_description": "ì¥ë©´ì„ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë¦´ ë•Œ í•„ìš”í•œ ì‹œê°ì  ë¬˜ì‚¬",
      "mood": "ë¶„ìœ„ê¸° (ì˜ˆ: ê¸´ì¥ê°, í‰í™”ë¡œì›€)",
      "generated_queries": ["ì´ ì¥ë©´ì„ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ ì§ˆë¬¸1", "ê²€ìƒ‰ ì§ˆë¬¸2", "ê²€ìƒ‰ ì§ˆë¬¸3"] 
    }
  ]
}

ì£¼ì˜ì‚¬í•­:
1. ì˜¤ì§ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´(```json)ì„ ë¶™ì´ì§€ ë§ˆì„¸ìš”.
2. ëª¨ë“  ë‚´ìš©ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”.
3. 'generated_queries'ëŠ” Document-to-Query(D2Q)ë¥¼ ìœ„í•´ ë°˜ë“œì‹œ 3ê°œ ì´ìƒ ì‘ì„±í•˜ì„¸ìš”.
"""

def extract_storyboard(chunk_text: str) -> List[Dict]:
    messages = [
        {"role": "system", "content": STORYBOARD_SYSTEM_PROMPT},
        {"role": "user", "content": f"ë‹¤ìŒ ì†Œì„¤ ë‚´ìš©ì„ ë¶„ì„í•´ ìŠ¤í† ë¦¬ë³´ë“œ JSONì„ ë§Œë“œì‹œì˜¤:\n\n{chunk_text}"}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        outputs = llm_model.generate(
            **inputs, 
            max_new_tokens=2048, 
            temperature=0.1,
            do_sample=True
        )
    
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    try:
        clean_json = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(clean_json)
        return data.get("scenes", [])
    except json.JSONDecodeError:
        print("   âš ï¸ [Error] JSON íŒŒì‹± ì‹¤íŒ¨. ëª¨ë¸ì´ í˜•ì‹ì„ ì§€í‚¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

# ==========================================
# ğŸ’¾ 3. íŒŒì¼ ì €ì¥ ìœ í‹¸ë¦¬í‹°
# ==========================================
def save_results_to_json(all_scenes: List[Dict], filename="storyboard_output.json"):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_scenes, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ [Save] ì¶”ì¶œ ê²°ê³¼ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ==========================================
# ğŸ“Š 4. ì •ëŸ‰ì  í‰ê°€ í•¨ìˆ˜ (Detailed Metrics)
# ==========================================
def evaluate_retrieval(db, eval_dataset: List[Dict], k_values=[1, 3, 5]):
    """
    Hit@k ë° MRR@kë¥¼ ê³„ì‚°í•˜ê³ , ì´ˆê¸° ëª‡ ê°œì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë””ë²„ê¹… ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*60)
    print(f"ğŸ“Š ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì‹œì‘ (Hit@k, MRR@k) - ì´ {len(eval_dataset)}ê°œ ì§ˆë¬¸")
    print("="*60)
    
    # ì ìˆ˜ ì €ì¥ì†Œ ì´ˆê¸°í™”
    scores = {k: {"hit": 0, "mrr": 0} for k in k_values}
    
    for i, item in enumerate(eval_dataset):
        query = item['query']
        target_id = item['target_parent_id'] # ì •ë‹µ(ì›ë³¸ ë¶€ëª¨ ID)
        
        # ê°€ì¥ í° kë§Œí¼ ê²€ìƒ‰ ìˆ˜í–‰
        max_k = max(k_values)
        results = db.search(query, top_k=max_k)
        
        # ê²€ìƒ‰ëœ Parent ID ëª©ë¡ ì¶”ì¶œ
        retrieved_ids = [res['parent_id'] for res in results]
        
        # ğŸ” ë””ë²„ê¹…ìš© ë¡œê·¸ (ì²« 3ê°œ ì§ˆë¬¸ë§Œ ìì„¸íˆ ì¶œë ¥)
        if i < 3:
            print(f"ğŸ” [Test Q{i+1}] ì§ˆë¬¸: {query}")
            print(f"    - ì •ë‹µ ID (Target): ...{target_id[-8:]}")
            print(f"    - ê²€ìƒ‰ ID (Top {max_k}): {[rid[-8:] for rid in retrieved_ids]}")
            
            # ì •ë‹µ ì—¬ë¶€ í‘œì‹œ
            is_hit = target_id in retrieved_ids
            status = "âœ… ì„±ê³µ" if is_hit else "âŒ ì‹¤íŒ¨"
            print(f"    - ê²°ê³¼: {status}")
            print("-" * 40)

        # ì§€í‘œ ê³„ì‚° ë¡œì§
        for k in k_values:
            # ìƒìœ„ kê°œë§Œ ìŠ¬ë¼ì´ì‹±
            top_k_ids = retrieved_ids[:k]
            
            # 1. Hit@k ê³„ì‚°
            if target_id in top_k_ids:
                scores[k]["hit"] += 1
                
                # 2. MRR@k ê³„ì‚° (Hití•œ ê²½ìš°ì—ë§Œ ê³„ì‚°)
                # indexëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1 í•˜ì—¬ ìˆœìœ„(rank)ë¥¼ êµ¬í•¨
                rank = top_k_ids.index(target_id) + 1
                scores[k]["mrr"] += (1.0 / rank)
    
    # ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥
    print("\nğŸ“ˆ [ìµœì¢… í‰ê°€ ì„±ì í‘œ]")
    total = len(eval_dataset)
    if total == 0:
        print("í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"{'Metric':<10} | {'Hit Score':<12} | {'MRR Score':<12}")
    print("-" * 40)
    for k in k_values:
        hit = scores[k]["hit"] / total
        mrr = scores[k]["mrr"] / total
        print(f"Top-{k:<6} | {hit:.4f}       | {mrr:.4f}")
    print("="*60)

# ==========================================
# ğŸš€ 5. ë©”ì¸ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
# ==========================================
if __name__ == "__main__":
    
    # 
    
    # 0. ì…ë ¥ íŒŒì¼ ì„¤ì •
    FILE_NAME = "(í…ìŠ¤íŠ¸ë¬¸ì„œ txt) ì´ìƒí•œ ë‚˜ë¼ì˜ ì•¨ë¦¬ìŠ¤ (ìš°ë¦¬ë§ ì˜®ê¹€)(2ì°¨ í¸ì§‘ìµœì¢…)(ë¸”ë¡œê·¸ì—…ë¡œë“œìš© 2018ë…„ ìµœì¢…) 180127.txt"
    
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„° ìƒì„±
    if not os.path.exists(FILE_NAME):
        print("âš ï¸ ì…ë ¥ íŒŒì¼ì´ ì—†ì–´ í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        dummy_text = "ì•¨ë¦¬ìŠ¤ëŠ” ê°•ë‘‘ì— ì•‰ì•„ ì–¸ë‹ˆê°€ ì±… ì½ëŠ” ê²ƒì„ êµ¬ê²½í•˜ê³  ìˆì—ˆë‹¤. ì‹¬ì‹¬í•´ì„œ ì£½ì„ ì§€ê²½ì´ì—ˆë‹¤. " * 300
        with open("test_novel.txt", "w", encoding='utf-8') as f:
            f.write(dummy_text)
        FILE_NAME = "test_novel.txt"

    # 1. í…ìŠ¤íŠ¸ ë¡œë“œ ë° Parent Chunking
    print("\n[Step 1] Parent Chunking (3000ì ë‹¨ìœ„)...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)
    with open(FILE_NAME, 'r', encoding='utf-8') as f:
        full_text = f.read()
    parents = splitter.split_text(full_text)
    print(f"   -> ì´ {len(parents)}ê°œì˜ Parent Chunk ìƒì„±ë¨.")

    # 2. DB ì´ˆê¸°í™”
    db = ParentChildVectorDB()
    all_extracted_scenes = []
    eval_dataset = [] 

    # 3. ì¶”ì¶œ ë° ì¸ë±ì‹± ë£¨í”„
    print("\n[Step 2] ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ ë° ë²¡í„° DB ì ì¬...")
    
    # [ì£¼ì˜] ì „ì²´ ì‹¤í–‰ ì‹œ parents[:3] -> parents ë¡œ ë³€ê²½í•˜ì„¸ìš”.
    target_chunks = parents[:3] 
    
    for i, p_text in enumerate(target_chunks):
        print(f"   -> Chunk {i+1}/{len(target_chunks)} ì²˜ë¦¬ ì¤‘...")
        
        # (1) Parent ì €ì¥
        p_id = db.add_parent(p_text)
        
        # (2) LLM ì¶”ì¶œ
        scenes = extract_storyboard(p_text)
        
        for scene in scenes:
            scene['origin_chunk_id'] = p_id
            all_extracted_scenes.append(scene)
            
            # (3) ì„ë² ë”© í…ìŠ¤íŠ¸ (D2Q)
            queries = " ".join(scene.get('generated_queries', []))
            embed_text = f"{scene['title']} {scene['summary']} {scene['visual_description']} {queries}"
            
            # (4) Child ì €ì¥
            db.add_child(p_id, embed_text, scene)
            
            # (5) í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ (ì§ˆë¬¸ -> ì •ë‹µID)
            for q in scene.get('generated_queries', []):
                eval_dataset.append({
                    "query": q,
                    "target_parent_id": p_id
                })

    # 4. ê²°ê³¼ JSON íŒŒì¼ ì €ì¥
    save_results_to_json(all_extracted_scenes)

    # 5. ì •ëŸ‰ì  í‰ê°€ ì‹¤í–‰ (ìˆ˜ì •ëœ ìƒì„¸ ë¡œì§ ì ìš©ë¨)
    evaluate_retrieval(db, eval_dataset, k_values=[1, 3, 5])

    # 6. ì‹¤ì œ ê²€ìƒ‰ í™•ì¸
    print("\n[Step 3] ì‚¬ìš©ì ê´€ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    if eval_dataset:
        test_q = eval_dataset[0]['query']
    else:
        test_q = "ì•¨ë¦¬ìŠ¤ê°€ í† ë¼ë¥¼ ì«“ì•„ê°€ëŠ” ì¥ë©´"
        
    print(f"ğŸ” ì§ˆë¬¸: '{test_q}'")
    results = db.search(test_q, top_k=1)
    
    if results:
        res = results[0]
        print("-" * 40)
        print(f"âœ… ë§¤ì¹­ëœ ì”¬: {res['matched_scene']}")
        print(f"ğŸ“ ìš”ì•½: {res['summary']}")
        print(f"ğŸ“„ ì›ë³¸(Parent) ì¼ë¶€:\n{res['full_context'][:150]}...")
        print("-" * 40)
    else:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
    print("\nâœ… ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ.")