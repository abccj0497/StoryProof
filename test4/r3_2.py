import os
import json
import uuid
import numpy as np
import torch
from typing import List, Dict, Any

# Langchain ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    from langchain_text_splitters import RecursiveCharacterTextSplitter

from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity

# ==========================================
# âš™ï¸ 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
# ==========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ [Setting] ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

# [ëª¨ë¸ ì„¤ì •]
LLM_ID = "Qwen/Qwen2.5-1.5B-Instruct"
EMBED_ID = "BAAI/bge-m3"

print(f"ğŸ“¥ [Model] LLM ë¡œë”© ì¤‘ ({LLM_ID})...")
try:
    tokenizer = AutoTokenizer.from_pretrained(LLM_ID, trust_remote_code=True)
    llm_model = AutoModelForCausalLM.from_pretrained(
        LLM_ID, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
    ).eval()
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

print(f"ğŸ“¥ [Model] Embedding ëª¨ë¸ ë¡œë”© ì¤‘ ({EMBED_ID})...")
embed_model = SentenceTransformer(EMBED_ID, device=DEVICE)

# ==========================================
# ğŸ“ 2. [ì—…ê·¸ë ˆì´ë“œ] ìƒì„¸ ìŠ¤í† ë¦¬ë³´ë“œ í”„ë¡¬í”„íŠ¸
# ==========================================
# (ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹ˆë¼ ì—°ì¶œ/ì‹œê° ì •ë³´ë¥¼ ë½‘ë„ë¡ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤)
STORYBOARD_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì˜í™” ì „ë¬¸ ìŠ¤í† ë¦¬ë³´ë“œ ì•„í‹°ìŠ¤íŠ¸ì´ì ì—°ì¶œ ê°ë…ì…ë‹ˆë‹¤.
ì œê³µëœ ì†Œì„¤ í…ìŠ¤íŠ¸ë¥¼ ì‹œê°í™” ê°€ëŠ¥í•œ 'ìŠ¤í† ë¦¬ë³´ë“œ ìƒ·(Shot)' ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì„¸ìš”.
ë‹¨ìˆœí•œ ì¤„ê±°ë¦¬ ìš”ì•½ì´ ì•„ë‹ˆë¼, ì¹´ë©”ë¼ ì•µê¸€, ì¡°ëª…, í”¼ì‚¬ì²´ì˜ ì›€ì§ì„ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ì„ ë”°ë¥´ì‹­ì‹œì˜¤.

[JSON ì¶œë ¥ í¬ë§·]
{
  "scenes": [
    {
      "scene_id": "scene_N",
      "title": "ì¥ë©´ì˜ ì œëª©",
      "summary": "ì¥ë©´ì˜ ìƒí™© ì„¤ëª… (í•œê¸€)",
      "visual_spec": {
          "shot_type": "ì¹´ë©”ë¼ ìƒ· ì¢…ë¥˜ (ì˜ˆ: Close-up, Wide Shot, Over the Shoulder)",
          "camera_angle": "ì¹´ë©”ë¼ ì•µê¸€ (ì˜ˆ: Low Angle, High Angle, Eye Level)",
          "lighting": "ì¡°ëª… ë° ë‚ ì”¨ (ì˜ˆ: ì–´ë‘ìš´ ë‹¬ë¹›, ë”°ëœ»í•œ í–‡ì‚´, ì—­ê´‘)",
          "composition": "í™”ë©´ êµ¬ì„± (ì˜ˆ: ì™¼ìª½ì—ëŠ” ë‚˜ë¬´ê°€ ìˆê³  ì¤‘ì•™ì— ì¸ë¬¼ì´ ì„œ ìˆë‹¤)"
      },
      "action_description": "ì¸ë¬¼ì˜ êµ¬ì²´ì ì¸ í–‰ë™ (ì˜ˆ: ê²ì— ì§ˆë ¤ ë’·ê±¸ìŒì§ˆ ì¹œë‹¤)",
      "sound_sfx": "íš¨ê³¼ìŒ (ì˜ˆ: ê±°ì¹œ ìˆ¨ì†Œë¦¬, ë©€ë¦¬ì„œ ë“¤ë¦¬ëŠ” ì‚¬ì´ë Œ)",
      "generated_queries": ["ê²€ìƒ‰ ì§ˆë¬¸1", "ê²€ìƒ‰ ì§ˆë¬¸2"] 
    }
  ]
}

ì£¼ì˜ì‚¬í•­:
1. ì˜¤ì§ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
2. ëª¨ë“  ë‚´ìš©ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

# ==========================================
# ğŸ—ï¸ 3. Parent-Child Vector DB í´ë˜ìŠ¤
# ==========================================
class ParentChildVectorDB:
    def __init__(self):
        self.parents = {}   # {parent_id: "ì›ë³¸ í…ìŠ¤íŠ¸"}
        self.children = []  # [{parent_id, vector, metadata}, ...]

    def add_parent(self, text: str) -> str:
        p_id = str(uuid.uuid4())
        self.parents[p_id] = text
        return p_id

    def add_child(self, parent_id: str, text_to_embed: str, metadata: Dict):
        vector = embed_model.encode(text_to_embed, convert_to_tensor=False)
        self.children.append({
            "parent_id": parent_id,
            "vector": vector,
            "metadata": metadata 
        })

    def search(self, query: str, top_k=3):
        if not self.children: return []
        
        query_vec = embed_model.encode(query, convert_to_tensor=False)
        child_vectors = [c['vector'] for c in self.children]
        
        scores = cosine_similarity([query_vec], child_vectors)[0]
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        seen_parents = set()
        
        for idx in top_indices:
            child = self.children[idx]
            p_id = child['parent_id']
            
            # Parent ì¤‘ë³µ ì œê±° (ê°™ì€ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ ë‚´ ì—¬ëŸ¬ ì”¬ì´ ì¡í˜€ë„ í•œ ë²ˆë§Œ ë¦¬í„´)
            if p_id not in seen_parents:
                results.append({
                    "score": float(scores[idx]),
                    "parent_id": p_id,
                    "scene_id": child['metadata'].get('scene_id', 'Unknown'), # ID í™•ì¸ìš©
                    "matched_scene": child['metadata']['title'],
                    "summary": child['metadata']['summary'],
                    "visual": child['metadata'].get('visual_spec', {}), # ì‹œê° ì •ë³´
                    "full_context": self.parents[p_id]
                })
                seen_parents.add(p_id)
        
        return results

# ==========================================
# ğŸ“ 4. ì¶”ì¶œ ë° íŒŒì‹± í•¨ìˆ˜ (ì—ëŸ¬ ìˆ˜ì •ë¨)
# ==========================================
def extract_storyboard(chunk_text: str) -> List[Dict]:
    messages = [
        {"role": "system", "content": STORYBOARD_SYSTEM_PROMPT},
        {"role": "user", "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì‹œì˜¤:\n\n{chunk_text}"}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        outputs = llm_model.generate(**inputs, max_new_tokens=2048, temperature=0.1, do_sample=True)
    
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    try:
        # JSON ì „ì²˜ë¦¬ (ë§ˆí¬ë‹¤ìš´ ì œê±°)
        clean_json = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(clean_json)
        
        # âœ… [ìˆ˜ì • ì™„ë£Œ] ë¦¬ìŠ¤íŠ¸([])ë¡œ ì˜¤ë“  ë”•ì…”ë„ˆë¦¬({})ë¡œ ì˜¤ë“  ì²˜ë¦¬
        if isinstance(data, list):
            return data
        elif isinstance(data, dict):
            return data.get("scenes", [])
        else:
            return []
            
    except json.JSONDecodeError:
        print(f"   âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (ì‘ë‹µ ì•ë¶€ë¶„: {response[:50]}...)")
        return []

# ==========================================
# ğŸ’¾ 5. ê²°ê³¼ íŒŒì¼ ì €ì¥ í•¨ìˆ˜ë“¤
# ==========================================
def save_results_to_json(all_scenes, filename="storyboard_output.json"):
    # scene_id ìˆœì„œëŒ€ë¡œ ì •ë ¬ (scene_1, scene_2, scene_10...)
    try:
        all_scenes.sort(key=lambda x: int(x['scene_id'].split('_')[1]))
    except:
        pass # ì •ë ¬ ì‹¤íŒ¨ì‹œ ê·¸ëƒ¥ ì €ì¥

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({"scenes": all_scenes}, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ [File 2] Storyboard(Child)ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def save_parents_to_json(parents_dict, filename="parent_chunks.json"):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(parents_dict, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ [File 1] Parent Chunks(ì›ë³¸)ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ==========================================
# ğŸ“Š 6. ì •ëŸ‰ì  í‰ê°€ í•¨ìˆ˜ (Hit@k, MRR@k)
# ==========================================
def calculate_metrics(db, eval_dataset, k_values=[1, 3, 5]):
    print("\n" + "="*50)
    print(f"ğŸ“Š ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì‹œì‘ (ì´ {len(eval_dataset)}ê°œ ì§ˆë¬¸)")
    print("="*50)
    
    scores = {k: {"hit": 0, "mrr": 0} for k in k_values}
    
    for i, item in enumerate(eval_dataset):
        query = item['query']
        target_id = item['target_parent_id'] # ì •ë‹µ(ì›ë³¸ ë¶€ëª¨ ID)
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        max_k = max(k_values)
        results = db.search(query, top_k=max_k)
        retrieved_ids = [res['parent_id'] for res in results]
        
        # ë¡œê·¸ ì¶œë ¥ (ì•ìª½ 1ê°œë§Œ)
        if i < 1:
            print(f"Q Sample: {query}")
            print(f"   -> ì •ë‹µ Scene: {item.get('target_scene_id', 'Unknown')}")
            print("-" * 30)

        # ì§€í‘œ ê³„ì‚°
        for k in k_values:
            top_k_ids = retrieved_ids[:k]
            if target_id in top_k_ids:
                scores[k]["hit"] += 1
                rank = top_k_ids.index(target_id) + 1
                scores[k]["mrr"] += (1.0 / rank)
    
    print("\nğŸ“ˆ [ìµœì¢… í‰ê°€ ê²°ê³¼]")
    total = len(eval_dataset)
    for k in k_values:
        hit_score = scores[k]["hit"] / total
        mrr_score = scores[k]["mrr"] / total
        print(f" -> @{k}: Hit={hit_score:.4f}, MRR={mrr_score:.4f}")
        
    return scores

# ==========================================
# ğŸš€ 7. ë©”ì¸ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
# ==========================================
if __name__ == "__main__":
    file_path = "KR_fantasy_alice.txt"
    
    if not os.path.exists(file_path):
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„° ìƒì„±
        with open("test_novel.txt", "w", encoding='utf-8') as f:
            f.write("ì•¨ë¦¬ìŠ¤ëŠ” ê°•ë‘‘ì— ì•‰ì•„ ìˆì—ˆë‹¤. " * 300)
        file_path = "test_novel.txt"

    # [Step 1] Parent Chunking
    print(f"\n[Step 1] '{file_path}' ë¡œë”© ë° ë¶„í•  (Chunking)...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f: text = f.read()
    except UnicodeDecodeError:
        with open(file_path, 'r', encoding='cp949') as f: text = f.read()

    parents = splitter.split_text(text)
    print(f"   -> {len(parents)}ê°œì˜ Parent Chunk ìƒì„±ë¨.")

    db = ParentChildVectorDB()
    all_extracted_scenes = [] # ğŸ’¾ ì €ì¥ìš©
    eval_dataset = []         # ğŸ“Š í‰ê°€ìš©

    # âœ… [í•µì‹¬ ê¸°ëŠ¥] ì „ì—­ ì”¬ ì¹´ìš´í„° (scene_1, scene_2, ... ìˆœì„œ ë³´ì¥)
    global_scene_counter = 1

    # [Step 2] ì¶”ì¶œ ë° DB ì ì¬
    print("\n[Step 2] ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ ë° ì¸ë±ì‹±...")
    
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì•ë¶€ë¶„ 5ê°œ ì²­í¬ë§Œ ì‚¬ìš© (ì „ì²´ëŠ” parents[:5] -> parents)
    target_chunks = parents[:5] 
    
    for i, p_text in enumerate(target_chunks): 
        print(f"   -> Processing Chunk {i+1}/{len(target_chunks)}... (Scene {global_scene_counter}~)")
        
        # (1) Parent ì €ì¥
        p_id = db.add_parent(p_text)
        
        # (2) LLM ì¶”ì¶œ
        scenes = extract_storyboard(p_text)
        
        # (3) í›„ì²˜ë¦¬ ë° DB ì ì¬
        for scene in scenes:
            # ğŸ·ï¸ ID ìˆœì°¨ ë¶€ì—¬ (scene_1, scene_2...)
            current_scene_id = f"scene_{global_scene_counter}"
            scene['scene_id'] = current_scene_id
            scene['original_chunk_id'] = p_id 
            
            # ì¹´ìš´í„° ì¦ê°€
            global_scene_counter += 1
            
            all_extracted_scenes.append(scene)

            # ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„± (Visual Spec í¬í•¨)
            visual_info = scene.get('visual_spec', {})
            visual_text = f"{visual_info.get('shot_type', '')} {visual_info.get('camera_angle', '')} {visual_info.get('composition', '')}"
            queries = " ".join(scene.get('generated_queries', []))
            
            embed_text = f"{scene['title']} {scene['summary']} {visual_text} {queries}"
            
            # DB ì¶”ê°€
            db.add_child(p_id, embed_text, scene)
            
            # í‰ê°€ ë°ì´í„° ì¶”ê°€
            for q in scene.get('generated_queries', []):
                eval_dataset.append({
                    "query": q,
                    "target_parent_id": p_id,
                    "target_scene_id": current_scene_id
                })

    # [Step 3] íŒŒì¼ ì €ì¥
    print("\n" + "="*30)
    print("ğŸ’¾ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹œì‘")
    print("="*30)

    if db.parents:
        save_parents_to_json(db.parents, "parent_chunks.json")

    if all_extracted_scenes:
        save_results_to_json(all_extracted_scenes, "storyboard_output.json")
    else:
        print("âš ï¸ ì¶”ì¶œëœ ì”¬ì´ ì—†ìŠµë‹ˆë‹¤.")

    # [Step 4] ì •ëŸ‰ í‰ê°€
    if eval_dataset:
        scores = calculate_metrics(db, eval_dataset, k_values=[1, 3, 5])
        
        with open("evaluation_scores.txt", "w", encoding="utf-8") as f:
            json.dump(scores, f, ensure_ascii=False, indent=4)
        print("ğŸ’¾ [File 3] í‰ê°€ ì ìˆ˜ê°€ 'evaluation_scores.txt'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")