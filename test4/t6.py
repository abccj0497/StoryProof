import os
import json
import uuid
import numpy as np
import torch
from typing import List, Dict, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity

# ==========================================
# âš™ï¸ 0. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
# ==========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ [System] ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

# [ëª¨ë¸ ì„¤ì •]
LLM_ID = "Qwen/Qwen2.5-1.5B-Instruct"
EMBED_ID = "BAAI/bge-m3"

print(f"ğŸ“¥ [Model] LLM ë¡œë”© ì¤‘ ({LLM_ID})...")
try:
    tokenizer = AutoTokenizer.from_pretrained(LLM_ID, trust_remote_code=True)
    llm_model = AutoModelForCausalLM.from_pretrained(
        LLM_ID, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
    ).eval()
except Exception as e:
    print(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

print(f"ğŸ“¥ [Model] Embedding ëª¨ë¸ ë¡œë”© ì¤‘ ({EMBED_ID})...")
embed_model = SentenceTransformer(EMBED_ID, device=DEVICE)

# ==========================================
# ğŸ—ï¸ 1. Parent-Child Vector DB í´ë˜ìŠ¤
# ==========================================
class ParentChildVectorDB:
    def __init__(self):
        self.parents = {}   
        self.children = []  

    def add_parent(self, text: str) -> str:
        p_id = str(uuid.uuid4())
        self.parents[p_id] = text
        return p_id

    def add_child(self, parent_id: str, text_to_embed: str, metadata: Dict):
        vector = embed_model.encode(text_to_embed, convert_to_tensor=False)
        self.children.append({
            "parent_id": parent_id,
            "vector": vector,
            "metadata": metadata 
        })

    def search(self, query: str, top_k=5) -> List[Dict]:
        if not self.children: return []
        
        query_vec = embed_model.encode(query, convert_to_tensor=False)
        child_vectors = [c['vector'] for c in self.children]
        
        scores = cosine_similarity([query_vec], child_vectors)[0]
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        seen_parents = set()
        
        for idx in top_indices:
            child = self.children[idx]
            p_id = child['parent_id']
            
            if p_id not in seen_parents:
                results.append({
                    "score": float(scores[idx]),
                    "parent_id": p_id,
                    "scene_id": child['metadata']['scene_id'], # ğŸ‘ˆ ê²°ê³¼ í™•ì¸ìš© scene_id ì¶”ê°€
                    "matched_scene": child['metadata']['title'],
                    "summary": child['metadata']['summary'],
                    "full_context": self.parents[p_id]
                })
                seen_parents.add(p_id)
        
        return results

# ==========================================
# ğŸ“ 2. ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (ID ì˜ˆì‹œ ìˆ˜ì •)
# ==========================================
STORYBOARD_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì˜í™” ìŠ¤í† ë¦¬ë³´ë“œ ì‘ê°€ì…ë‹ˆë‹¤. ì†Œì„¤ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  'ì¥ë©´(Scene)' ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ì„ ì—„ê²©í•˜ê²Œ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤.

[JSON ì¶œë ¥ í¬ë§·]
{
  "scenes": [
    {
      "scene_id": "scene_1",
      "title": "ì¥ë©´ ì œëª©",
      "summary": "ì¥ë©´ì˜ í•µì‹¬ ì¤„ê±°ë¦¬ ìš”ì•½ (í•œê¸€)",
      "characters": ["ë“±ì¥ì¸ë¬¼1", "ë“±ì¥ì¸ë¬¼2"],
      "location": "ì¥ì†Œ",
      "time": "ì‹œê°„ì  ë°°ê²½",
      "visual_description": "ì¥ë©´ì„ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë¦´ ë•Œ í•„ìš”í•œ ì‹œê°ì  ë¬˜ì‚¬",
      "mood": "ë¶„ìœ„ê¸°",
      "generated_queries": ["ê²€ìƒ‰ ì§ˆë¬¸1", "ê²€ìƒ‰ ì§ˆë¬¸2", "ê²€ìƒ‰ ì§ˆë¬¸3"] 
    }
  ]
}

ì£¼ì˜ì‚¬í•­:
1. ì˜¤ì§ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
2. ê° ì¥ë©´ì˜ ë‚´ìš©ì€ êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
"""

def extract_storyboard(chunk_text: str) -> List[Dict]:
    messages = [
        {"role": "system", "content": STORYBOARD_SYSTEM_PROMPT},
        {"role": "user", "content": f"ë‹¤ìŒ ì†Œì„¤ ë‚´ìš©ì„ ë¶„ì„í•´ ìŠ¤í† ë¦¬ë³´ë“œ JSONì„ ë§Œë“œì‹œì˜¤:\n\n{chunk_text}"}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        outputs = llm_model.generate(
            **inputs, max_new_tokens=2048, temperature=0.1, do_sample=True
        )
    
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    try:
        clean_json = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(clean_json)
        return data.get("scenes", [])
    except json.JSONDecodeError:
        return []

# ==========================================
# ğŸ“Š 3. í‰ê°€ í•¨ìˆ˜
# ==========================================
def calculate_metrics(db, eval_dataset: List[Dict], k_values=[1, 3, 5]):
    print("\n" + "="*50)
    print(f"ğŸ“Š ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì‹œì‘ (ì´ {len(eval_dataset)}ê°œ ì§ˆë¬¸)")
    print("="*50)
    
    metrics = {k: {"hit_count": 0, "mrr_sum": 0} for k in k_values}
    
    for i, item in enumerate(eval_dataset):
        query = item['query']
        target_id = item['target_parent_id']
        
        results = db.search(query, top_k=max(k_values))
        retrieved_ids = [res['parent_id'] for res in results]
        
        # ë””ë²„ê¹…: ì²˜ìŒ 1ê°œë§Œ ì¶œë ¥
        if i < 1:
            print(f"[Query Sample] {query}")
            print(f"   -> ì •ë‹µ Scene ID: {item.get('target_scene_id', 'Unknown')}") # scene_id í™•ì¸

        for k in k_values:
            top_k_ids = retrieved_ids[:k]
            if target_id in top_k_ids:
                metrics[k]["hit_count"] += 1
                rank = top_k_ids.index(target_id) + 1
                metrics[k]["mrr_sum"] += (1.0 / rank)

    print("\nğŸ“ˆ [ìµœì¢… í‰ê°€ ê²°ê³¼]")
    total = len(eval_dataset)
    for k in k_values:
        hit_score = metrics[k]["hit_count"] / total
        mrr_score = metrics[k]["mrr_sum"] / total
        print(f" -> @{k}: Hit = {hit_score:.4f}, MRR = {mrr_score:.4f}")
    return metrics

# ==========================================
# ğŸš€ 4. ë©”ì¸ ì‹¤í–‰ (ìˆœì°¨ì  ë²ˆí˜¸ ë¶€ì—¬ ì ìš©)
# ==========================================
if __name__ == "__main__":
    # ... (íŒŒì¼ ë¡œë”© ë° ëª¨ë¸ ì„¤ì • ë¶€ë¶„ì€ ë™ì¼) ...

    # 1. í…ìŠ¤íŠ¸ ë¡œë“œ ë° ì²­í‚¹
    splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)
    with open(FILE_NAME, 'r', encoding='utf-8') as f: full_text = f.read()
    parents = splitter.split_text(full_text)

    db = ParentChildVectorDB()
    eval_dataset = []

    # âœ… [í•µì‹¬] ì „ì²´ ë£¨í”„ ë°–ì—ì„œ ì¹´ìš´í„°ë¥¼ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    global_scene_counter = 1 

    print("\n[Step] ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ ë° ì ì¬...")
    
    # ì „ì²´ ì²­í¬ë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    for i, p_text in enumerate(parents):
        print(f"   -> Chunk {i+1}/{len(parents)} ì²˜ë¦¬ ì¤‘... (í˜„ì¬ Scene ë²ˆí˜¸: {global_scene_counter}ë¶€í„° ì‹œì‘)")
        
        # (1) Parent ì €ì¥
        p_id = db.add_parent(p_text)
        
        # (2) LLM ì¶”ì¶œ (LLMì€ ë²ˆí˜¸ë¥¼ ì‹ ê²½ ì“°ì§€ ì•Šê³  ì¥ë©´ ë¦¬ìŠ¤íŠ¸ë§Œ ë±‰ìŠµë‹ˆë‹¤)
        scenes = extract_storyboard(p_text)
        
        # (3) Pythonì—ì„œ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸í‘œ ë¶™ì´ê¸°
        for scene in scenes:
            # ğŸ·ï¸ ì—¬ê¸°ì„œ ìˆœì°¨ì ìœ¼ë¡œ IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. (scene_1, scene_2, scene_3 ...)
            current_scene_id = f"scene_{global_scene_counter}"
            
            # ë©”íƒ€ë°ì´í„°ì— ë°˜ì˜
            scene['scene_id'] = current_scene_id
            
            # ë²¡í„° DB ì €ì¥ìš© í…ìŠ¤íŠ¸ ìƒì„±
            queries = " ".join(scene.get('generated_queries', []))
            embed_text = f"{scene['title']} {scene['summary']} {scene['visual_description']} {queries}"
            
            # DB ì €ì¥
            db.add_child(p_id, embed_text, scene)
            
            # í‰ê°€ ë°ì´í„° ì €ì¥
            for q in scene.get('generated_queries', []):
                eval_dataset.append({
                    "query": q,
                    "target_parent_id": p_id,
                    "target_scene_id": current_scene_id 
                })
            
            # ğŸ”¢ ë‹¤ìŒ ì¥ë©´ì„ ìœ„í•´ ë²ˆí˜¸ ì¦ê°€
            global_scene_counter += 1

    # ... (ì´í›„ í‰ê°€ ë° ì €ì¥ ë¡œì§ ë™ì¼) ...
    # ì„ë² ë”© ë° ì €ì¥
            queries = " ".join(scene.get('generated_queries', []))
            embed_text = f"{scene['title']} {scene['summary']} {scene['visual_description']} {queries}"
            db.add_child(p_id, embed_text, scene)
            
            # í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ (í™•ì¸ìš© scene_id ì¶”ê°€)
            for q in scene.get('generated_queries', []):
                eval_dataset.append({
                    "query": q,
                    "target_parent_id": p_id,
                    "target_scene_id": scene_id_formatted 
                })

    # í‰ê°€ ì‹¤í–‰
    if eval_dataset:
        calculate_metrics(db, eval_dataset, k_values=[1, 3, 5])

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n[ê²€ìƒ‰ í…ŒìŠ¤íŠ¸]")
    test_q = eval_dataset[0]['query'] if eval_dataset else "í…ŒìŠ¤íŠ¸"
    results = db.search(test_q, top_k=1)
    
    for res in results:
        # scene_idê°€ scene_1, scene_2 í˜•íƒœë¡œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸
        print(f"ğŸ†” Scene ID: {res['scene_id']}") 
        print(f"ğŸ¬ ì¥ë©´ ì œëª©: {res['matched_scene']}")
        print(f"ğŸ“„ ìš”ì•½: {res['summary']}")