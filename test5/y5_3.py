import os
import re
import json
import time
from typing import List, Dict
from collections import defaultdict

# =========================================================
# [ë¼ì´ë¸ŒëŸ¬ë¦¬] DB(psycopg2) ê´€ë ¨ì€ ë‹¤ ëºìŠµë‹ˆë‹¤.
# =========================================================
from google import genai
from google.genai import types

# =========================================================
# [ì„¤ì •] API í‚¤ë§Œ í™•ì¸í•˜ì„¸ìš”!
# =========================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4")

LLM_MODEL_NAME = "gemini-2.5-flash"
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)
        print(f"ğŸ“ í´ë” ìƒì„± ì™„ë£Œ: {SCENE_DIR}")

# ==============================================================================
# [1ë‹¨ê³„] ì†Œì„¤ ì½ê¸° ë° ì²­í‚¹ (íŒŒì¼ ì €ì¥ í¬í•¨)
# ==============================================================================
class SceneChunker:
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'ìˆ²', 'êµ´', 'ì •ì›', 'í™€', 'ë°”ë‹¤', 'ì§‘ì•ˆ', 'ë‚˜ë¬´']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ê°‘ìê¸°']

    def __init__(self, threshold: int = 7):
        self.threshold = threshold

    def split_into_scenes(self, text: str) -> List[str]:
        sentences = re.split(r'([.!?]\s+)', text)
        merged = []
        for i in range(0, len(sentences)-1, 2):
            merged.append(sentences[i] + (sentences[i+1] if i+1 < len(sentences) else ""))
        
        scenes, current_scene, score = [], [], 0
        for sent in merged:
            if not sent.strip(): continue
            if "***" in sent: score += 10
            if any(k in sent for k in self.LOCATION_KEYWORDS): score += 5
            if any(k in sent for k in self.TIME_TRANSITIONS): score += 4
            current_scene.append(sent)
            if score >= self.threshold:
                scenes.append(" ".join(current_scene))
                current_scene, score = [], 0
        if current_scene: scenes.append(" ".join(current_scene))
        return scenes

def process_and_save_chunks(file_path: str) -> List[Dict]:
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f: text = f.read()
    except:
        with open(file_path, 'r', encoding='cp949') as f: text = f.read()

    chunks = SceneChunker().split_into_scenes(text)
    
    result_list = []
    print(f"ğŸ’¾ [1. ì²­í‚¹] ì”¬ë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì¤‘ ({SCENE_DIR})...")
    for i, scene_text in enumerate(chunks):
        scene_id = f"scene_{i+1:03d}"
        file_name = os.path.join(SCENE_DIR, f"{scene_id}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(scene_text)
        result_list.append({'id': scene_id, 'text': scene_text, 'scene_index': i})
    
    print(f"âœ… ì´ {len(result_list)}ê°œ ì”¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ.")
    return result_list

# ==============================================================================
# [2ë‹¨ê³„] AI ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ (Gemini)
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = LLM_MODEL_NAME

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        Analyze this novel scene (Korean).
        [TEXT] {chunk['text'][:2000]}
        [OUTPUT JSON FORMAT]
        {{
          "scene_id": "{chunk['id']}", 
          "title": "ì†Œì œëª©",
          "meta": {{ "time": "ì‹œê°„", "place": "ì¥ì†Œ", "characters": ["ì¸ë¬¼1", "ì¸ë¬¼2"] }},
          "wiki_entities": [ {{ "name": "ì´ë¦„", "category": "ì¸ë¬¼/ë¬¼í’ˆ/ì¥ì†Œ", "description": "íŠ¹ì§•", "action": "í–‰ë™" }} ],
          "dense_summary": "ìš”ì•½ë¬¸"
        }}
        """
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return json.loads(response.text)
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨ ({chunk['id']}): {e}")
            return None

# ==============================================================================
# [3ë‹¨ê³„] ì„¤ì •ì§‘(Bible) íŒŒì¼ ìƒì„±
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def save_report_to_file(storyboard_list: List[Dict]):
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        print(f"\nğŸ’¾ [3. ì„¤ì •ì§‘] ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘: {file_path}")
        
        wiki_db = defaultdict(lambda: defaultdict(list))
        
        for scene in storyboard_list:
            s_id = scene.get('scene_id')
            title = scene.get('title', 'ë¬´ì œ')
            for entity in scene.get('wiki_entities', []):
                wiki_db[entity.get('category','ê¸°íƒ€')][entity.get('name','ë¯¸ìƒ')].append({
                    "scene": f"{s_id} ({title})", 
                    "desc": entity.get('description'), 
                    "action": entity.get('action')
                })

        with open(file_path, "w", encoding="utf-8") as f:
            f.write("# ğŸ“š ì†Œì„¤ ë¶„ì„ ë³´ê³ ì„œ (Writer's Bible)\n\n")
            
            # 1. ì”¬ ë¦¬ìŠ¤íŠ¸ ìš”ì•½
            f.write("## 1. ì”¬ ëª©ë¡\n")
            for scene in storyboard_list:
                f.write(f"- **{scene['scene_id']}**: {scene['title']} (ìš”ì•½: {scene['dense_summary']})\n")
            
            # 2. ì—”í‹°í‹° ì‚¬ì „
            f.write("\n## 2. ì¸ë¬¼ ë° ì‚¬ë¬¼ ì‚¬ì „\n")
            for cat, items in wiki_db.items():
                f.write(f"\n### [{cat}]\n")
                for name, recs in items.items():
                    f.write(f"#### {name}\n")
                    for r in recs: f.write(f"- ({r['scene']}) {r['desc']} / {r['action']}\n")
                    
        print("âœ… ì„¤ì •ì§‘ íŒŒì¼ ìƒì„± ì™„ë£Œ.")

# ==============================================================================
# [ë©”ì¸ ì‹¤í–‰]
# ==============================================================================
def main():
    if "YOUR_GOOGLE" in GOOGLE_API_KEY:
        print("âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš” (ì½”ë“œ ìƒë‹¨ GOOGLE_API_KEY ë³€ìˆ˜)")
        return

    create_output_dirs()
    input_file = "KR_fantasy_alice.txt"
    
    if not os.path.exists(input_file):
        print(f"âŒ '{input_file}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. ì²­í‚¹ ë° ì €ì¥
    chunks = process_and_save_chunks(input_file)

    # 2. AI ë¶„ì„ (DB ì—°ê²° ì—†ì´ ìˆœìˆ˜ ë¶„ì„ë§Œ)
    analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    all_storyboards = []
    
    print("\nğŸš€ [2. ë¶„ì„] AI ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ ì‹œì‘...")
    
    # â˜… ì „ì²´ë¥¼ ë‹¤ í•˜ë ¤ë©´ ì•„ë˜ [:5]ë¥¼ ì§€ìš°ê³  chunks ë¡œ ë°”ê¾¸ì„¸ìš”!
    for chunk in chunks[:5]: 
        print(f"  â–¶ {chunk['id']} ì²˜ë¦¬ ì¤‘...", end=" ")
        result = analyzer.analyze(chunk)
        if result:
            all_storyboards.append(result)
            print(f"ì™„ë£Œ ({result['title']})")
            time.sleep(1) # API ì†ë„ ì œí•œ ë°©ì§€
        else:
            print("ì‹¤íŒ¨")

    # 3. ë°ì´í„° ì €ì¥ (JSON + Markdown)
    json_path = os.path.join(OUTPUT_DIR, "storyboard_analysis.json")
    print(f"\nğŸ’¾ [ì €ì¥] ì „ì²´ ë°ì´í„° JSON ì €ì¥: {json_path}")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_storyboards, f, indent=2, ensure_ascii=False)
    
    # ì„¤ì •ì§‘ ë§Œë“¤ê¸°
    WikiGenerator.save_report_to_file(all_storyboards)
    
    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ëë‚¬ìŠµë‹ˆë‹¤!")
    print(f"1. ì”¬ íŒŒì¼ë“¤: {SCENE_DIR}")
    print(f"2. ì „ì²´ ë°ì´í„°(ë‚˜ì¤‘ì— DB ë„£ì„ ë•Œ ì‚¬ìš©): {json_path}")
    print(f"3. ì„¤ì •ì§‘(ì½ëŠ” ìš©ë„): {os.path.join(OUTPUT_DIR, 'writer_bible.md')}")

if __name__ == "__main__":
    main()