import os
import re
import json
import time
from typing import List, Dict, Any
from collections import defaultdict

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai
import psycopg2
from psycopg2.extras import Json
from sentence_transformers import SentenceTransformer
# ì¸ì½”ë”© ê°ì§€ (y4ì˜ ê²¬ê³ í•¨ ìœ ì§€)
try:
    import chardet
except ImportError:
    chardet = None

# ==============================================================================
# [ì„¤ì • ì˜ì—­] API í‚¤ ë° DB ì •ë³´, ì¶œë ¥ ê²½ë¡œ ì„¤ì •
# ==============================================================================
# 1. êµ¬ê¸€ API í‚¤
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4")

# 2. LLM ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
LLM_MODEL_NAME = "gemini-2.5-flash"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"

# 3. PostgreSQL ì ‘ì† ì •ë³´
DB_CONFIG = {
    "dbname": "postgres",
    "user": "postgres",
    "password": "mysecretpassword",
    "host": "localhost",
    "port": "5432"
}

# 4. ì¶œë ¥ ê²½ë¡œ ì„¤ì • (y5ì˜ ê¸°ëŠ¥ ì¶”ê°€)
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

def create_output_dirs():
    """ê²°ê³¼ë¬¼ì„ ì €ì¥í•  í´ë” ìƒì„±"""
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)
        print(f"ğŸ“ í´ë” ìƒì„± ì™„ë£Œ: {SCENE_DIR}")

# ==============================================================================
# [PART 1] ì†Œì„¤ ë¡œë“œ ë° ì²­í‚¹ (y4ì˜ ë¡œì§ + y5ì˜ ì €ì¥ ê¸°ëŠ¥)
# ==============================================================================
class DocumentLoader:
    """ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì—ì„œ ë¬¸ì„œ ë¡œë“œ (y4ì˜ ì¸ì½”ë”© ë°©ì–´ ë¡œì§ ìœ ì§€)"""
    @staticmethod
    def load_txt(file_path: str) -> str:
        # 1. chardetì„ ì´ìš©í•œ ì •ë°€ ê°ì§€
        if chardet:
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read()
                    result = chardet.detect(raw_data)
                    encoding = result['encoding']
                    if result['confidence'] > 0.7 and encoding:
                        return raw_data.decode(encoding)
            except Exception:
                pass
        
        # 2. ì‹¤íŒ¨ ì‹œ ì£¼ìš” ì¸ì½”ë”© ìˆœì°¨ ì‹œë„
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
        for enc in encodings:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    return f.read()
            except: continue
        raise ValueError(f"íŒŒì¼ ì¸ì½”ë”©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    @staticmethod
    def load_document(file_path: str) -> str:
        ext = os.path.splitext(file_path)[1].lower()
        if ext == '.txt': return DocumentLoader.load_txt(file_path)
        else: raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")

class SceneChunker:
    """ì”¬ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í•  (y4 ë¡œì§ ìœ ì§€)"""
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'í•™êµ', 'ì‚¬ë¬´ì‹¤', 'ì¹´í˜', 'ê³µì›', 'ë³‘ì›', 'ì—­', 'ìˆ²', 'ë°”ë‹¤', 'ê°•']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ê·¸ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ìƒˆë²½']

    def __init__(self, threshold: int = 7):
        self.threshold = threshold

    def split_into_scenes(self, text: str) -> List[str]:
        sentences = re.split(r'([.!?]\s+)', text)
        merged = []
        for i in range(0, len(sentences)-1, 2):
            merged.append(sentences[i] + (sentences[i+1] if i+1 < len(sentences) else ""))
        
        scenes, current_scene, score = [], [], 0
        for sent in merged:
            if not sent.strip(): continue
            if "***" in sent or "---" in sent: score += 10
            if any(loc in sent for loc in self.LOCATION_KEYWORDS): score += 5
            if any(t in sent for t in self.TIME_TRANSITIONS): score += 4
            
            current_scene.append(sent)
            if score >= self.threshold:
                scenes.append(" ".join(current_scene))
                current_scene, score = [], 0
        
        if current_scene: scenes.append(" ".join(current_scene))
        return scenes

class ParentChunker:
    """Parent ì²­í¬ ìƒì„± (y4 êµ¬ì¡° ìœ ì§€)"""
    @staticmethod
    def create_parent_chunks(scenes: List[str]) -> List[Dict]:
        return [{
            'id': f"scene_{i+1:03d}",
            'text': scene,
            'scene_index': i
        } for i, scene in enumerate(scenes)]

def process_file_chunking(file_path: str) -> List[Dict]:
    """í†µí•© ì²­í‚¹ ë° íŒŒì¼ ì €ì¥ í•¨ìˆ˜"""
    print(f"ğŸ“– íŒŒì¼ ì½ê¸° ë° ì²­í‚¹ ì‹œì‘: {file_path}")
    text = DocumentLoader.load_document(file_path)
    scenes = SceneChunker().split_into_scenes(text)
    chunks = ParentChunker.create_parent_chunks(scenes)
    
    # [ì¶”ê°€ë¨] ì”¬ë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ê¸°ëŠ¥ (y5 feature)
    print(f"ğŸ’¾ [ì €ì¥ 1] ì”¬ë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì¤‘ ({SCENE_DIR})...")
    for chunk in chunks:
        file_name = os.path.join(SCENE_DIR, f"{chunk['id']}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(chunk['text'])
            
    print(f"âœ… ì´ {len(chunks)}ê°œì˜ ì”¬(Scene)ìœ¼ë¡œ ë¶„í•  ë° ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return chunks

# ==============================================================================
# [PART 2] ë°ì´í„°ë² ì´ìŠ¤ (y4 ë¡œì§ ìœ ì§€)
# ==============================================================================
class NovelBibleDB:
    def __init__(self, db_params):
        print(f"ğŸ”Œ DB ì—°ê²° ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ({EMBEDDING_MODEL_NAME})...")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        self.conn = psycopg2.connect(**db_params)
        self.conn.autocommit = True
        self._initialize_table()

    def _initialize_table(self):
        with self.conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS story_bible (
                    id TEXT PRIMARY KEY,
                    embedding vector(1024), 
                    data JSONB
                );
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_story_data ON story_bible USING GIN (data);")

    def insert_scene(self, scene_data: Dict):
        vector = self.embedding_model.encode(scene_data['dense_summary']).tolist()
        with self.conn.cursor() as cur:
            cur.execute("""
                INSERT INTO story_bible (id, embedding, data)
                VALUES (%s, %s, %s)
                ON CONFLICT (id) DO UPDATE 
                SET embedding = EXCLUDED.embedding, data = EXCLUDED.data;
            """, (scene_data['scene_id'], vector, Json(scene_data)))
        print(f"ğŸ’¾ DB ì €ì¥: [{scene_data['scene_id']}] {scene_data['title']}")

    def search_similar_scenes(self, query: str, top_k=3):
        query_vec = self.embedding_model.encode(query).tolist()
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT data->>'title', data->>'dense_summary', 1 - (embedding <=> %s::vector) as score
                FROM story_bible
                ORDER BY embedding <=> %s::vector LIMIT %s;
            """, (query_vec, query_vec, top_k))
            return cur.fetchall()

    def get_all_wiki_data(self):
        with self.conn.cursor() as cur:
            cur.execute("SELECT data FROM story_bible ORDER BY id ASC;")
            return [row[0] for row in cur.fetchall()]

# ==============================================================================
# [PART 3] AI ë¶„ì„ê¸° (y4ì˜ ìƒì„¸ í”„ë¡¬í”„íŠ¸ ìœ ì§€)
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            model_name=LLM_MODEL_NAME,
            generation_config={"response_mime_type": "application/json"}
        )

    def analyze(self, chunk: Dict) -> Dict:
        """ë‹¨ì¼ ì”¬ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON ë°˜í™˜ (y4ì˜ ìƒì„¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        prompt = f"""
        ë‹¹ì‹ ì€ ì†Œì„¤ ì§‘í•„ì„ ë•ëŠ” 'ìŠ¤í† ë¦¬ ì–´ì‹œìŠ¤í„´íŠ¸'ì…ë‹ˆë‹¤.
        ì•„ë˜ ì†Œì„¤ì˜ í•œ ì¥ë©´(Scene)ì„ ì½ê³ , ì§‘í•„ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

        [ì…ë ¥ í…ìŠ¤íŠ¸]
        {chunk['text']}

        [ìš”ì²­ ì‚¬í•­]
        1. Meta Layer: ì–¸ì œ, ì–´ë””ì„œ, ëˆ„ê°€ ë‚˜ì˜¤ëŠ”ì§€ (í•„í„°ë§ìš©)
        2. Wiki Layer: ë“±ì¥í•œ ê³ ìœ ëª…ì‚¬(ì¸ë¬¼, ì¥ì†Œ, ë¬¼í’ˆ)ë¥¼ ë°±ê³¼ì‚¬ì „ í˜•íƒœë¡œ ìƒì„¸ ë¶„ì„.
           - description: ì´ ì¥ë©´ì—ì„œ ë¬˜ì‚¬ëœ ì™¸ì–‘ì´ë‚˜ ìƒíƒœ
           - action: ì´ ì¥ë©´ì—ì„œì˜ ì£¼ìš” í–‰ë™ (ì¸ë¬¼ì¸ ê²½ìš°)
        3. Vector Layer (dense_summary): "ëˆ„ê°€ ë¬´ì—‡ì„ ì™œ í–ˆëŠ”ì§€" ì¸ê³¼ê´€ê³„ê°€ ëª…í™•í•œ ìš”ì•½ë¬¸ (ê²€ìƒ‰ìš©)

        [ì¶œë ¥ í¬ë§· (JSON)]
        {{
          "scene_id": "{chunk['id']}",
          "title": "í•µì‹¬ì„ ê´€í†µí•˜ëŠ” ì†Œì œëª©",
          "meta": {{
            "time": "ì‹œê°„ì  ë°°ê²½",
            "place": "ê³µê°„ì  ë°°ê²½",
            "characters": ["ì¸ë¬¼1", "ì¸ë¬¼2"]
          }},
          "wiki_entities": [
            {{
              "name": "ì´ë¦„",
              "category": "ì¸ë¬¼" or "ì¥ì†Œ" or "ë¬¼í’ˆ",
              "sub_category": "ìƒì„¸ë¶„ë¥˜ (ì˜ˆ: ì£¼ì—°, ë¬´ê¸°)",
              "description": "ì„¤ëª…",
              "action": "í–‰ë™"
            }}
          ],
          "dense_summary": "ìš”ì•½ë¬¸"
        }}
        """
        try:
            response = self.model.generate_content(prompt)
            return json.loads(response.text)
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨ ({chunk['id']}): {e}")
            return None

# ==============================================================================
# [PART 4] ìœ„í‚¤/ë„ê° ìƒì„±ê¸° (y4 ë¡œì§ ê¸°ë°˜ + íŒŒì¼ ì €ì¥ ì¶”ê°€)
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def generate_and_save_report(storyboard_list: List[Dict]):
        """ë¶„ì„ëœ ë°ì´í„°ë¥¼ íŒŒì¼(Markdown)ë¡œ ì €ì¥"""
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        print(f"\nğŸ’¾ [ì €ì¥ 3] ì„¤ì •ì§‘ íŒŒì¼ ìƒì„± ì¤‘: {file_path}")
        
        wiki_db = defaultdict(lambda: defaultdict(list))

        # ë°ì´í„° ì§‘ê³„ (Aggregation)
        for scene in storyboard_list:
            title = scene.get('title', 'ë¬´ì œ')
            s_id = scene.get('scene_id')
            
            for entity in scene.get('wiki_entities', []):
                cat = entity.get('category', 'ê¸°íƒ€')
                name = entity.get('name', 'ì´ë¦„ë¯¸ìƒ')
                
                wiki_db[cat][name].append({
                    "scene": f"{s_id} ({title})",
                    "desc": entity.get('description'),
                    "action": entity.get('action')
                })

        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì“°ê¸°
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ“š ì†Œì„¤ ì„¤ì • ìë£Œì§‘ (Writer's Bible)\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ëª©ì°¨
            f.write("## ğŸ“‘ ëª©ì°¨\n")
            for cat in ["ì¸ë¬¼", "ë¬¼í’ˆ", "ì¥ì†Œ"]:
                if cat in wiki_db:
                    f.write(f"- {cat} ì‚¬ì „\n")
            f.write("\n---\n")

            # ë³¸ë¬¸ ì¶œë ¥
            for category in ["ì¸ë¬¼", "ë¬¼í’ˆ", "ì¥ì†Œ"]:
                if category in wiki_db:
                    f.write(f"\n## ğŸ“‚ {category} ì‚¬ì „\n")
                    for name, records in wiki_db[category].items():
                        f.write(f"\n### ğŸ”¹ {name} (ì´ {len(records)}íšŒ ë“±ì¥)\n")
                        for rec in records:
                            f.write(f"- **[{rec['scene']}]**\n")
                            f.write(f"  - ìƒíƒœ: {rec['desc']}\n")
                            if category == "ì¸ë¬¼" and rec['action']:
                                f.write(f"  - í–‰ë™: {rec['action']}\n")

        print("âœ… ì„¤ì •ì§‘(Markdown) ì €ì¥ ì™„ë£Œ.")

# ==============================================================================
# [ë©”ì¸ ì‹¤í–‰ ë¡œì§]
# ==============================================================================
def main():
    # 0. ì¤€ë¹„ ë° í´ë” ìƒì„±
    if "YOUR_GOOGLE_API_KEY" in GOOGLE_API_KEY:
        print("âŒ ê²½ê³ : êµ¬ê¸€ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    create_output_dirs()

    # í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ìƒì„±
    input_file = "test_novel.txt"
    if not os.path.exists(input_file):
        with open(input_file, "w", encoding="utf-8") as f:
            f.write("ì² ìˆ˜ê°€ ë‚¡ì€ ê²€ì„ ë“¤ê³  ìˆ²ìœ¼ë¡œ ë“¤ì–´ê°”ë‹¤. ìˆ²ì€ ì–´ë‘ì› ë‹¤. 'ì—¬ê¸° ì–´ë”˜ê°€ì— ì „ì„¤ì˜ ë°©íŒ¨ê°€ ìˆì„ ê±°ì•¼.' ê·¸ë•Œ ë‚˜ë¬´ ë’¤ì—ì„œ ëŠ‘ëŒ€ê°€ ë‚˜íƒ€ë‚¬ë‹¤.\n")
            f.write("ë‹¤ìŒë‚ , ì˜í¬ëŠ” ë§ˆì„ ê´‘ì¥ì—ì„œ ì² ìˆ˜ë¥¼ ê¸°ë‹¤ë ¸ë‹¤. ì² ìˆ˜ëŠ” ìƒì²˜íˆ¬ì„±ì´ì˜€ì§€ë§Œ ì†ì—ëŠ” ë¹›ë‚˜ëŠ” ë°©íŒ¨ë¥¼ ë“¤ê³  ìˆì—ˆë‹¤.")

    # 1. ì†Œì„¤ ì½ê¸° ë° ì²­í‚¹ (+ txt íŒŒì¼ ì €ì¥)
    chunks = process_file_chunking(input_file)

    # 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        db = NovelBibleDB(DB_CONFIG)
        analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    except Exception as e:
        print(f"\nâŒ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ Dockerê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, pip installì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 3. ë¶„ì„ ë° DB ì €ì¥ ë£¨í”„ (+ JSON ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘)
    print("\nğŸš€ AI ë¶„ì„ ë° DB êµ¬ì¶• ì‹œì‘...")
    all_storyboards = [] # íŒŒì¼ ì €ì¥ì„ ìœ„í•´ ë©”ëª¨ë¦¬ì— ì ì‹œ ë³´ê´€

    for chunk in chunks:
        storyboard = analyzer.analyze(chunk)
        if storyboard:
            # DB ì €ì¥
            db.insert_scene(storyboard)
            # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (íŒŒì¼ ì €ì¥ìš©)
            all_storyboards.append(storyboard)
            time.sleep(1) # API ì œí•œ ê³ ë ¤

    # 4. [ì €ì¥ 2] ì „ì²´ ë¶„ì„ ë°ì´í„° JSON ì €ì¥ (y5 feature)
    json_path = os.path.join(OUTPUT_DIR, "storyboard_analysis.json")
    print(f"\nğŸ’¾ [ì €ì¥ 2] ì „ì²´ ë¶„ì„ ë°ì´í„°(JSON) ì €ì¥ ì¤‘: {json_path}")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_storyboards, f, indent=2, ensure_ascii=False)
    print("âœ… JSON ì €ì¥ ì™„ë£Œ.")

    # 5. [ì €ì¥ 3] ìœ„í‚¤ ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„± (y5 feature + y4 logic)
    WikiGenerator.generate_and_save_report(all_storyboards)

    # 6. ê¸°ëŠ¥ ì‹œì—°: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (y4 feature)
    print("\nğŸ” [ê²€ìƒ‰ í…ŒìŠ¤íŠ¸] 'ì „íˆ¬ í›„ ì–»ì€ ì•„ì´í…œ'")
    results = db.search_similar_scenes("ì „íˆ¬ í›„ ì–»ì€ ì•„ì´í…œ")
    for title, summary, score in results:
        print(f"  - {title} (ìœ ì‚¬ë„: {score:.4f}) : {summary}")

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'output' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()