import os
import re
import json
import time
from typing import List, Dict, Any
from collections import defaultdict

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai
import psycopg2
from psycopg2.extras import Json
from sentence_transformers import SentenceTransformer

# ==============================================================================
# [ì„¤ì • ì˜ì—­] API í‚¤ ë° DB ì •ë³´ ì…ë ¥
# ==============================================================================
# 1. êµ¬ê¸€ API í‚¤ (ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "YOUR_GOOGLE_API_KEY_HERE")

# 2. LLM ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
LLM_MODEL_NAME = "gemini-2.0-flash-exp"  # (ë˜ëŠ” gemini-1.5-flash)
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"     # 1024ì°¨ì› ë‹¤êµ­ì–´ ëª¨ë¸

# 3. PostgreSQL ì ‘ì† ì •ë³´ (ìœ„ì˜ Docker ì„¤ì • ê¸°ì¤€)
DB_CONFIG = {
    "dbname": "postgres",
    "user": "postgres",
    "password": "mysecretpassword",
    "host": "localhost",
    "port": "5432"
}

# ==============================================================================
# [PART 1] ì†Œì„¤ ë¡œë“œ ë° ì²­í‚¹ (ì‘ì„±í•´ì£¼ì‹  ì½”ë“œ ê·¸ëŒ€ë¡œ í†µí•©)
# ==============================================================================
class DocumentLoader:
    """ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
    @staticmethod
    def load_txt(file_path: str) -> str:
        try:
            import chardet
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                result = chardet.detect(raw_data)
                encoding = result['encoding']
                if result['confidence'] > 0.7 and encoding:
                    return raw_data.decode(encoding)
        except ImportError:
            pass
        
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
        for enc in encodings:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    return f.read()
            except: continue
        raise ValueError("íŒŒì¼ ì¸ì½”ë”©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    @staticmethod
    def load_document(file_path: str) -> str:
        ext = os.path.splitext(file_path)[1].lower()
        if ext == '.txt': return DocumentLoader.load_txt(file_path)
        else: raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")

class SceneChunker:
    """ì”¬ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• """
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'í•™êµ', 'ì‚¬ë¬´ì‹¤', 'ì¹´í˜', 'ê³µì›', 'ë³‘ì›', 'ì—­', 'ìˆ²', 'ë°”ë‹¤', 'ê°•']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ê·¸ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ìƒˆë²½']

    def __init__(self, threshold: int = 7):
        self.threshold = threshold

    def split_into_scenes(self, text: str) -> List[str]:
        sentences = re.split(r'([.!?]\s+)', text)
        merged = []
        for i in range(0, len(sentences)-1, 2):
            merged.append(sentences[i] + (sentences[i+1] if i+1 < len(sentences) else ""))
        
        scenes, current_scene, score = [], [], 0
        for sent in merged:
            if not sent.strip(): continue
            if "***" in sent or "---" in sent: score += 10
            if any(loc in sent for loc in self.LOCATION_KEYWORDS): score += 5
            if any(t in sent for t in self.TIME_TRANSITIONS): score += 4
            
            current_scene.append(sent)
            if score >= self.threshold:
                scenes.append(" ".join(current_scene))
                current_scene, score = [], 0
        
        if current_scene: scenes.append(" ".join(current_scene))
        return scenes

class ParentChunker:
    """Parent ì²­í¬ ìƒì„±"""
    @staticmethod
    def create_parent_chunks(scenes: List[str]) -> List[Dict]:
        return [{
            'id': f"scene_{i+1:03d}",
            'text': scene,
            'scene_index': i
        } for i, scene in enumerate(scenes)]

def process_file_chunking(file_path: str) -> List[Dict]:
    """í†µí•© ì²­í‚¹ í•¨ìˆ˜"""
    print(f"ğŸ“– íŒŒì¼ ì½ê¸° ë° ì²­í‚¹ ì‹œì‘: {file_path}")
    text = DocumentLoader.load_document(file_path)
    scenes = SceneChunker().split_into_scenes(text)
    chunks = ParentChunker.create_parent_chunks(scenes)
    print(f"âœ… ì´ {len(chunks)}ê°œì˜ ì”¬(Scene)ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return chunks

# ==============================================================================
# [PART 2] ë°ì´í„°ë² ì´ìŠ¤ (PostgreSQL + pgvector + JSONB)
# ==============================================================================
class NovelBibleDB:
    def __init__(self, db_params):
        print(f"ğŸ”Œ DB ì—°ê²° ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ({EMBEDDING_MODEL_NAME})...")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        self.conn = psycopg2.connect(**db_params)
        self.conn.autocommit = True
        self._initialize_table()

    def _initialize_table(self):
        """í…Œì´ë¸” ë° í™•ì¥ ê¸°ëŠ¥ ì´ˆê¸°í™”"""
        with self.conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            # JSONB êµ¬ì¡°: { scene_id, title, meta: {}, wiki_entities: [], dense_summary: "" }
            cur.execute("""
                CREATE TABLE IF NOT EXISTS story_bible (
                    id TEXT PRIMARY KEY,
                    embedding vector(1024), 
                    data JSONB
                );
            """)
            # JSONB ë‚´ë¶€ ê²€ìƒ‰ ê°€ì†ì„ ìœ„í•œ GIN ì¸ë±ìŠ¤
            cur.execute("CREATE INDEX IF NOT EXISTS idx_story_data ON story_bible USING GIN (data);")

    def insert_scene(self, scene_data: Dict):
        """ë¶„ì„ëœ JSON ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
        # ì„ë² ë”© ìƒì„± (ê²€ìƒ‰ìš© ìš”ì•½ë¬¸ ê¸°ì¤€)
        vector = self.embedding_model.encode(scene_data['dense_summary']).tolist()
        
        with self.conn.cursor() as cur:
            cur.execute("""
                INSERT INTO story_bible (id, embedding, data)
                VALUES (%s, %s, %s)
                ON CONFLICT (id) DO UPDATE 
                SET embedding = EXCLUDED.embedding, data = EXCLUDED.data;
            """, (scene_data['scene_id'], vector, Json(scene_data)))
        print(f"ğŸ’¾ DB ì €ì¥: [{scene_data['scene_id']}] {scene_data['title']}")

    def search_similar_scenes(self, query: str, top_k=3):
        """ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜)"""
        query_vec = self.embedding_model.encode(query).tolist()
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT data->>'title', data->>'dense_summary', 1 - (embedding <=> %s::vector) as score
                FROM story_bible
                ORDER BY embedding <=> %s::vector LIMIT %s;
            """, (query_vec, query_vec, top_k))
            return cur.fetchall()

    def get_all_wiki_data(self):
        """Wiki ìƒì„±ì„ ìœ„í•´ ëª¨ë“  ë°ì´í„° ì¡°íšŒ"""
        with self.conn.cursor() as cur:
            cur.execute("SELECT data FROM story_bible ORDER BY id ASC;")
            return [row[0] for row in cur.fetchall()]

# ==============================================================================
# [PART 3] AI ë¶„ì„ê¸° (Gemini)
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            model_name=LLM_MODEL_NAME,
            generation_config={"response_mime_type": "application/json"}
        )

    def analyze(self, chunk: Dict) -> Dict:
        """ë‹¨ì¼ ì”¬ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON ë°˜í™˜"""
        prompt = f"""
        ë‹¹ì‹ ì€ ì†Œì„¤ ì§‘í•„ì„ ë•ëŠ” 'ìŠ¤í† ë¦¬ ì–´ì‹œìŠ¤í„´íŠ¸'ì…ë‹ˆë‹¤.
        ì•„ë˜ ì†Œì„¤ì˜ í•œ ì¥ë©´(Scene)ì„ ì½ê³ , ì§‘í•„ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

        [ì…ë ¥ í…ìŠ¤íŠ¸]
        {chunk['text']}

        [ìš”ì²­ ì‚¬í•­]
        1. Meta Layer: ì–¸ì œ, ì–´ë””ì„œ, ëˆ„ê°€ ë‚˜ì˜¤ëŠ”ì§€ (í•„í„°ë§ìš©)
        2. Wiki Layer: ë“±ì¥í•œ ê³ ìœ ëª…ì‚¬(ì¸ë¬¼, ì¥ì†Œ, ë¬¼í’ˆ)ë¥¼ ë°±ê³¼ì‚¬ì „ í˜•íƒœë¡œ ìƒì„¸ ë¶„ì„.
           - description: ì´ ì¥ë©´ì—ì„œ ë¬˜ì‚¬ëœ ì™¸ì–‘ì´ë‚˜ ìƒíƒœ
           - action: ì´ ì¥ë©´ì—ì„œì˜ ì£¼ìš” í–‰ë™ (ì¸ë¬¼ì¸ ê²½ìš°)
        3. Vector Layer (dense_summary): "ëˆ„ê°€ ë¬´ì—‡ì„ ì™œ í–ˆëŠ”ì§€" ì¸ê³¼ê´€ê³„ê°€ ëª…í™•í•œ ìš”ì•½ë¬¸ (ê²€ìƒ‰ìš©)

        [ì¶œë ¥ í¬ë§· (JSON)]
        {{
          "scene_id": "{chunk['id']}",
          "title": "í•µì‹¬ì„ ê´€í†µí•˜ëŠ” ì†Œì œëª©",
          "meta": {{
            "time": "ì‹œê°„ì  ë°°ê²½",
            "place": "ê³µê°„ì  ë°°ê²½",
            "characters": ["ì¸ë¬¼1", "ì¸ë¬¼2"]
          }},
          "wiki_entities": [
            {{
              "name": "ì´ë¦„",
              "category": "ì¸ë¬¼" or "ì¥ì†Œ" or "ë¬¼í’ˆ",
              "sub_category": "ìƒì„¸ë¶„ë¥˜ (ì˜ˆ: ì£¼ì—°, ë¬´ê¸°)",
              "description": "ì„¤ëª…",
              "action": "í–‰ë™"
            }}
          ],
          "dense_summary": "ìš”ì•½ë¬¸"
        }}
        """
        try:
            response = self.model.generate_content(prompt)
            return json.loads(response.text)
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨ ({chunk['id']}): {e}")
            return None

# ==============================================================================
# [PART 4] ìœ„í‚¤/ë„ê° ìƒì„±ê¸° (Aggregation)
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def generate_report(db: NovelBibleDB):
        print("\n" + "="*50)
        print("ğŸ“š [ìë™ ìƒì„±] ì†Œì„¤ ì„¤ì • ìë£Œì§‘ (Writer's Bible)")
        print("="*50)
        
        all_scenes = db.get_all_wiki_data()
        wiki_db = defaultdict(lambda: defaultdict(list))

        # ë°ì´í„° ì§‘ê³„ (Aggregation)
        for scene in all_scenes:
            title = scene.get('title', 'ë¬´ì œ')
            s_id = scene.get('scene_id')
            
            for entity in scene.get('wiki_entities', []):
                cat = entity.get('category', 'ê¸°íƒ€')
                name = entity.get('name', 'ì´ë¦„ë¯¸ìƒ')
                
                wiki_db[cat][name].append({
                    "scene": f"{s_id} ({title})",
                    "desc": entity.get('description'),
                    "action": entity.get('action')
                })

        # ì¶œë ¥
        for category in ["ì¸ë¬¼", "ë¬¼í’ˆ", "ì¥ì†Œ"]:
            if category in wiki_db:
                print(f"\n## ğŸ“‚ {category} ì‚¬ì „")
                for name, records in wiki_db[category].items():
                    print(f"\n  ğŸ”¹ {name} (ì´ {len(records)}íšŒ ë“±ì¥)")
                    for rec in records:
                        print(f"     [ğŸ“{rec['scene']}]")
                        print(f"       - ìƒíƒœ: {rec['desc']}")
                        if category == "ì¸ë¬¼" and rec['action']:
                            print(f"       - í–‰ë™: {rec['action']}")

# ==============================================================================
# [ë©”ì¸ ì‹¤í–‰ ë¡œì§]
# ==============================================================================
def main():
    # 0. ì¤€ë¹„
    if "YOUR_GOOGLE_API_KEY" in GOOGLE_API_KEY:
        print("âŒ ê²½ê³ : êµ¬ê¸€ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ íŒŒì¼ ìƒì„± (íŒŒì¼ì´ ì—†ì„ ê²½ìš°)
    input_file = "test_novel.txt"
    if not os.path.exists(input_file):
        with open(input_file, "w", encoding="utf-8") as f:
            f.write("ì² ìˆ˜ê°€ ë‚¡ì€ ê²€ì„ ë“¤ê³  ìˆ²ìœ¼ë¡œ ë“¤ì–´ê°”ë‹¤. ìˆ²ì€ ì–´ë‘ì› ë‹¤. 'ì—¬ê¸° ì–´ë”˜ê°€ì— ì „ì„¤ì˜ ë°©íŒ¨ê°€ ìˆì„ ê±°ì•¼.' ê·¸ë•Œ ë‚˜ë¬´ ë’¤ì—ì„œ ëŠ‘ëŒ€ê°€ ë‚˜íƒ€ë‚¬ë‹¤.\n")
            f.write("ë‹¤ìŒë‚ , ì˜í¬ëŠ” ë§ˆì„ ê´‘ì¥ì—ì„œ ì² ìˆ˜ë¥¼ ê¸°ë‹¤ë ¸ë‹¤. ì² ìˆ˜ëŠ” ìƒì²˜íˆ¬ì„±ì´ì˜€ì§€ë§Œ ì†ì—ëŠ” ë¹›ë‚˜ëŠ” ë°©íŒ¨ë¥¼ ë“¤ê³  ìˆì—ˆë‹¤.")

    # 1. ì†Œì„¤ ì½ê¸° ë° ì²­í‚¹
    chunks = process_file_chunking(input_file)

    # 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        db = NovelBibleDB(DB_CONFIG)
        analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    except Exception as e:
        print(f"\nâŒ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ Dockerê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, pip installì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 3. ë¶„ì„ ë° DB ì €ì¥ ë£¨í”„
    print("\nğŸš€ AI ë¶„ì„ ë° DB êµ¬ì¶• ì‹œì‘...")
    for chunk in chunks:
        # ì´ë¯¸ ì²˜ë¦¬ëœ IDì¸ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŒ
        storyboard = analyzer.analyze(chunk)
        if storyboard:
            db.insert_scene(storyboard)
            time.sleep(1) # API ì œí•œ ê³ ë ¤

    # 4. ê¸°ëŠ¥ ì‹œì—°: ìœ„í‚¤ ë¦¬í¬íŠ¸ ìƒì„±
    WikiGenerator.generate_report(db)

    # 5. ê¸°ëŠ¥ ì‹œì—°: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    print("\nğŸ” [ê²€ìƒ‰ í…ŒìŠ¤íŠ¸] 'ì „íˆ¬ í›„ ì–»ì€ ì•„ì´í…œ'")
    results = db.search_similar_scenes("ì „íˆ¬ í›„ ì–»ì€ ì•„ì´í…œ")
    for title, summary, score in results:
        print(f"  - {title} (ìœ ì‚¬ë„: {score:.4f}) : {summary}")

if __name__ == "__main__":
    main()