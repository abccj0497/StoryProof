import os
import re
import json
import time
from typing import List, Dict, Any
from collections import defaultdict
from google import genai
from google.genai import types

# =========================================================
# [í™˜ê²½ ì„¤ì •]
# =========================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4")
INPUT_FILE_NAME = "KR_fantasy_alice.txt" 

# gemini-2.5ëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê°€ì¥ ìµœì‹ ì¸ 1.5-flash ë˜ëŠ” 2.0-flash-expë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
LLM_MODEL_NAME = "gemini-2.5-flash" 
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)

# ==============================================================================
# [PART 1] í•˜ì´ë¸Œë¦¬ë“œ ì²­ì»¤ (Hybrid Chunker)
# ==============================================================================
class HybridSceneChunker:
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'ìˆ²', 'êµ´', 'ì •ì›', 'í™€', 'ë°”ë‹¤', 'ì§‘ì•ˆ', 'ë‚˜ë¬´', 'ì„±', 'ë§ˆì„', 'êµì‹¤', 'ë³µë„', 'ì°½ê°€']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ê°‘ìê¸°', 'ë©°ì¹  ë’¤', 'ëª‡ ì‹œê°„ í›„', 'ìƒˆë²½', 'ì˜¤í›„']
    CHAPTER_PATTERNS = [r"^\s*ì œ\s*[0-9]+\s*[ì¥í™”í¸]", r"^\s*Chapter\s*[0-9]+", r"^\s*\*\*\*"]

    def __init__(self, target_chars=3000, min_chars=1000, threshold=5):
        self.target_chars = target_chars
        self.min_chars = min_chars
        self.threshold = threshold

    def _calculate_score(self, text_segment):
        score = 0
        if any(k in text_segment for k in self.LOCATION_KEYWORDS): score += 5
        if any(k in text_segment for k in self.TIME_TRANSITIONS): score += 4
        return score

    def split_content(self, text: str) -> List[str]:
        text = text.replace('\r\n', '\n')
        paragraphs = re.split(r'\n\s*\n', text)
        final_scenes, current_scene, current_len = [], [], 0
        
        for para in paragraphs:
            para = para.strip()
            if not para: continue
            is_chapter = any(re.match(p, para, re.IGNORECASE) for p in self.CHAPTER_PATTERNS)
            score = self._calculate_score(para[:50])

            if (is_chapter and current_len > 0) or \
               (score >= self.threshold and current_len >= self.min_chars) or \
               (current_len + len(para) > self.target_chars):
                final_scenes.append("\n\n".join(current_scene))
                current_scene, current_len = [para], len(para)
            else:
                current_scene.append(para)
                current_len += len(para)

        if current_scene: final_scenes.append("\n\n".join(current_scene))
        return final_scenes

# ==============================================================================
# [PART 2] ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œê¸° (Story Analyzer)
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        ì†Œì„¤ì˜ ì¥ë©´ì„ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. 
        'Character' ë¶„ë¥˜ ì‹œ ì‘ê°€, ì €ì, ì±… ì œëª©(í”„ë¡œì íŠ¸ ì´ë¦„ ë“±)ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        [TEXT]
        {chunk['text'][:4000]}
        
        [OUTPUT JSON FORMAT]
        {{
          "scene_id": "{chunk['id']}",
          "book_info": {{ "title": "ì†Œì„¤ ì œëª©", "author": "ì‘ê°€ ì´ë¦„" }},
          "scene_title": "ì¥ë©´ ì†Œì œëª©",
          "summary": "ì¥ë©´ ìš”ì•½(3-5ë¬¸ì¥)",
          "entities": [
            {{ "name": "ì´ë¦„", "type": "Character/Place/Item", "desc": "ì™¸ëª¨ë‚˜ íŠ¹ì§•(ì˜ˆ: ì•¨ë¦¬ìŠ¤ì˜ ì—¬ë™ìƒ)", "action": "ì´ ì¥ë©´ì—ì„œì˜ í–‰ë™" }}
          ]
        }}
        """
        try:
            response = self.client.models.generate_content(
                model=LLM_MODEL_NAME,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            # AI ê²°ê³¼ íŒŒì‹±
            result = json.loads(response.text)
            
            # [ì—ëŸ¬ ë°©ì§€] ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë“¤ì–´ì˜¤ë©´ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì„ íƒ
            if isinstance(result, list):
                result = result[0]
            
            return result
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨ ({chunk['id']}): {e}")
            return None

# ==============================================================================
# [PART 3] ì„¤ì •ì§‘ ìƒì„±ê¸° (Wiki Generator)
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def save_report_to_file(storyboard_list: List[Dict]):
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        
        if not storyboard_list:
            print("âŒ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return

        # 1. ì†Œì„¤ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        first_valid = storyboard_list[0]
        book_title = first_valid.get('book_info', {}).get('title', 'Unknown Title')
        author = first_valid.get('book_info', {}).get('author', 'Unknown Author')
        
        # 2. ì—”í‹°í‹° ë¶„ë¥˜ (ì¸ë¬¼/ì¥ì†Œ/ì•„ì´í…œ)
        wiki = defaultdict(lambda: defaultdict(list))
        
        for scene in storyboard_list:
            # ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ë°©ì–´ ë¡œì§ ì¶”ê°€
            if isinstance(scene, list): scene = scene[0]
            
            s_id = scene.get('scene_id', 'unknown')
            for ent in scene.get('entities', []):
                # ì‘ê°€, ì œëª©, í”„ë¡œì íŠ¸ëª… ë“±ì´ ì¸ë¬¼ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° í•„í„°ë§
                if ent['name'] in [book_title, author, "Project", "Book"]: continue
                
                wiki[ent['type']][ent['name']].append({
                    "scene": s_id, "desc": ent['desc'], "action": ent['action']
                })

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ“š ì†Œì„¤ ë¶„ì„ ë°”ì´ë¸”: {book_title}\n\n")
            f.write(f"## 1. ì±… ì •ë³´ (Book Info)\n")
            f.write(f"- **ì œëª©:** {book_title}\n- **ì‘ê°€:** {author}\n\n")
            
            f.write(f"## 2. ì „ì²´ ìŠ¤í† ë¦¬ë¼ì¸ (Storyline)\n")
            for scene in storyboard_list:
                if isinstance(scene, list): scene = scene[0]
                f.write(f"- **[{scene.get('scene_id')}] {scene.get('scene_title','')}**\n")
                f.write(f"  - {scene.get('summary','')}\n")

            # ì‚¬ì „ ì„¹ì…˜ êµ¬ì„± (Character, Place, Item)
            sections = {
                "Character": "ë“±ì¥ì¸ë¬¼ ì‚¬ì „ (Characters)",
                "Place": "ì¥ì†Œ ì‚¬ì „ (Places)",
                "Item": "ì•„ì´í…œ ì‚¬ì „ (Items)"
            }
            
            for key, section_title in sections.items():
                f.write(f"\n## {section_title}\n")
                items = wiki.get(key, {})
                if not items:
                    f.write("- ê¸°ë¡ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
                    continue
                for name, details in items.items():
                    f.write(f"### {name}\n")
                    for d in details:
                        # (scene_001) íŠ¹ì§• / í–‰ë™ ìˆœìœ¼ë¡œ ê¸°ë¡
                        f.write(f"- `({d['scene']})` {d['desc']} / *{d['action']}*\n")
        
        print(f"âœ… ë°”ì´ë¸” ìƒì„± ì™„ë£Œ: {file_path}")

# ==============================================================================
# [ë©”ì¸ ì‹¤í–‰ë¶€]
# ==============================================================================
def main():
    create_output_dirs()
    if not os.path.exists(INPUT_FILE_NAME):
        print(f"âŒ {INPUT_FILE_NAME} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."); return

    # 1. ì›ë¬¸ ì½ê¸° ë° ì²­í‚¹
    print(f"ğŸ“– ì†Œì„¤ íŒŒì¼ì„ ì½ëŠ” ì¤‘: {INPUT_FILE_NAME}")
    try:
        with open(INPUT_FILE_NAME, 'r', encoding='utf-8') as f:
            text = f.read()
    except UnicodeDecodeError:
        with open(INPUT_FILE_NAME, 'r', encoding='cp949') as f:
            text = f.read()

    chunks = HybridSceneChunker().split_content(text)
    scene_data = [{'id': f"scene_{i+1:03d}", 'text': txt} for i, txt in enumerate(chunks)]

    # 2. ë¶„ì„ (ì „ì²´ ë¶„ì„)
    analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    results = []
    print(f"ğŸš€ ë¶„ì„ ì‹œì‘ (ì´ {len(scene_data)}ê°œ ì”¬)")

    for i, chunk in enumerate(scene_data):
        print(f"  â–¶ [{i+1}/{len(scene_data)}] {chunk['id']} ë¶„ì„ ì¤‘...")
        res = analyzer.analyze(chunk)
        if res: 
            results.append(res)
        
        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ íœ´ì‹
        time.sleep(1.0) 

    # 3. ì €ì¥
    if results:
        # JSON ë°ì´í„° ì €ì¥
        with open(os.path.join(OUTPUT_DIR, "storyboard_analysis.json"), "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ë§ˆí¬ë‹¤ìš´ ì„¤ì •ì§‘ ì €ì¥
        WikiGenerator.save_report_to_file(results)
    else:
        print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()