import os
import re
import json
import time
from typing import List, Dict, Optional
from collections import defaultdict

# =========================================================
# [ë³€ê²½ë¨] ìµœì‹  Google GenAI SDK ì„í¬íŠ¸
# =========================================================
from google import genai
from google.genai import types

# DB ë° ë²¡í„° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
import psycopg2
from psycopg2.extras import Json
from sentence_transformers import SentenceTransformer

# ==============================================================================
# [ì„¤ì • ì˜ì—­]
# ==============================================================================
# â˜… ì—¬ê¸°ì— ë³¸ì¸ì˜ API í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4")

LLM_MODEL_NAME = "gemini-2.0-flash-exp"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"

OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

# DB ì ‘ì† ì •ë³´
DB_CONFIG = {
    "dbname": "postgres", "user": "postgres", "password": "mysecretpassword",
    "host": "localhost", "port": "5432"
}

def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)
        print(f"ğŸ“ í´ë” ìƒì„± ì™„ë£Œ: {SCENE_DIR}")

# ==============================================================================
# [PART 1] ì†Œì„¤ ë¡œë“œ ë° ì²­í‚¹ (ê¸°ì¡´ ìœ ì§€)
# ==============================================================================
class DocumentLoader:
    @staticmethod
    def load_document(file_path: str) -> str:
        try:
            with open(file_path, 'r', encoding='utf-8') as f: return f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='cp949') as f: return f.read()

class SceneChunker:
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'ìˆ²', 'êµ´', 'ì •ì›', 'í™€', 'ë°”ë‹¤', 'ì§‘ì•ˆ', 'ë‚˜ë¬´']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ê°‘ìê¸°']

    def __init__(self, threshold: int = 7):
        self.threshold = threshold

    def split_into_scenes(self, text: str) -> List[str]:
        sentences = re.split(r'([.!?]\s+)', text)
        merged = []
        for i in range(0, len(sentences)-1, 2):
            merged.append(sentences[i] + (sentences[i+1] if i+1 < len(sentences) else ""))
        
        scenes, current_scene, score = [], [], 0
        for sent in merged:
            if not sent.strip(): continue
            if "***" in sent: score += 10
            if any(k in sent for k in self.LOCATION_KEYWORDS): score += 5
            if any(k in sent for k in self.TIME_TRANSITIONS): score += 4
            current_scene.append(sent)
            if score >= self.threshold:
                scenes.append(" ".join(current_scene))
                current_scene, score = [], 0
        if current_scene: scenes.append(" ".join(current_scene))
        return scenes

def process_and_save_chunks(file_path: str) -> List[Dict]:
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {file_path}")
    text = DocumentLoader.load_document(file_path)
    scenes = SceneChunker().split_into_scenes(text)
    
    chunks = []
    print(f"ğŸ’¾ [ì €ì¥ 1] ì²­í‚¹ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì¤‘ ({SCENE_DIR})...")
    for i, scene_text in enumerate(scenes):
        scene_id = f"scene_{i+1:03d}"
        file_name = os.path.join(SCENE_DIR, f"{scene_id}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(scene_text)
        chunks.append({'id': scene_id, 'text': scene_text, 'scene_index': i})
    print(f"âœ… ì´ {len(chunks)}ê°œ ì”¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ.")
    return chunks

# ==============================================================================
# [PART 2] DB ê´€ë¦¬ (PostgreSQL) - ê¸°ì¡´ ìœ ì§€
# ==============================================================================
class NovelBibleDB:
    def __init__(self, db_params):
        print(f"ğŸ”Œ DB ì—°ê²° ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ({EMBEDDING_MODEL_NAME})...")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        self.conn = psycopg2.connect(**db_params)
        self.conn.autocommit = True
        with self.conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            cur.execute("""CREATE TABLE IF NOT EXISTS story_bible (
                id TEXT PRIMARY KEY, embedding vector(1024), data JSONB);""")
            # JSON ë‚´ë¶€ ê²€ìƒ‰ì„ ìœ„í•œ GIN ì¸ë±ìŠ¤ (ì„ íƒì‚¬í•­)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_story_data ON story_bible USING GIN (data);")

    def insert_scene(self, scene_data: Dict):
        vector = self.embedding_model.encode(scene_data['dense_summary']).tolist()
        with self.conn.cursor() as cur:
            cur.execute("""INSERT INTO story_bible (id, embedding, data) VALUES (%s, %s, %s)
                ON CONFLICT (id) DO UPDATE SET embedding = EXCLUDED.embedding, data = EXCLUDED.data;""",
                (scene_data['scene_id'], vector, Json(scene_data)))
    
    def search_hybrid(self, query_text: str, filter_place: Optional[str] = None, filter_character: Optional[str] = None):
        query_vector = self.embedding_model.encode(query_text).tolist()
        sql = "SELECT data, 1 - (embedding <=> %s::vector) as similarity FROM story_bible WHERE 1=1"
        params = [query_vector]

        if filter_place:
            sql += " AND data->'meta'->>'place' LIKE %s"
            params.append(f"%{filter_place}%")
        if filter_character:
            sql += " AND data->'meta'->>'characters' LIKE %s"
            params.append(f"%{filter_character}%")

        sql += " ORDER BY embedding <=> %s::vector LIMIT 3;"
        params.append(query_vector)

        with self.conn.cursor() as cur:
            cur.execute(sql, params)
            results = cur.fetchall()
            print(f"\nğŸ” ê²€ìƒ‰: '{query_text}' (ì¥ì†Œ:{filter_place}, ì¸ë¬¼:{filter_character})")
            if not results: print("   ğŸ‘‰ ê²°ê³¼ ì—†ìŒ")
            for row in results:
                print(f"   [{row[1]:.4f}] {row[0]['title']}")
                print(f"     â”” {row[0]['dense_summary'][:60]}...")

# ==============================================================================
# [PART 3] AI ë¶„ì„ê¸° (â˜… ì¤‘ìš”: ì´ ë¶€ë¶„ì´ ë³€ê²½ë¨ â˜…)
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        # [ë³€ê²½] Client ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë°©ì‹ ì‚¬ìš©
        self.client = genai.Client(api_key=api_key)
        self.model_name = LLM_MODEL_NAME

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        Analyze this novel scene (Korean).
        [TEXT] {chunk['text'][:2000]}
        [OUTPUT JSON FORMAT]
        {{
          "scene_id": "{chunk['id']}", 
          "title": "ì†Œì œëª©",
          "meta": {{ "time": "ì‹œê°„", "place": "ì¥ì†Œ", "characters": ["ì¸ë¬¼1", "ì¸ë¬¼2"] }},
          "wiki_entities": [ {{ "name": "ì´ë¦„", "category": "ì¸ë¬¼/ë¬¼í’ˆ/ì¥ì†Œ", "description": "íŠ¹ì§•", "action": "í–‰ë™" }} ],
          "dense_summary": "ìš”ì•½ë¬¸"
        }}
        """
        try:
            # [ë³€ê²½] client.models.generate_content ì‚¬ìš©
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            return json.loads(response.text)
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨ ({chunk['id']}): {e}")
            return None

# ==============================================================================
# [PART 4] ë¦¬í¬íŠ¸ ìƒì„± (ê¸°ì¡´ ìœ ì§€)
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def save_report_to_file(storyboard_list: List[Dict]):
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        print(f"\nğŸ’¾ [ì €ì¥ 3] ì„¤ì •ì§‘ ìƒì„±: {file_path}")
        wiki_db = defaultdict(lambda: defaultdict(list))
        
        for scene in storyboard_list:
            s_id = scene.get('scene_id')
            title = scene.get('title', 'ë¬´ì œ')
            for entity in scene.get('wiki_entities', []):
                wiki_db[entity.get('category','ê¸°íƒ€')][entity.get('name','ë¯¸ìƒ')].append({
                    "scene": f"{s_id} ({title})", 
                    "desc": entity.get('description'), 
                    "action": entity.get('action')
                })

        with open(file_path, "w", encoding="utf-8") as f:
            f.write("# ğŸ“š ì†Œì„¤ ë¶„ì„ ë³´ê³ ì„œ\n\n")
            for cat, items in wiki_db.items():
                f.write(f"\n## {cat}\n")
                for name, recs in items.items():
                    f.write(f"### {name}\n")
                    for r in recs: f.write(f"- **{r['scene']}**: {r['desc']} / {r['action']}\n")

# ==============================================================================
# [MAIN]
# ==============================================================================
def main():
    if "YOUR_GOOGLE" in GOOGLE_API_KEY:
        print("âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš” (ì½”ë“œ ìƒë‹¨ GOOGLE_API_KEY ë³€ìˆ˜)")
        return

    create_output_dirs()
    input_file = "KR_fantasy_alice.txt"
    
    if not os.path.exists(input_file):
        print(f"âŒ '{input_file}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. ì²­í‚¹
    chunks = process_and_save_chunks(input_file)

    # 2. ì´ˆê¸°í™”
    try:
        db = NovelBibleDB(DB_CONFIG)
        analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    except Exception as e:
        print(f"âŒ ì ‘ì† ì˜¤ë¥˜: {e}")
        return

    all_storyboards = []
    
    print("\nğŸš€ ë¶„ì„ ì‹œì‘...")
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì•ë¶€ë¶„ 5ê°œë§Œ ë¶„ì„ (ì „ì²´ëŠ” chunks[:5] ì œê±°)
    for chunk in chunks[:5]: 
        print(f"  â–¶ {chunk['id']} ë¶„ì„ ì¤‘...")
        result = analyzer.analyze(chunk)
        if result:
            db.insert_scene(result)
            all_storyboards.append(result)
            time.sleep(1)

    # 3. ê²°ê³¼ ì €ì¥
    with open(os.path.join(OUTPUT_DIR, "storyboard_analysis.json"), "w", encoding="utf-8") as f:
        json.dump(all_storyboards, f, indent=2, ensure_ascii=False)
    
    WikiGenerator.save_report_to_file(all_storyboards)

    # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("ğŸ” DB ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*50)
    db.search_hybrid("ì´ìƒí•œ í† ë¼ë¥¼ ë”°ë¼ê°€ëŠ” ìƒí™©")
    db.search_hybrid("ë¬´ì–¸ê°€ë¥¼ ë¨¹ê±°ë‚˜ ë§ˆì‹œëŠ” ìƒí™©", filter_character="ì•¨ë¦¬ìŠ¤")

if __name__ == "__main__":
    main()