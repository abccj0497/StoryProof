import os
import re
import json
import time
from typing import List, Dict, Optional
from collections import defaultdict

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai
import psycopg2
from psycopg2.extras import Json
from sentence_transformers import SentenceTransformer

# ==============================================================================
# [ì„¤ì • ì˜ì—­]
# ==============================================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4") # <- API í‚¤ ì…ë ¥
LLM_MODEL_NAME = "gemini-2.5-flash"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"

# ê²°ê³¼ë¬¼ì„ ì €ì¥í•  í´ë” ì´ë¦„
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

DB_CONFIG = {
    "dbname": "postgres", "user": "postgres", "password": "mysecretpassword",
    "host": "localhost", "port": "5432"
}

# í´ë” ìë™ ìƒì„± í•¨ìˆ˜
def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)
        print(f"ğŸ“ í´ë” ìƒì„± ì™„ë£Œ: {SCENE_DIR}")

# ==============================================================================
# [PART 1] ì†Œì„¤ ë¡œë“œ ë° ì²­í‚¹ (+ TXT íŒŒì¼ ì €ì¥)
# ==============================================================================
class DocumentLoader:
    @staticmethod
    def load_document(file_path: str) -> str:
        try:
            with open(file_path, 'r', encoding='utf-8') as f: return f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='cp949') as f: return f.read()

class SceneChunker:
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'í•™êµ', 'ì‚¬ë¬´ì‹¤', 'ì¹´í˜', 'ê³µì›', 'ìˆ²', 'ì„±', 'ë§ˆì„']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ìƒˆë²½']

    def __init__(self, threshold: int = 7):
        self.threshold = threshold

    def split_into_scenes(self, text: str) -> List[str]:
        sentences = re.split(r'([.!?]\s+)', text)
        merged = []
        for i in range(0, len(sentences)-1, 2):
            merged.append(sentences[i] + (sentences[i+1] if i+1 < len(sentences) else ""))
        
        scenes, current_scene, score = [], [], 0
        for sent in merged:
            if not sent.strip(): continue
            if "***" in sent: score += 10
            if any(k in sent for k in self.LOCATION_KEYWORDS): score += 5
            if any(k in sent for k in self.TIME_TRANSITIONS): score += 4
            current_scene.append(sent)
            if score >= self.threshold:
                scenes.append(" ".join(current_scene))
                current_scene, score = [], 0
        if current_scene: scenes.append(" ".join(current_scene))
        return scenes

def process_and_save_chunks(file_path: str) -> List[Dict]:
    """ì²­í‚¹ í›„ íŒŒì¼ë¡œ ì €ì¥"""
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {file_path}")
    text = DocumentLoader.load_document(file_path)
    scenes = SceneChunker().split_into_scenes(text)
    
    chunks = []
    print(f"ğŸ’¾ [ì €ì¥ 1] ì²­í‚¹ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì¤‘ ({SCENE_DIR})...")
    
    for i, scene_text in enumerate(scenes):
        scene_id = f"scene_{i+1:03d}"
        file_name = os.path.join(SCENE_DIR, f"{scene_id}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(scene_text)
        chunks.append({'id': scene_id, 'text': scene_text, 'scene_index': i})
        
    print(f"âœ… ì´ {len(chunks)}ê°œ ì”¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ.")
    return chunks

# ==============================================================================
# [PART 2 & 3] DB ë° ë¶„ì„ê¸° (â˜… ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€ë¨ â˜…)
# ==============================================================================
class NovelBibleDB:
    def __init__(self, db_params):
        print(f"ğŸ”Œ DB ì—°ê²° ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ({EMBEDDING_MODEL_NAME})...")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        self.conn = psycopg2.connect(**db_params)
        self.conn.autocommit = True
        with self.conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            cur.execute("""CREATE TABLE IF NOT EXISTS story_bible (
                id TEXT PRIMARY KEY, embedding vector(1024), data JSONB);""")
            # JSONB ê²€ìƒ‰ ì†ë„ í–¥ìƒì„ ìœ„í•œ ì¸ë±ìŠ¤
            cur.execute("CREATE INDEX IF NOT EXISTS idx_story_data ON story_bible USING GIN (data);")

    def insert_scene(self, scene_data: Dict):
        vector = self.embedding_model.encode(scene_data['dense_summary']).tolist()
        with self.conn.cursor() as cur:
            cur.execute("""INSERT INTO story_bible (id, embedding, data) VALUES (%s, %s, %s)
                ON CONFLICT (id) DO UPDATE SET embedding = EXCLUDED.embedding, data = EXCLUDED.data;""",
                (scene_data['scene_id'], vector, Json(scene_data)))
    
    # â˜…â˜…â˜… [ì¶”ê°€ëœ í•µì‹¬ ê¸°ëŠ¥] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â˜…â˜…â˜…
    def search_hybrid(self, query_text: str, filter_place: Optional[str] = None, filter_character: Optional[str] = None, top_k: int = 3):
        """
        ë²¡í„° ê²€ìƒ‰(ì˜ë¯¸) + JSONB ê²€ìƒ‰(ì¡°ê±´)ì„ ë™ì‹œì— ìˆ˜í–‰
        """
        # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
        query_vector = self.embedding_model.encode(query_text).tolist()
        
        # 2. ê¸°ë³¸ SQL (ë²¡í„° ìœ ì‚¬ë„ ìˆœ ì •ë ¬)
        sql = """
            SELECT data, 1 - (embedding <=> %s::vector) as similarity
            FROM story_bible
            WHERE 1=1
        """
        params = [query_vector]

        # 3. ì¡°ê±´ í•„í„°ë§ ì¶”ê°€ (JSONB í™œìš©)
        
        # A. ì¥ì†Œ í•„í„° (meta -> placeê°€ ì¼ì¹˜í•˜ëŠ”ì§€)
        if filter_place:
            sql += " AND data->'meta'->>'place' = %s"
            params.append(filter_place)
            
        # B. ì¸ë¬¼ í•„í„° (wiki_entities ë°°ì—´ ì•ˆì— í•´ë‹¹ ì´ë¦„ì„ ê°€ì§„ ê°ì²´ê°€ ìˆëŠ”ì§€)
        if filter_character:
            # JSONBì˜ @> ì—°ì‚°ì ì‚¬ìš©: [{"name": "ì² ìˆ˜"}] ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            sql += " AND data->'wiki_entities' @> %s::jsonb"
            filter_json = json.dumps([{"name": filter_character}]) # ë°°ì—´ í˜•íƒœë¡œ ê²€ìƒ‰
            params.append(filter_json)

        # 4. ì •ë ¬ ë° ê°œìˆ˜ ì œí•œ
        sql += " ORDER BY embedding <=> %s::vector LIMIT %s;"
        params.append(query_vector)
        params.append(top_k)

        # 5. ì‹¤í–‰ ë° ì¶œë ¥
        with self.conn.cursor() as cur:
            cur.execute(sql, params)
            results = cur.fetchall()
            
            print(f"\nğŸ” ê²€ìƒ‰: '{query_text}' (ì¥ì†Œí•„í„°: {filter_place}, ì¸ë¬¼í•„í„°: {filter_character})")
            if not results:
                print("   ğŸ‘‰ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            for row in results:
                data = row[0]
                score = row[1]
                print(f"   [{score:.4f}] {data['title']}")
                print(f"     â”” ìš”ì•½: {data['dense_summary'][:60]}...")
                print(f"     â”” ì¥ì†Œ: {data['meta']['place']} | ì¸ë¬¼: {data['meta']['characters']}")

class StoryAnalyzer:
    def __init__(self, api_key):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name=LLM_MODEL_NAME, generation_config={"response_mime_type": "application/json"})

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        ì†Œì„¤ ì¥ë©´ ë¶„ì„ ìš”ì²­:
        [TEXT] {chunk['text']}
        [OUTPUT JSON]
        {{
          "scene_id": "{chunk['id']}", "title": "ì†Œì œëª©",
          "meta": {{ "time": "ì‹œê°„", "place": "ì¥ì†Œ", "characters": ["ì¸ë¬¼ëª…"] }},
          "wiki_entities": [ {{ "name": "ì´ë¦„", "category": "ì¸ë¬¼/ë¬¼í’ˆ/ì¥ì†Œ", "description": "íŠ¹ì§•", "action": "í–‰ë™" }} ],
          "dense_summary": "ìš”ì•½ë¬¸"
        }}
        """
        try:
            return json.loads(self.model.generate_content(prompt).text)
        except: return None

# ==============================================================================
# [PART 4] ìœ„í‚¤ ë¦¬í¬íŠ¸ ìƒì„± ë° íŒŒì¼ ì €ì¥
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def save_report_to_file(storyboard_list: List[Dict]):
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        print(f"\nğŸ’¾ [ì €ì¥ 3] ì„¤ì •ì§‘ íŒŒì¼ ìƒì„± ì¤‘: {file_path}")

        wiki_db = defaultdict(lambda: defaultdict(list))
        for scene in storyboard_list:
            title = scene.get('title', 'ë¬´ì œ')
            s_id = scene.get('scene_id')
            for entity in scene.get('wiki_entities', []):
                cat = entity.get('category', 'ê¸°íƒ€')
                name = entity.get('name', 'ì´ë¦„ë¯¸ìƒ')
                wiki_db[cat][name].append({
                    "scene": f"{s_id} ({title})",
                    "desc": entity.get('description'),
                    "action": entity.get('action')
                })

        with open(file_path, "w", encoding="utf-8") as f:
            f.write("# ğŸ“š ì†Œì„¤ ì„¤ì • ìë£Œì§‘ (Writer's Bible)\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("## ëª©ì°¨\n")
            for cat in ["ì¸ë¬¼", "ë¬¼í’ˆ", "ì¥ì†Œ"]:
                if cat in wiki_db: f.write(f"- {cat} ì‚¬ì „\n")
            f.write("\n---\n")
            for category in ["ì¸ë¬¼", "ë¬¼í’ˆ", "ì¥ì†Œ"]:
                if category in wiki_db:
                    f.write(f"\n## ğŸ“‚ {category} ì‚¬ì „\n")
                    for name, records in wiki_db[category].items():
                        f.write(f"\n### ğŸ”¹ {name} (ì´ {len(records)}íšŒ ë“±ì¥)\n")
                        for rec in records:
                            f.write(f"- **{rec['scene']}**\n")
                            f.write(f"  - ìƒíƒœ: {rec['desc']}\n")
                            if category == "ì¸ë¬¼" and rec['action']:
                                f.write(f"  - í–‰ë™: {rec['action']}\n")
        print("âœ… ì„¤ì •ì§‘ íŒŒì¼ ì €ì¥ ì™„ë£Œ.")

# ==============================================================================
# [MAIN]
# ==============================================================================
def main():
    if "YOUR_GOOGLE_API_KEY" in GOOGLE_API_KEY:
        print("âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    create_output_dirs()
    
    # 0. í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¤€ë¹„
    input_file = "test_novel.txt"
    if not os.path.exists(input_file):
        with open(input_file, "w", encoding="utf-8") as f:
            f.write("ì² ìˆ˜ê°€ ì–´ë‘ìš´ ìˆ²ì— ë“¤ì–´ê°”ë‹¤. ëŠ‘ëŒ€ê°€ ë‚˜íƒ€ë‚˜ ê·¸ë¥¼ ìœ„í˜‘í–ˆë‹¤.\në‹¤ìŒë‚ , ì² ìˆ˜ëŠ” ë§ˆì„ ê´‘ì¥ì—ì„œ ì˜í¬ë¥¼ ë§Œë‚˜ ë‚¡ì€ ê²€ì„ ìë‘í–ˆë‹¤.")

    # 1. ì²­í‚¹ ë° TXT ì €ì¥
    chunks = process_and_save_chunks(input_file)

    # 2. DB ë° ë¶„ì„ê¸° ì¤€ë¹„
    try:
        db = NovelBibleDB(DB_CONFIG)
        analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    except Exception as e:
        print(f"âŒ ì ‘ì† ì˜¤ë¥˜: {e}"); return

    # 3. ë¶„ì„, DB ì €ì¥, JSON íŒŒì¼ ì €ì¥ ì¤€ë¹„
    all_storyboards = []
    
    print("\nğŸš€ ë¶„ì„ ì‹œì‘...")
    for chunk in chunks:
        storyboard = analyzer.analyze(chunk)
        if storyboard:
            db.insert_scene(storyboard) # DB ì €ì¥
            all_storyboards.append(storyboard) # íŒŒì¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
            time.sleep(1)

    # 4. íŒŒì¼ ì €ì¥ ìˆ˜í–‰
    json_path = os.path.join(OUTPUT_DIR, "storyboard_analysis.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_storyboards, f, indent=2, ensure_ascii=False)
    
    WikiGenerator.save_report_to_file(all_storyboards)

    # 5. [ì¶”ê°€ë¨] ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("ğŸ” DB ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Semantic + Condition)")
    print("="*50)

    # Case A: ë‹¨ìˆœ ì˜ë¯¸ ê²€ìƒ‰
    # (í…ìŠ¤íŠ¸ì—ëŠ” 'ìœ„í˜‘'ë§Œ ìˆì§€ë§Œ, 'ê¸´ì¥ê°'ìœ¼ë¡œ ê²€ìƒ‰í•´ë„ ì°¾ìŒ)
    db.search_hybrid("ê¸´ì¥ê°ì´ ê°ë„ëŠ” ìˆ²ì†")

    # Case B: ì¡°ê±´ ê²€ìƒ‰ (ì¸ë¬¼ í•„í„°)
    # ('ì² ìˆ˜'ê°€ ë‚˜ì˜¨ ì¥ë©´ ì¤‘ì—ì„œ 'ì•„ì´í…œ' ê´€ë ¨ ë‚´ìš© ì°¾ê¸°)
    db.search_hybrid("ë¬´ì–¸ê°€ë¥¼ ì–»ê±°ë‚˜ ìë‘í•¨", filter_character="ì² ìˆ˜")

    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    main()