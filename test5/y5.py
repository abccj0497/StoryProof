import os
import re
import json
import time
from typing import List, Dict
from collections import defaultdict

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai
import psycopg2
from psycopg2.extras import Json
from sentence_transformers import SentenceTransformer

# ==============================================================================
# [ì„¤ì • ì˜ì—­]
# ==============================================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4") # API í‚¤ ì…ë ¥
LLM_MODEL_NAME = "gemini-2.5-flash"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"

# ê²°ê³¼ë¬¼ì„ ì €ì¥í•  í´ë” ì´ë¦„
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

DB_CONFIG = {
    "dbname": "postgres", "user": "postgres", "password": "mysecretpassword",
    "host": "localhost", "port": "5432"
}

# í´ë” ìë™ ìƒì„± í•¨ìˆ˜
def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)
        print(f"ğŸ“ í´ë” ìƒì„± ì™„ë£Œ: {SCENE_DIR}")

# ==============================================================================
# [PART 1] ì†Œì„¤ ë¡œë“œ ë° ì²­í‚¹ (+ TXT íŒŒì¼ ì €ì¥)
# ==============================================================================
class DocumentLoader:
    @staticmethod
    def load_document(file_path: str) -> str:
        # (ê°„ì†Œí™”ë¥¼ ìœ„í•´ utf-8 ê¸°ë³¸ ë¡œë“œ, í•„ìš”ì‹œ chardet ì¶”ê°€)
        try:
            with open(file_path, 'r', encoding='utf-8') as f: return f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='cp949') as f: return f.read()

class SceneChunker:
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'í•™êµ', 'ì‚¬ë¬´ì‹¤', 'ì¹´í˜', 'ê³µì›', 'ìˆ²', 'ì„±', 'ë§ˆì„']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ìƒˆë²½']

    def __init__(self, threshold: int = 7):
        self.threshold = threshold

    def split_into_scenes(self, text: str) -> List[str]:
        sentences = re.split(r'([.!?]\s+)', text)
        merged = []
        for i in range(0, len(sentences)-1, 2):
            merged.append(sentences[i] + (sentences[i+1] if i+1 < len(sentences) else ""))
        
        scenes, current_scene, score = [], [], 0
        for sent in merged:
            if not sent.strip(): continue
            if "***" in sent: score += 10
            if any(k in sent for k in self.LOCATION_KEYWORDS): score += 5
            if any(k in sent for k in self.TIME_TRANSITIONS): score += 4
            current_scene.append(sent)
            if score >= self.threshold:
                scenes.append(" ".join(current_scene))
                current_scene, score = [], 0
        if current_scene: scenes.append(" ".join(current_scene))
        return scenes

def process_and_save_chunks(file_path: str) -> List[Dict]:
    """ì²­í‚¹ í›„ íŒŒì¼ë¡œ ì €ì¥"""
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {file_path}")
    text = DocumentLoader.load_document(file_path)
    scenes = SceneChunker().split_into_scenes(text)
    
    chunks = []
    print(f"ğŸ’¾ [ì €ì¥ 1] ì²­í‚¹ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì¤‘ ({SCENE_DIR})...")
    
    for i, scene_text in enumerate(scenes):
        scene_id = f"scene_{i+1:03d}"
        
        # 1. ì”¬ë³„ txt íŒŒì¼ ì €ì¥
        file_name = os.path.join(SCENE_DIR, f"{scene_id}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(scene_text)
            
        chunks.append({'id': scene_id, 'text': scene_text, 'scene_index': i})
        
    print(f"âœ… ì´ {len(chunks)}ê°œ ì”¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ.")
    return chunks

# ==============================================================================
# [PART 2 & 3] DB ë° ë¶„ì„ê¸° (ê¸°ì¡´ ë™ì¼)
# ==============================================================================
class NovelBibleDB:
    def __init__(self, db_params):
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        self.conn = psycopg2.connect(**db_params)
        self.conn.autocommit = True
        with self.conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            cur.execute("""CREATE TABLE IF NOT EXISTS story_bible (
                id TEXT PRIMARY KEY, embedding vector(1024), data JSONB);""")

    def insert_scene(self, scene_data: Dict):
        vector = self.embedding_model.encode(scene_data['dense_summary']).tolist()
        with self.conn.cursor() as cur:
            cur.execute("""INSERT INTO story_bible (id, embedding, data) VALUES (%s, %s, %s)
                ON CONFLICT (id) DO UPDATE SET embedding = EXCLUDED.embedding, data = EXCLUDED.data;""",
                (scene_data['scene_id'], vector, Json(scene_data)))

class StoryAnalyzer:
    def __init__(self, api_key):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name=LLM_MODEL_NAME, generation_config={"response_mime_type": "application/json"})

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        ì†Œì„¤ ì¥ë©´ ë¶„ì„ ìš”ì²­:
        [TEXT] {chunk['text']}
        [OUTPUT JSON]
        {{
          "scene_id": "{chunk['id']}", "title": "ì†Œì œëª©",
          "meta": {{ "time": "ì‹œê°„", "place": "ì¥ì†Œ", "characters": ["ì¸ë¬¼ëª…"] }},
          "wiki_entities": [ {{ "name": "ì´ë¦„", "category": "ì¸ë¬¼/ë¬¼í’ˆ/ì¥ì†Œ", "description": "íŠ¹ì§•", "action": "í–‰ë™" }} ],
          "dense_summary": "ìš”ì•½ë¬¸"
        }}
        """
        try:
            return json.loads(self.model.generate_content(prompt).text)
        except: return None

# ==============================================================================
# [PART 4] ìœ„í‚¤ ë¦¬í¬íŠ¸ ìƒì„± ë° íŒŒì¼ ì €ì¥ (í•µì‹¬ ì¶”ê°€)
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def save_report_to_file(storyboard_list: List[Dict]):
        """
        ë¶„ì„ëœ ëª¨ë“  ë°ì´í„°ë¥¼ ëª¨ì•„ì„œ 'ì„¤ì •ì§‘ íŒŒì¼(Markdown)'ë¡œ ì €ì¥
        """
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        print(f"\nğŸ’¾ [ì €ì¥ 3] ì„¤ì •ì§‘ íŒŒì¼ ìƒì„± ì¤‘: {file_path}")

        wiki_db = defaultdict(lambda: defaultdict(list))
        
        # ë°ì´í„° ì§‘ê³„
        for scene in storyboard_list:
            title = scene.get('title', 'ë¬´ì œ')
            s_id = scene.get('scene_id')
            for entity in scene.get('wiki_entities', []):
                cat = entity.get('category', 'ê¸°íƒ€')
                name = entity.get('name', 'ì´ë¦„ë¯¸ìƒ')
                wiki_db[cat][name].append({
                    "scene": f"{s_id} ({title})",
                    "desc": entity.get('description'),
                    "action": entity.get('action')
                })

        # íŒŒì¼ ì“°ê¸°
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("# ğŸ“š ì†Œì„¤ ì„¤ì • ìë£Œì§‘ (Writer's Bible)\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ëª©ì°¨
            f.write("## ëª©ì°¨\n")
            for cat in ["ì¸ë¬¼", "ë¬¼í’ˆ", "ì¥ì†Œ"]:
                if cat in wiki_db: f.write(f"- {cat} ì‚¬ì „\n")
            f.write("\n---\n")

            # ë‚´ìš©
            for category in ["ì¸ë¬¼", "ë¬¼í’ˆ", "ì¥ì†Œ"]:
                if category in wiki_db:
                    f.write(f"\n## ğŸ“‚ {category} ì‚¬ì „\n")
                    for name, records in wiki_db[category].items():
                        f.write(f"\n### ğŸ”¹ {name} (ì´ {len(records)}íšŒ ë“±ì¥)\n")
                        for rec in records:
                            f.write(f"- **{rec['scene']}**\n")
                            f.write(f"  - ìƒíƒœ: {rec['desc']}\n")
                            if category == "ì¸ë¬¼" and rec['action']:
                                f.write(f"  - í–‰ë™: {rec['action']}\n")
        
        print("âœ… ì„¤ì •ì§‘ íŒŒì¼ ì €ì¥ ì™„ë£Œ.")

# ==============================================================================
# [MAIN]
# ==============================================================================
def main():
    if "YOUR_GOOGLE_API_KEY" in GOOGLE_API_KEY:
        print("âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    create_output_dirs() # í´ë” ìƒì„±
    
    # 0. í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¤€ë¹„
    input_file = "test_novel.txt"
    if not os.path.exists(input_file):
        with open(input_file, "w", encoding="utf-8") as f:
            f.write("ì² ìˆ˜ê°€ ìˆ²ì— ê°”ë‹¤. ëŠ‘ëŒ€ê°€ ë‚˜íƒ€ë‚¬ë‹¤. 'ìœ¼ì•…!' ì² ìˆ˜ëŠ” ë„ë§ì³¤ë‹¤.\në‹¤ìŒë‚ , ì² ìˆ˜ëŠ” ë‚¡ì€ ê²€ì„ ì°¾ì•˜ë‹¤.")

    # 1. ì²­í‚¹ ë° TXT ì €ì¥
    chunks = process_and_save_chunks(input_file)

    # 2. DB ë° ë¶„ì„ê¸° ì¤€ë¹„
    try:
        db = NovelBibleDB(DB_CONFIG)
        analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    except Exception as e:
        print(f"âŒ ì ‘ì† ì˜¤ë¥˜: {e}"); return

    # 3. ë¶„ì„, DB ì €ì¥, JSON íŒŒì¼ ì €ì¥ ì¤€ë¹„
    all_storyboards = [] # ì „ì²´ ë°ì´í„°ë¥¼ ëª¨ì„ ë¦¬ìŠ¤íŠ¸
    
    print("\nğŸš€ ë¶„ì„ ì‹œì‘...")
    for chunk in chunks:
        storyboard = analyzer.analyze(chunk)
        if storyboard:
            # DB ì €ì¥
            db.insert_scene(storyboard)
            # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (íŒŒì¼ ì €ì¥ìš©)
            all_storyboards.append(storyboard)
            time.sleep(1)

    # 4. [ì €ì¥ 2] JSON í†µí•© íŒŒì¼ ì €ì¥
    json_path = os.path.join(OUTPUT_DIR, "storyboard_analysis.json")
    print(f"\nğŸ’¾ [ì €ì¥ 2] ì „ì²´ ë¶„ì„ ë°ì´í„°(JSON) ì €ì¥ ì¤‘: {json_path}")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_storyboards, f, indent=2, ensure_ascii=False)
    print("âœ… JSON ì €ì¥ ì™„ë£Œ.")

    # 5. [ì €ì¥ 3] ì„¤ì •ì§‘(Wiki) íŒŒì¼ ìƒì„±
    WikiGenerator.save_report_to_file(all_storyboards)

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'output' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()