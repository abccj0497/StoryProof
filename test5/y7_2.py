import os
import re
import json
import time
from typing import List, Dict, Any
from collections import defaultdict

# =========================================================
# [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •]
# =========================================================
from google import genai
from google.genai import types

# =========================================================
# [í™˜ê²½ ì„¤ì •]
# =========================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4") 
INPUT_FILE_NAME = "KR_fantasy_alice.txt" 

LLM_MODEL_NAME = "gemini-1.5-flash" 
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)

# ==============================================================================
# [PART 1] í•˜ì´ë¸Œë¦¬ë“œ ì²­ì»¤ (Hybrid Chunker)
# ==============================================================================
class HybridSceneChunker:
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'ìˆ²', 'êµ´', 'ì •ì›', 'í™€', 'ë°”ë‹¤', 'ì§‘ì•ˆ', 'ë‚˜ë¬´', 'ì„±', 'ë§ˆì„', 'êµì‹¤', 'ë³µë„']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ê°‘ìê¸°', 'ë©°ì¹  ë’¤']
    CHAPTER_PATTERNS = [r"^\s*ì œ\s*[0-9]+\s*[ì¥í™”í¸]", r"^\s*Chapter\s*[0-9]+", r"^\s*\*\*\*"]

    def __init__(self, target_chars=3000, min_chars=1000, threshold=5):
        self.target_chars = target_chars
        self.min_chars = min_chars
        self.threshold = threshold

    def _calculate_score(self, text_segment):
        score = 0
        if any(k in text_segment for k in self.LOCATION_KEYWORDS): score += 5
        if any(k in text_segment for k in self.TIME_TRANSITIONS): score += 4
        return score

    def split_content(self, text: str) -> List[str]:
        text = text.replace('\r\n', '\n')
        paragraphs = re.split(r'\n\s*\n', text)
        final_scenes, current_scene, current_len = [], [], 0
        
        for para in paragraphs:
            para = para.strip()
            if not para: continue
            is_chapter = any(re.match(p, para) for p in self.CHAPTER_PATTERNS)
            score = self._calculate_score(para[:50])

            if (is_chapter and current_len > 0) or \
               (score >= self.threshold and current_len >= self.min_chars) or \
               (current_len + len(para) > self.target_chars):
                final_scenes.append("\n\n".join(current_scene))
                current_scene, current_len = [para], len(para)
            else:
                current_scene.append(para)
                current_len += len(para)

        if current_scene: final_scenes.append("\n\n".join(current_scene))
        return final_scenes

def process_and_save_chunks(file_path: str) -> List[Dict]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f: text = f.read()
    except:
        with open(file_path, 'r', encoding='cp949') as f: text = f.read()

    chunker = HybridSceneChunker()
    chunks_text = chunker.split_content(text)
    result_list = []
    
    if os.path.exists(SCENE_DIR):
        for f in os.listdir(SCENE_DIR): os.remove(os.path.join(SCENE_DIR, f))

    for i, scene_text in enumerate(chunks_text):
        scene_id = f"scene_{i+1:03d}"
        with open(os.path.join(SCENE_DIR, f"{scene_id}.txt"), "w", encoding="utf-8") as f:
            f.write(scene_text)
        result_list.append({'id': scene_id, 'text': scene_text})
    return result_list

# ==============================================================================
# [PART 2] ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œê¸° (Story Analyzer)
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        ì†Œì„¤ì˜ ì¥ë©´ì„ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. 
        íŠ¹íˆ 'ì¸ë¬¼' ë¶„ë¥˜ ì‹œ ì‘ê°€ë‚˜ ì±… ì œëª©ì€ ì œì™¸í•˜ê³  ì‹¤ì œ ë“±ì¥ì¸ë¬¼ë§Œ ë„£ìœ¼ì„¸ìš”.

        [TEXT]
        {chunk['text'][:4000]}
        
        [OUTPUT JSON FORMAT]
        {{
          "scene_id": "{chunk['id']}",
          "novel_info": {{ "title": "ì†Œì„¤ ì œëª©", "author": "ì‘ê°€ ì´ë¦„" }},
          "scene_title": "ì¥ë©´ ì†Œì œëª©",
          "summary": "ì¥ë©´ ìš”ì•½(3ì¤„ ì´ë‚´)",
          "entities": [
            {{ "name": "ì´ë¦„", "type": "ì¸ë¬¼/ì¥ì†Œ/ì•„ì´í…œ", "desc": "ì™¸í˜•ì´ë‚˜ íŠ¹ì§•", "action": "ì´ ì¥ë©´ì—ì„œì˜ í–‰ë™/ì—­í• " }}
          ]
        }}
        """
        try:
            response = self.client.models.generate_content(
                model=LLM_MODEL_NAME,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return json.loads(response.text)
        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬: {e}")
            return None

# ==============================================================================
# [PART 3] ë°”ì´ë¸” ìƒì„±ê¸° (Wiki Generator)
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def save_to_markdown(data_list: List[Dict]):
        path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        
        # ë°ì´í„° ì •ë¦¬
        novel_title = data_list[0].get('novel_info', {}).get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')
        author = data_list[0].get('novel_info', {}).get('author', 'ì•Œ ìˆ˜ ì—†ìŒ')
        
        wiki = defaultdict(lambda: defaultdict(list))
        for scene in data_list:
            s_id = scene['scene_id']
            for ent in scene.get('entities', []):
                # ì‘ê°€ë‚˜ ì œëª©ì´ ì¸ë¬¼ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° í•„í„°ë§
                if ent['name'] in [novel_title, author]: continue
                wiki[ent['type']][ent['name']].append({
                    "scene": s_id, "desc": ent['desc'], "action": ent['action']
                })

        with open(path, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ“š ì†Œì„¤ ë¶„ì„ ë°”ì´ë¸”: {novel_title}\n\n")
            
            f.write(f"## 1. ê¸°ë³¸ ì •ë³´ (Book Info)\n")
            f.write(f"- **ì œëª©:** {novel_title}\n- **ì‘ê°€:** {author}\n\n")
            
            f.write(f"## 2. ì „ì²´ ìŠ¤í† ë¦¬ë¼ì¸\n")
            for scene in data_list:
                f.write(f"- **[{scene['scene_id']}] {scene['scene_title']}**\n")
                f.write(f"  - {scene['summary']}\n")

            # ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ì „ (ì¸ë¬¼, ì¥ì†Œ, ì•„ì´í…œ)
            type_map = {"ì¸ë¬¼": "ë“±ì¥ì¸ë¬¼ (Characters)", "ì¥ì†Œ": "ì¥ì†Œ (Places)", "ì•„ì´í…œ": "ì•„ì´í…œ (Items)"}
            for k, v in type_map.items():
                f.write(f"\n## {v}\n")
                items = wiki.get(k, {})
                if not items: f.write("- ë°ì´í„° ì—†ìŒ\n")
                for name, info in items.items():
                    f.write(f"### {name}\n")
                    for r in info:
                        f.write(f"- `({r['scene']})` {r['desc']} / *{r['action']}*\n")
        
        print(f"âœ… ë°”ì´ë¸” ì €ì¥ ì™„ë£Œ: {path}")

# ==============================================================================
# [ë©”ì¸ ì‹¤í–‰]
# ==============================================================================
def main():
    create_output_dirs()
    chunks = process_and_save_chunks(INPUT_FILE_NAME)
    analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    results = []
    
    print(f"ğŸš€ ë¶„ì„ ì‹œì‘ (ì´ {len(chunks)}ê°œ ì”¬)")
    
    # [ìˆ˜ì •] [:5]ë¥¼ ì œê±°í•˜ì—¬ ì „ì²´ ë¶„ì„ ì§„í–‰
    for i, chunk in enumerate(chunks):
        print(f"  â–¶ [{i+1}/{len(chunks)}] {chunk['id']} ë¶„ì„ ì¤‘...")
        res = analyzer.analyze(chunk)
        if res: results.append(res)
        time.sleep(1.2) # API ì†ë„ ì¡°ì ˆ

    if results:
        with open(os.path.join(OUTPUT_DIR, "analysis.json"), "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        WikiGenerator.save_to_markdown(results)

if __name__ == "__main__":
    main()