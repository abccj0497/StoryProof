import os
import re
import json
import time
from typing import List, Dict
from collections import defaultdict
from google import genai
from google.genai import types

# =========================================================
# [ì„¤ì •]
# =========================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "YOUR_GOOGLE_API_KEY")
LLM_MODEL_NAME = "gemini-1.5-flash"
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)

# ==============================================================================
# [í•µì‹¬] í•˜ì´ë¸Œë¦¬ë“œ ì²­ì»¤ (í‚¤ì›Œë“œ + ê¸¸ì´ + ì±•í„°)
# ==============================================================================
class HybridSceneChunker:
    # 1. ë‹˜ì´ ì •ì˜í•œ í‚¤ì›Œë“œ (Scene Change Signals)
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'ìˆ²', 'êµ´', 'ì •ì›', 'í™€', 'ë°”ë‹¤', 'ì§‘ì•ˆ', 'ë‚˜ë¬´', 'ì„±', 'ë§ˆì„', 'êµì‹¤', 'ë³µë„']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ê°‘ìê¸°', 'ë©°ì¹  ë’¤', 'ëª‡ ì‹œê°„ í›„', 'ìƒˆë²½']
    
    # 2. ì±•í„° íŒ¨í„´ (Chapter Boundaries)
    CHAPTER_PATTERNS = [
        r"^\s*ì œ\s*[0-9]+\s*[ì¥í™”í¸]",   # ì œ 1 ì¥
        r"^\s*Chapter\s*[0-9]+",       # Chapter 1
        r"^\s*Epilogue", r"^\s*Prologue",
        r"^\s*\*\*\*",                 # êµ¬ë¶„ì„ 
        r"^\s*[0-9]+\.",               # 1. 
    ]

    def __init__(self, target_chars=3000, min_chars=1000, threshold=5):
        self.target_chars = target_chars # ì´ ì •ë„ ë˜ë©´ ìë¥¼ ì¤€ë¹„
        self.min_chars = min_chars       # ìµœì†Œ ì´ë§Œí¼ì€ ë­‰ì³ë¼
        self.threshold = threshold       # í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ì¤€

    def _calculate_score(self, sentence):
        """í•œ ë¬¸ì¥ì— ì¥ë©´ ì „í™˜ ì‹œê·¸ë„ì´ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ ê³„ì‚°"""
        score = 0
        if "***" in sentence: score += 10
        if any(k in sentence for k in self.LOCATION_KEYWORDS): score += 5
        if any(k in sentence for k in self.TIME_TRANSITIONS): score += 4
        return score

    def split_content(self, text: str) -> List[str]:
        # [Step 1] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì¤„ë°”ê¿ˆ í†µì¼)
        text = text.replace('\r\n', '\n')
        
        # [Step 2] 1ì°¨ ë¶„í• : ì±•í„° í—¤ë”ê°€ ìˆìœ¼ë©´ ì¼ë‹¨ í¬ê²Œ ìë¦„
        # (êµ¬í˜„ ë‹¨ìˆœí™”ë¥¼ ìœ„í•´, ì—¬ê¸°ì„œëŠ” ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥/ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ íë¥´ë©° ì²˜ë¦¬í•©ë‹ˆë‹¤)
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ 1ì°¨ ë¶„ë¦¬ (ì—”í„° ë‘ ë²ˆ ê¸°ì¤€)
        paragraphs = re.split(r'\n\s*\n', text)
        
        final_scenes = []
        current_scene = []
        current_len = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para: continue

            # ì±•í„° í—¤ë”ì¸ì§€ í™•ì¸ (ê°•ì œ ì ˆë‹¨)
            is_chapter = any(re.match(p, para, re.IGNORECASE) for p in self.CHAPTER_PATTERNS)
            
            # ë¬¸ë‹¨ ë‚´ì—ì„œ ì ìˆ˜ ê³„ì‚° (ë¬¸ë‹¨ì˜ ì²« ë¬¸ì¥ ê¸°ì¤€)
            first_sentence = para.split('.')[0] if '.' in para else para
            score = self._calculate_score(first_sentence)

            # --- [ê²°ì • ë¡œì§] ---
            
            # A. ì±•í„° í—¤ë”ê°€ ë‚˜ì™”ì„ ë•Œ -> ë¬´ì¡°ê±´ ìë¦„ (ì´ì „ ë‚´ìš© ì €ì¥)
            if is_chapter and current_len > 0:
                final_scenes.append("\n\n".join(current_scene))
                current_scene = [para]
                current_len = len(para)
                continue

            # B. í‚¤ì›Œë“œ ì ìˆ˜ê°€ ë†’ìŒ + ìµœì†Œ ë¶„ëŸ‰ì€ ë„˜ê¹€ -> ìì—°ìŠ¤ëŸ½ê²Œ ìë¦„
            if score >= self.threshold and current_len >= self.min_chars:
                final_scenes.append("\n\n".join(current_scene))
                current_scene = [para] # í˜„ì¬ ë¬¸ë‹¨ë¶€í„° ìƒˆ ì”¬ ì‹œì‘
                current_len = len(para)
                continue

            # C. ë„ˆë¬´ ê¸¸ì–´ì§ (ìµœëŒ€ ë¶„ëŸ‰ ì´ˆê³¼) -> ê°•ì œë¡œ ìë¦„
            if current_len + len(para) > self.target_chars:
                final_scenes.append("\n\n".join(current_scene))
                current_scene = [para]
                current_len = len(para)
                continue

            # D. ì•„ì§ ëœ ì°¼ê±°ë‚˜, ìë¥¼ íƒ€ì´ë° ì•„ë‹˜ -> ê³„ì† ë­‰ì¹¨
            current_scene.append(para)
            current_len += len(para)

        # ë‚¨ì€ ìíˆ¬ë¦¬ ì²˜ë¦¬
        if current_scene:
            # ë§ˆì§€ë§‰ ì¡°ê°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´(500ì ë¯¸ë§Œ) ì• ì”¬ì— í•©ì¹¨
            if len("\n\n".join(current_scene)) < 500 and final_scenes:
                final_scenes[-1] += "\n\n" + "\n\n".join(current_scene)
            else:
                final_scenes.append("\n\n".join(current_scene))
                
        return final_scenes

# ==============================================================================
# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
# ==============================================================================
def process_and_save_chunks(file_path: str) -> List[Dict]:
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f: text = f.read()
    except:
        with open(file_path, 'r', encoding='cp949') as f: text = f.read()

    # í•˜ì´ë¸Œë¦¬ë“œ ì²­ì»¤ ìƒì„± (ëª©í‘œ 3000ì, ìµœì†Œ 1000ì, ê°ë„ 5ì )
    chunker = HybridSceneChunker(target_chars=3000, min_chars=1000, threshold=5)
    
    # ë¶„í•  ì‹¤í–‰
    chunks_text = chunker.split_content(text)
    
    # ì €ì¥ ë° ë¦¬í„´
    result_list = []
    
    if os.path.exists(SCENE_DIR): # ê¸°ì¡´ íŒŒì¼ ì²­ì†Œ
        for f in os.listdir(SCENE_DIR): os.remove(os.path.join(SCENE_DIR, f))

    print(f"ğŸ’¾ [1. ì²­í‚¹] í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹(í‚¤ì›Œë“œ+ê¸¸ì´)ìœ¼ë¡œ ìë¥´ëŠ” ì¤‘...")
    
    for i, scene_text in enumerate(chunks_text):
        scene_id = f"scene_{i+1:03d}"
        file_name = os.path.join(SCENE_DIR, f"{scene_id}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(scene_text)
        
        # ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ (ì˜ë¦° ì´ìœ  ì¶”ì¸¡)
        snippet = scene_text[:30].replace('\n', ' ')
        print(f"   - {scene_id} ({len(scene_text)}ì): {snippet}...")
        
        result_list.append({'id': scene_id, 'text': scene_text, 'scene_index': i})
    
    print(f"âœ… ì´ {len(result_list)}ê°œ ì”¬ìœ¼ë¡œ ë¶„í•  ì™„ë£Œ.")
    return result_list

# ==============================================================================
# [ë¶„ì„ê¸° & ì„¤ì •ì§‘ ìƒì„±ê¸°] (ê¸°ì¡´ ë™ì¼)
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = LLM_MODEL_NAME

    def analyze(self, chunk: Dict) -> Dict:
        prompt = f"""
        Analyze this novel scene (Korean).
        [TEXT START]
        {chunk['text'][:4000]}
        [TEXT END]
        [OUTPUT JSON FORMAT]
        {{
          "scene_id": "{chunk['id']}", 
          "title": "ì†Œì œëª©",
          "dense_summary": "í•œ ì¤„ ìš”ì•½",
          "meta": {{ "time": "ì‹œê°„", "place": "ì¥ì†Œ", "characters": ["ì¸ë¬¼ëª…"] }},
          "wiki_entities": [ {{ "name": "ì´ë¦„", "category": "ì¸ë¬¼/ì¥ì†Œ/ì‚¬ë¬¼", "description": "íŠ¹ì§•", "action": "í–‰ë™" }} ]
        }}
        """
        try:
            response = self.client.models.generate_content(
                model=self.model_name, contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return json.loads(response.text)
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

class WikiGenerator:
    @staticmethod
    def save_report_to_file(storyboard_list: List[Dict]):
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        wiki_db = defaultdict(lambda: defaultdict(list))
        for scene in storyboard_list:
            s_id = scene.get('scene_id')
            for entity in scene.get('wiki_entities', []):
                wiki_db[entity.get('category','ê¸°íƒ€')][entity.get('name','ë¯¸ìƒ')].append({
                    "scene": s_id, "desc": entity.get('description'), "action": entity.get('action')
                })

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ“š ì†Œì„¤ ë¶„ì„ ë¦¬í¬íŠ¸\n\n## 1. ìŠ¤í† ë¦¬ë¼ì¸\n")
            for scene in storyboard_list:
                f.write(f"- **{scene['scene_id']} {scene.get('title','')}**: {scene.get('dense_summary','')}\n")
            f.write("\n## 2. ì—”í‹°í‹° ë°±ê³¼ì‚¬ì „\n")
            for cat, items in wiki_db.items():
                f.write(f"\n### [{cat}]\n")
                for name, recs in items.items():
                    f.write(f"#### {name}\n")
                    for r in recs: f.write(f"- `({r['scene']})` {r['desc']} / *{r['action']}*\n")
        print(f"\nğŸ’¾ ì„¤ì •ì§‘ ì €ì¥ ì™„ë£Œ: {file_path}")

# ==============================================================================
# [ë©”ì¸ ì‹¤í–‰]
# ==============================================================================
def main():
    if "YOUR_GOOGLE" in GOOGLE_API_KEY:
        print("âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”"); return

    create_output_dirs()
    input_file = "KR_fantasy_alice.txt"
    if not os.path.exists(input_file): print(f"âŒ '{input_file}' ì—†ìŒ"); return

    # 1. í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹
    chunks = process_and_save_chunks(input_file)

    # 2. ë¶„ì„ (í…ŒìŠ¤íŠ¸ìš© 5ê°œ)
    analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    all_storyboards = []
    print(f"\nğŸš€ [2. ë¶„ì„] AI ë¶„ì„ ì‹œì‘ (í…ŒìŠ¤íŠ¸ìš© ì•ë¶€ë¶„ 5ê°œ)...")
    
    for chunk in chunks[:5]: 
        print(f"  â–¶ {chunk['id']} ë¶„ì„ ì¤‘...", end=" ")
        result = analyzer.analyze(chunk)
        if result:
            all_storyboards.append(result)
            print(f"ì™„ë£Œ! ({result.get('title')})")
            time.sleep(1.5)
        else: print("ì‹¤íŒ¨")

    # 3. ì €ì¥
    if all_storyboards:
        with open(os.path.join(OUTPUT_DIR, "storyboard_analysis.json"), "w", encoding="utf-8") as f:
            json.dump(all_storyboards, f, indent=2, ensure_ascii=False)
        WikiGenerator.save_report_to_file(all_storyboards)

if __name__ == "__main__":
    main()