import os
import re
import json
import time
from typing import List, Dict, Any
from collections import defaultdict

# =========================================================
# [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •]
# ìµœì‹  Google GenAI SDK ì‚¬ìš© (pip install google-genai)
# =========================================================
from google import genai
from google.genai import types

# =========================================================
# [í™˜ê²½ ì„¤ì •] API í‚¤ì™€ íŒŒì¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”!
# =========================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyA0ADjqddqoa6ipqXNFaO5i4c2_-ByY5l4") # <-- í‚¤ ì…ë ¥!
INPUT_FILE_NAME = "KR_fantasy_alice.txt" # <-- ë¶„ì„í•  ì†Œì„¤ íŒŒì¼ëª…

LLM_MODEL_NAME = "gemini-2.5-flash" # ê°€ì„±ë¹„/ì†ë„ ìµœì í™” ëª¨ë¸
OUTPUT_DIR = "output"
SCENE_DIR = os.path.join(OUTPUT_DIR, "scenes")

def create_output_dirs():
    if not os.path.exists(SCENE_DIR):
        os.makedirs(SCENE_DIR)
        print(f"ğŸ“ í´ë” ìƒì„± ì™„ë£Œ: {SCENE_DIR}")

# ==============================================================================
# [PART 1] í•˜ì´ë¸Œë¦¬ë“œ ì²­ì»¤ (Hybrid Chunker)
# : ì±•í„°, í‚¤ì›Œë“œ, ê¸€ììˆ˜ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ë‹¨ìœ„ë¡œ ìë¦…ë‹ˆë‹¤.
# ==============================================================================
class HybridSceneChunker:
    # 1. ì¥ë©´ ì „í™˜ì„ ì•”ì‹œí•˜ëŠ” í‚¤ì›Œë“œë“¤
    LOCATION_KEYWORDS = ['ë°©', 'ì§‘', 'ê±°ë¦¬', 'ìˆ²', 'êµ´', 'ì •ì›', 'í™€', 'ë°”ë‹¤', 'ì§‘ì•ˆ', 'ë‚˜ë¬´', 'ì„±', 'ë§ˆì„', 'êµì‹¤', 'ë³µë„', 'ì°½ê°€']
    TIME_TRANSITIONS = ['ê·¸ë•Œ', 'ë‹¤ìŒë‚ ', 'ì ì‹œ í›„', 'ì•„ì¹¨', 'ì €ë…', 'ë°¤', 'ê°‘ìê¸°', 'ë©°ì¹  ë’¤', 'ëª‡ ì‹œê°„ í›„', 'ìƒˆë²½', 'ì˜¤í›„']
    
    # 2. ì±•í„°ë‚˜ ì ˆì„ ë‚˜ëˆ„ëŠ” íŒ¨í„´ë“¤
    CHAPTER_PATTERNS = [
        r"^\s*ì œ\s*[0-9]+\s*[ì¥í™”í¸]",   # ì˜ˆ: ì œ 1 ì¥
        r"^\s*Chapter\s*[0-9]+",       # ì˜ˆ: Chapter 1
        r"^\s*Epilogue", r"^\s*Prologue",
        r"^\s*\*\*\*",                 # êµ¬ë¶„ì„ 
        r"^\s*[0-9]+\.",               # ì˜ˆ: 1. 
    ]

    def __init__(self, target_chars=3000, min_chars=1000, threshold=5):
        self.target_chars = target_chars # ëª©í‘œ ê¸€ììˆ˜ (ì´ê²Œ ë„˜ìœ¼ë©´ ìë¥¼ ì¤€ë¹„)
        self.min_chars = min_chars       # ìµœì†Œ ê¸€ììˆ˜ (ì´ê²ƒë³´ë‹¤ ì ìœ¼ë©´ ì•ˆ ìë¦„)
        self.threshold = threshold       # í‚¤ì›Œë“œ ì ìˆ˜ ì»¤íŠ¸ë¼ì¸

    def _calculate_score(self, text_segment):
        """ë¬¸ë‹¨ ì•ë¶€ë¶„ì—ì„œ ì¥ë©´ ì „í™˜ ì‹œê·¸ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        if "***" in text_segment: score += 10
        if any(k in text_segment for k in self.LOCATION_KEYWORDS): score += 5
        if any(k in text_segment for k in self.TIME_TRANSITIONS): score += 4
        return score

    def split_content(self, text: str) -> List[str]:
        # ìœˆë„ìš°/ë§¥ ì¤„ë°”ê¿ˆ í†µì¼
        text = text.replace('\r\n', '\n')
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ 1ì°¨ ë¶„ë¦¬ (ì—”í„° ë‘ ë²ˆ ê¸°ì¤€)
        paragraphs = re.split(r'\n\s*\n', text)
        
        final_scenes = []
        current_scene = []
        current_len = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para: continue

            # ì±•í„° í—¤ë”ì¸ì§€ í™•ì¸
            is_chapter = any(re.match(p, para, re.IGNORECASE) for p in self.CHAPTER_PATTERNS)
            
            # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚° (ë¬¸ë‹¨ì˜ ì²« 50ìë§Œ ê²€ì‚¬)
            score = self._calculate_score(para[:50])

            # --- [ìë¥´ê¸° ê²°ì • ë¡œì§] ---
            
            # Case A: ì±•í„° í—¤ë”ê°€ ë‚˜ì™”ì„ ë•Œ -> ë¬´ì¡°ê±´ ìë¦„ (ì´ì „ ë‚´ìš© ì €ì¥)
            if is_chapter and current_len > 0:
                final_scenes.append("\n\n".join(current_scene))
                current_scene = [para]
                current_len = len(para)
                continue

            # Case B: í‚¤ì›Œë“œ ì ìˆ˜ê°€ ë†’ê³  + ìµœì†Œ ë¶„ëŸ‰ì€ ì±„ì› ì„ ë•Œ -> ìì—°ìŠ¤ëŸ½ê²Œ ìë¦„
            if score >= self.threshold and current_len >= self.min_chars:
                final_scenes.append("\n\n".join(current_scene))
                current_scene = [para]
                current_len = len(para)
                continue

            # Case C: ë„ˆë¬´ ê¸¸ì–´ì¡Œì„ ë•Œ (ìµœëŒ€ ë¶„ëŸ‰ ì´ˆê³¼) -> ê°•ì œë¡œ ìë¦„
            if current_len + len(para) > self.target_chars:
                final_scenes.append("\n\n".join(current_scene))
                current_scene = [para]
                current_len = len(para)
                continue

            # Case D: ê³„ì† ë­‰ì¹¨
            current_scene.append(para)
            current_len += len(para)

        # ë‚¨ì€ ìíˆ¬ë¦¬ ì²˜ë¦¬
        if current_scene:
            # ë§ˆì§€ë§‰ ì¡°ê°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´(500ì ë¯¸ë§Œ) ë°”ë¡œ ì• ì”¬ì— í•©ì³ë²„ë¦¼
            if len("\n\n".join(current_scene)) < 500 and final_scenes:
                final_scenes[-1] += "\n\n" + "\n\n".join(current_scene)
            else:
                final_scenes.append("\n\n".join(current_scene))
                
        return final_scenes

def process_and_save_chunks(file_path: str) -> List[Dict]:
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f: text = f.read()
    except:
        with open(file_path, 'r', encoding='cp949') as f: text = f.read()

    # í•˜ì´ë¸Œë¦¬ë“œ ì²­ì»¤ ì‹¤í–‰
    chunker = HybridSceneChunker(target_chars=3000, min_chars=1000, threshold=5)
    chunks_text = chunker.split_content(text)
    
    # ê²°ê³¼ ì €ì¥
    result_list = []
    
    # ê¸°ì¡´ íŒŒì¼ ì²­ì†Œ
    if os.path.exists(SCENE_DIR):
        for f in os.listdir(SCENE_DIR): os.remove(os.path.join(SCENE_DIR, f))

    print(f"ğŸ’¾ [1. ì²­í‚¹] ì†Œì„¤ì„ {len(chunks_text)}ê°œì˜ ì”¬ìœ¼ë¡œ ë¶„í•  ì¤‘...")
    
    for i, scene_text in enumerate(chunks_text):
        scene_id = f"scene_{i+1:03d}"
        file_name = os.path.join(SCENE_DIR, f"{scene_id}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(scene_text)
        
        result_list.append({'id': scene_id, 'text': scene_text, 'scene_index': i})
    
    print(f"âœ… ì²­í‚¹ ì™„ë£Œ (Output: {SCENE_DIR})")
    return result_list

# ==============================================================================
# [PART 2] ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œê¸° (Story Analyzer)
# : y5_3.pyì˜ ê¸°ëŠ¥ì„ ê°€ì ¸ì™€ì„œ JSON íŒŒì‹±ì„ ë” ê²¬ê³ í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
# ==============================================================================
class StoryAnalyzer:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = LLM_MODEL_NAME

    def analyze(self, chunk: Dict) -> Dict:
        # í”„ë¡¬í”„íŠ¸: AIì—ê²Œ ë‚´ë¦¬ëŠ” ì§€ì‹œì‚¬í•­
        prompt = f"""
        You are a professional novel editor. Analyze this novel scene.
        Input Text is in Korean. Output MUST be in JSON format.

        [TEXT START]
        {chunk['text'][:4000]}
        [TEXT END]
        
        [INSTRUCTION]
        Extract the following elements into a valid JSON object:
        1. scene_id: "{chunk['id']}"
        2. title: A suitable title for this scene.
        3. dense_summary: A detailed summary of the plot (3-5 sentences).
        4. meta: Time, Place, and a list of Characters appearing in this scene.
        5. wiki_entities: Extract key entities (Character, Place, Item) with their description and actions in this scene.

        [OUTPUT JSON FORMAT EXAMPLE]
        {{
          "scene_id": "scene_001", 
          "title": "ì†Œì œëª©",
          "dense_summary": "ìš”ì•½ë¬¸...",
          "meta": {{ "time": "ì˜¤í›„", "place": "ê±°ì‹¤", "characters": ["ì² ìˆ˜", "ì˜í¬"] }},
          "wiki_entities": [ 
            {{ "name": "ì² ìˆ˜", "category": "Character", "description": "ì£¼ì¸ê³µ, í•™ìƒ", "action": "ì˜í¬ì™€ ë‹¤íˆ¼" }} 
          ]
        }}
        """
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            # ë¬¸ìì—´ì„ JSON ê°ì²´ë¡œ ë³€í™˜
            return json.loads(response.text)
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨ ({chunk['id']}): {e}")
            return None

# ==============================================================================
# [PART 3] ì„¤ì •ì§‘ ìƒì„±ê¸° (Wiki Generator)
# : ë¶„ì„ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê¸° ì¢‹ì€ Markdown ë¬¸ì„œë¥¼ ë§Œë“­ë‹ˆë‹¤.
# ==============================================================================
class WikiGenerator:
    @staticmethod
    def save_report_to_file(storyboard_list: List[Dict]):
        file_path = os.path.join(OUTPUT_DIR, "writer_bible.md")
        print(f"\nğŸ’¾ [3. ì„¤ì •ì§‘] ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±: {file_path}")
        
        # ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¬ì •ë ¬
        wiki_db = defaultdict(lambda: defaultdict(list))
        for scene in storyboard_list:
            s_id = scene.get('scene_id')
            for entity in scene.get('wiki_entities', []):
                cat = entity.get('category', 'Etc')
                name = entity.get('name', 'Unknown')
                wiki_db[cat][name].append({
                    "scene": s_id, 
                    "desc": entity.get('description'), 
                    "action": entity.get('action')
                })

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ“š ì†Œì„¤ ë¶„ì„ ë°”ì´ë¸” (Writer's Bible)\n")
            f.write(f"Generated Date: {time.strftime('%Y-%m-%d %H:%M')}\n\n")
            
            # ì„¹ì…˜ 1: ì „ì²´ ìŠ¤í† ë¦¬ë¼ì¸
            f.write("## 1. ìŠ¤í† ë¦¬ë¼ì¸ (Scene List)\n")
            for scene in storyboard_list:
                f.write(f"- **[{scene['scene_id']}] {scene.get('title','')}**\n")
                f.write(f"  - {scene.get('dense_summary','')}\n")
            
            # ì„¹ì…˜ 2: ì—”í‹°í‹° ë°±ê³¼ì‚¬ì „
            f.write("\n## 2. ì—”í‹°í‹° ë°±ê³¼ì‚¬ì „ (Wiki)\n")
            for cat, items in wiki_db.items():
                f.write(f"\n### [{cat}]\n")
                for name, recs in items.items():
                    f.write(f"#### {name}\n")
                    for r in recs: 
                        f.write(f"- `({r['scene']})` {r['desc']} / *{r['action']}*\n")
                    
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ.")

# ==============================================================================
# [ë©”ì¸ ì‹¤í–‰ë¶€]
# ==============================================================================
def main():
    # 1. API í‚¤ í™•ì¸
    if "YOUR_GOOGLE" in GOOGLE_API_KEY:
        print("âŒ API í‚¤ ì˜¤ë¥˜: ì½”ë“œ ìƒë‹¨ì˜ GOOGLE_API_KEY ë³€ìˆ˜ì— ë³¸ì¸ì˜ í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    # 2. ì¤€ë¹„
    create_output_dirs()
    if not os.path.exists(INPUT_FILE_NAME):
        print(f"âŒ íŒŒì¼ ì—†ìŒ ì˜¤ë¥˜: '{INPUT_FILE_NAME}' íŒŒì¼ì´ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 3. [Step 1] ì²­í‚¹
    chunks = process_and_save_chunks(INPUT_FILE_NAME)

    # 4. [Step 2] ë¶„ì„
    analyzer = StoryAnalyzer(GOOGLE_API_KEY)
    all_storyboards = []
    
    print(f"\nğŸš€ [2. ë¶„ì„] AI ìŠ¤í† ë¦¬ë³´ë“œ ì¶”ì¶œ ì‹œì‘ (ì´ {len(chunks)}ê°œ ì”¬)")
    print("   (Tip: ì „ì²´ ë¶„ì„ì€ ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ, í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì½”ë“œë¥¼ ìˆ˜ì •í•´ ê°œìˆ˜ë¥¼ ì œí•œí•˜ì„¸ìš”)")

    # â˜… ì¤‘ìš”: ì „ì²´ë¥¼ ë¶„ì„í•˜ë ¤ë©´ ì•„ë˜ chunks[:5]ë¥¼ -> chunks ë¡œ ë°”ê¾¸ì„¸ìš”!
    target_chunks = chunks[:5] 
    
    for i, chunk in enumerate(target_chunks): 
        print(f"  â–¶ [{i+1}/{len(target_chunks)}] {chunk['id']} ë¶„ì„ ì¤‘...", end=" ")
        
        result = analyzer.analyze(chunk)
        
        if result:
            all_storyboards.append(result)
            print(f"ì™„ë£Œ! ({result.get('title', 'ì œëª©ì—†ìŒ')})")
            time.sleep(1.5) # API ê³¼ë¶€í•˜ ë°©ì§€ ë”œë ˆì´
        else:
            print("ì‹¤íŒ¨ (ë„˜ì–´ê°)")

    # 5. [Step 3] ì €ì¥
    if all_storyboards:
        # JSON ì €ì¥ (DB ëŒ€ìš©)
        json_path = os.path.join(OUTPUT_DIR, "storyboard_analysis.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(all_storyboards, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ JSON ë°ì´í„° ì €ì¥ ì™„ë£Œ: {json_path}")
        
        # ì„¤ì •ì§‘ ìƒì„±
        WikiGenerator.save_report_to_file(all_storyboards)
    else:
        print("\nâŒ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()