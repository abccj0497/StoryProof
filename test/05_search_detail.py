import json
import torch
import os
from sentence_transformers import SentenceTransformer, util

# ==========================================
# [ì„¤ì •] ê²€ìƒ‰í•  ë°ì´í„° íŒŒì¼ (1ë²ˆ Entity íŒŒì¼ ì¶”ì²œ)
# ==========================================
#TARGET_FILE = "02_recursive_data.json" 
#TARGET_FILE = "01_entity_data.json" 
TARGET_FILE = "03_sliding_data.json" # ë¹„êµí•´ë³´ê³  ì‹¶ìœ¼ë©´ ì´ê±¸ë¡œ ë³€ê²½

# ìš”ì²­í•˜ì‹  5ê°€ì§€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
QUESTIONS = [
    "1. ì•¨ë¦¬ìŠ¤ê°€ í† ë¼ êµ´ë¡œ ë”°ë¼ë“¤ì–´ê°„ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "2. í•˜ì–€ í† ë¼ê°€ ë“¤ê³  ë‹¤ë‹ˆë˜ ë¬¼ê±´ë“¤ì€ ë¬´ì—‡ì¸ê°€?",
    "3. ìê¸°ë²Œë ˆ(ì• ë²Œë ˆ)ëŠ” ì•¨ë¦¬ìŠ¤ì—ê²Œ ì–´ë–¤ ì¡°ì–¸ì„ í–ˆëŠ”ê°€?",
    "4. ì²´ì…” ê³ ì–‘ì´ì˜ ê°€ìž¥ í° íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€?",
    "5. ìž¬íŒìž¥ì—ì„œ ì•¨ë¦¬ìŠ¤ëŠ” ì™•ê³¼ ì—¬ì™•ì—ê²Œ ë­ë¼ê³  ì†Œë¦¬ì³¤ëŠ”ê°€?"
]

def format_list(items):
    """ë¦¬ìŠ¤íŠ¸ê°€ ìžˆìœ¼ë©´ ë¬¸ìžì—´ë¡œ, ì—†ìœ¼ë©´ 'ì—†ìŒ'ìœ¼ë¡œ ë³€í™˜"""
    if items and len(items) > 0:
        return ", ".join(items)
    return "ì—†ìŒ"

def run_detailed_search():
    # 1. íŒŒì¼ ì¡´ìž¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(TARGET_FILE):
        print(f"âŒ ì˜¤ë¥˜: '{TARGET_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € 01_make_entity.py ë“±ì„ ì‹¤í–‰í•´ì„œ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
        return

    print(f">>> [{TARGET_FILE}] ë°ì´í„° ë¡œë”© ë° ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
    # ëª¨ë¸ ë¡œë“œ (ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ)
    model = SentenceTransformer('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True)
    
    # ë°ì´í„° ë¡œë“œ
    with open(TARGET_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ìž„ë² ë”© í…ì„œ ë³€í™˜
    corpus_embeddings = torch.tensor([d['embedding'] for d in data])
    
    print("\n" + "="*70)
    print(f"ðŸš€ ê²€ìƒ‰ ì‹œìž‘ (ì´ {len(QUESTIONS)}ê°œ ì§ˆë¬¸)")
    print("="*70)

    # 2. ê° ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
    for q_idx, question in enumerate(QUESTIONS):
        print(f"\n\nì§ˆë¬¸ {q_idx + 1}: {question}")
        print("-" * 60)

        # ì§ˆë¬¸ ìž„ë² ë”© ë° ê²€ìƒ‰ (Top 3)
        query_embedding = model.encode(question, convert_to_tensor=True)
        results = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)
        
        # 3. ê²°ê³¼ ì¶œë ¥ (ìš”ì²­í•˜ì‹  í¬ë§· ì ìš©)
        print(f"ìƒìœ„ {len(results[0])}ê°œ ê²€ìƒ‰ ê²°ê³¼:")
        
        for rank, res in enumerate(results[0]):
            doc = data[res['corpus_id']]
            score = res['score']
            meta = doc.get('metadata', {})

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì—†ìœ¼ë©´ 'ì—†ìŒ' ì²˜ë¦¬)
            chars = format_list(meta.get('characters'))
            locs  = format_list(meta.get('location')) # 1ë²ˆ íŒŒì¼ì—” ì—†ì„ ìˆ˜ ìžˆìœ¼ë‚˜ í¬ë§· ìœ ì§€
            items = format_list(meta.get('items'))

            print(f"\n  {rank + 1}. ì²­í¬ ID: {doc['id'][:8]}... (ìœ ì‚¬ë„: {score:.4f})")
            print(f"   # ë©”íƒ€ë°ì´í„°")
            print(f"   # ì¸ë¬¼: {chars}")
            print(f"   # ìž¥ì†Œ: {locs}")
            print(f"   # ì•„ì´í…œ: {items}")
            print("   " + "="*50)
            
            # ë³¸ë¬¸ ì¶œë ¥ (ê°€ë…ì„±ì„ ìœ„í•´ ì¤„ë°”ê¿ˆì€ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ í›„ ì¶œë ¥)
            content_view = doc['content'].replace("\n", " ")
            # ë„ˆë¬´ ê¸¸ë©´ 300ìžê¹Œì§€ë§Œ ë³´ì—¬ì£¼ê³  ... ì²˜ë¦¬ (ì „ì²´ë¥¼ ë³´ê³  ì‹¶ìœ¼ë©´ ìŠ¬ë¼ì´ì‹± ì œê±°)
            if len(content_view) > 300:
                print(f"   {content_view[:300]} ... (ì¤‘ëžµ)")
            else:
                print(f"   {content_view}")
            print("   " + "="*50)

if __name__ == "__main__":
    run_detailed_search()